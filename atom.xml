<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>YT&#39;s blog</title>
  <icon>https://www.gravatar.com/avatar/736ec2aba8de6a35a6db7c1b6e6c0ca4</icon>
  <subtitle>Reading and Coding!</subtitle>
  <link href="https://ytno1.github.io/atom.xml" rel="self"/>
  
  <link href="https://ytno1.github.io/"/>
  <updated>2021-04-12T16:58:36.229Z</updated>
  <id>https://ytno1.github.io/</id>
  
  <author>
    <name>YuT</name>
    <email>2542106000@qq.com</email>
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>2020-AKT-Context-Aware Attentive Knowledge Tracing</title>
    <link href="https://ytno1.github.io/archives/af46df22.html"/>
    <id>https://ytno1.github.io/archives/af46df22.html</id>
    <published>2021-04-12T16:36:19.000Z</published>
    <updated>2021-04-12T16:58:36.229Z</updated>
    
    <content type="html"><![CDATA[<h1 id="上下文感知的注意力集中的知识追踪"><a href="#上下文感知的注意力集中的知识追踪" class="headerlink" title="上下文感知的注意力集中的知识追踪"></a>上下文感知的注意力集中的知识追踪</h1><h1 id="ABSTRACT-摘要"><a href="#ABSTRACT-摘要" class="headerlink" title="ABSTRACT 摘要"></a>ABSTRACT 摘要</h1><p>知识追踪(KT)指的是根据学习者过去在教育应用中的表现来预测其未来表现的问题。使用灵活的深度神经网络模型的 KT 的最新发展擅长于这一任务。然而，这些模式的可解释性往往有限，因此不足以满足个性化学习的需要。个性化学习需要使用可解释的反馈和可操作的建议来帮助学习者获得更好的学习结果。在本文中，我们提出了注意力知识追踪(AKT)，它将灵活的基于注意力的神经网络模型与一系列受认知和心理测量模型启发的新颖的、可解释的模型组件相结合。AKT 使用了一种新的单调注意机制，将学习者未来对评估问题的反应与他们过去的反应联系起来；除了问题之间的相似性外，还使用指数衰减和上下文感知的相对距离度量来计算注意力权重。此外，我们使用 Rasch 模型来规则化概念和问题嵌入，这些嵌入能够在不使用过多参数的情况下捕捉同一概念上问题之间的个体差异。我们在几个真实的基准数据集上进行了实验，结果表明，AKT 在预测未来学习者的反应方面优于现有的 KT 方法(在某些情况下 AUC 高达 6%)。我们还进行了几个案例研究，表明 AKT 表现出极好的可解释性，因此在现实世界的教育环境中具有自动反馈和个性化的潜力。</p><span id="more"></span><h1 id="1-INTRODUCTION-介绍"><a href="#1-INTRODUCTION-介绍" class="headerlink" title="1 INTRODUCTION 介绍"></a>1 INTRODUCTION 介绍</h1><p>数据分析和智能导学系统[32]的最新进展使大规模学习者数据的收集和分析成为可能；这些进步暗示了大规模个性化学习的潜力，方法是通过分析每个学习者的学习历史数据，自动向每个学习者提供个性化反馈[24]和学习活动建议[11]。<br>学习者数据分析中的一个关键问题是根据学习者过去的表现来预测他们未来的表现(他们对评估问题的反应)，这被称为知识追踪(KT)问题[3]。在过去的 30 年里，基于两个共同的假设发展了许多解决 KT 问题的方法：i)学习者过去的表现可以用一组变量来概括，这些变量代表了他们在一组概念/技能/知识组件上的当前潜在知识水平；ii)学习者的未来表现可以用他们当前的潜在概念知识水平来预测。具体地说，让 t 表示一组离散的时间指数，我们有以下关于学习者知识和表现的通用模型。<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618245513339-6a1903e4-09d4-49b1-9a7a-b402ee59d178.png#align=left&display=inline&height=28&margin=%5Bobject%20Object%5D&name=image.png&originHeight=28&originWidth=162&size=1763&status=done&style=none&width=162" alt="image.png"><br>       其中 rt∈{0,1}表示学习者在时间步长 t 上对评估问题的评分响应，通常是二进制值（1 对应正确的答案，0 对应不正确的答案）并得到观察。潜在变量 ht 表示学习者当前的知识水平，不会被观察到。 f（·）和 g（·）是表征学习者知识如何决定其反应以及其发展方式的函数；它们有时分别称为响应模型和知识演化模型。<br>       2010 年前 KT 方法的早期发展可以分为两类。第一类以贝叶斯知识追踪(BKT)方法[19，35]为中心，其中知识(ht)是表征学习者是否掌握问题所涵盖的(单个)概念的二进制标量。由于响应(rt)也是二进制值的，响应和知识演化模型简单地是噪声的二进制通道，由猜测、滑动、学习和遗忘概率来参数化。第二类以项目反应理论(IRT)模型[16]为中心，使用这些模型(特别是 S 型连接函数)作为反应模型 f(·)；然后将学习者的知识水平建模为涵盖多个概念的问题的实值向量(ht)。在这些方法中，SPARFA-Trace 方法[13]使用一个简单的仿射变换模型作为显式知识演化模型 g(·)。其他方法，例如，加法因素模型[1]、性能因素分析[22]、难度、能力和学生历史(DASH)模型[15]，以及包括知识分解机器[30]和 DAS3H 模型的扩展的一些最近的方法，使用手工制作的特征，例如在其知识进化模型中的每个概念上的先前尝试、成功和失败的次数。这两个类中的方法都依靠专家标签将问题与概念关联起来，由于它们可以有效地估计每个学习者对专家定义的概念的知识水平，因此具有极好的可解释性。<br>       KT 的最新发展集中在使用更复杂和更灵活的模型来充分利用大规模学习者反应数据集中包含的信息。深度知识追踪(DKT)方法[23]是通过使用长期短期记忆网络[7]作为知识进化模型 g(·)来探索(可能是深度)神经网络用于 KT 的第一种方法。由于 LSTM 单元是非线性的、复杂的函数，它们比仿射变换更灵活，更能捕捉真实数据中的细微差别。<br>       动态键值记忆网络(dynamic key-value memory networks, DKVMN)方法扩展了 DKT，利用外部记忆矩阵(external memory matrix, Ht)来表征学习者知识[36]。这个矩阵被分为两部分:一个静态的“键”矩阵，它包含每个概念的固定表示;一个动态的“值”矩阵，它包含每个学习者对每个概念的不断发展的知识水平。DKVMN 还在响应和知识演化模型的外部矩阵上使用单独的“读”和“写”过程;这些过程使它比 DKT 更加灵活。DKT 和 DKVMN 在预测未来学习者表现[9]上拥有最先进的性能，并已成为新的 KT 方法的基准。<br>       自我注意知识追踪(self-attentive knowledge tracing , SAKT)方法[18]是第一个在 KT 上下文（情境）中使用注意机制的方法。注意机制比循环和基于记忆的神经网络更灵活，在自然语言处理任务中表现出更好的性能。SAKT 的基本设置与变压器(Transformer)模型[29]有许多相似之处，后者是许多序列到序列(sequence-to-sequence)预测任务的有效模型。然而，我们观察到，在我们的实验中，SAKT 的性能并不优于 DKT 和 DKVMN；有关详细信息，请参阅第 4 节。这可能的原因包括：i)不像在语言任务中，单词之间强烈的长距离依赖更为普遍，未来学习者表现对过去的依赖可能被限制在更短的窗口内，以及 ii)学习者响应数据集的大小比自然语言数据集低几个数量级，并且不太可能从高度灵活和大规模的注意模型中受益。<br>       <a href="">(</a>研究问题)更重要的是，没有一种现有的 KT 方法能够真正在未来性能预测和可解释性两方面出类拔萃。早期的 KT 方法表现出很好的可解释性，但对未来学习者的成绩预测没有提供最先进的性能。最近的基于深度学习的知识理论方法在这方面表现出色，但提供的解释力有限。因此，这些 KT 方法并不能完全满足个性化学习的需要，个性化学习不仅需要准确的成绩预测，还需要能够提供自动化的、可解释的反馈和可操作的建议，以帮助学习者获得更好的学习结果。<a href="#_msocom_1">[喻清尘1]</a></p><h2 id="1-1-Contributions-贡献"><a href="#1-1-Contributions-贡献" class="headerlink" title="1.1  Contributions 贡献"></a>1.1  Contributions 贡献</h2><p>对于预测学习者对当前问题的反应的任务，我们提出了注意力知识追踪(AKT)方法，它使用一系列的注意力网络来将这个问题与学习者过去回答的每一个问题联系起来。我们将我们的主要创新总结如下：(本文贡献)<br>(1)与现有的使用原始问题和答案嵌入的注意方法相反，我们将原始嵌入放在上下文中，并通过考虑学习者的整个练习历史来使用针对于过去问题和答案的上下文感知表示。<br>(2)受认知科学关于遗忘机制研究的启发，我们提出了一种新的单调注意机制，该机制使用指数衰减曲线来降低问题在遥远过去的重要性。我们还开发了一种上下文感知措施来表征学习者过去回答过的问题之间的时间距离。<br>(3)利用 Rasch 模型这一简单且可解释的 IRT 模型，在不引入过多模型参数的情况下，使用一系列基于 Rasch 模型的嵌入来捕捉问题之间的个体差异。<br>我们在几个真实世界的基准教育数据集上进行了一系列实验，比较了 AKT 和最新的 KT 方法。我们的结果表明，AKT 在预测未来学习者的表现方面(有时非常显著)优于其他 KT 方法。此外，我们对 AKT 模型的每个关键组件进行了消融研究(ablation studies)，以证明它们的价值。我们还进行了几个案例研究，以表明 AKT 表现出极好的可解释性，并具有自动反馈和练习问题推荐的潜力，这两个都是个性化学习的关键要求。</p><h1 id="2-KNOWLEDGE-TRACING-PROBLEM-SETUP-知识跟踪问题设置"><a href="#2-KNOWLEDGE-TRACING-PROBLEM-SETUP-知识跟踪问题设置" class="headerlink" title="2 KNOWLEDGE TRACING PROBLEM SETUP 知识跟踪问题设置"></a>2 KNOWLEDGE TRACING PROBLEM SETUP 知识跟踪问题设置</h1><p>每个学习者的成绩记录由每个离散时间步的一系列问题和回答组成。对于时间步长 t 的学习者 i，我们将他们回答的问题、这个问题涵盖的概念以及他们的评分答案表示为一个元组<img src="https://cdn.nlark.com/yuque/__latex/8130b895cd0c04ce9451599151d88fe8.svg#card=math&code=%28q_t%5Ei%EF%BC%8Cc_t%5Ei%EF%BC%8Cr_t%5Ei%29&height=24&width=85">，其中<img src="https://cdn.nlark.com/yuque/__latex/906d682406e8a706acc0a9853ec6f45f.svg#card=math&code=q_t%5Ei%E2%88%88N%5E%2B&height=23&width=60">是问题索引，<img src="https://cdn.nlark.com/yuque/__latex/dfe46248827c8629b7501e680e083d3a.svg#card=math&code=c_t%5Ei%E2%88%88N%5E%2B&height=23&width=60">是概念索引，<img src="https://cdn.nlark.com/yuque/__latex/3119467097e5580585b2325ba84e5c6a.svg#card=math&code=r_t%5Ei%E2%88%88%20%5C%7B0%EF%BC%8C1%20%5C%7D&height=24&width=83">是答案。在这种记号下，<img src="https://cdn.nlark.com/yuque/__latex/067cf5704aa483f7c396ae660d761c98.svg#card=math&code=%28q_t%5Ei%EF%BC%8Cc_t%5Ei%EF%BC%8C1%29&height=24&width=79">表示学习者 i 在时间 t 对基于概念<img src="https://cdn.nlark.com/yuque/__latex/e1a0940d9146645d7dce33c6ae1d43ba.svg#card=math&code=c_t%5Ei&height=23&width=12">的问题<img src="https://cdn.nlark.com/yuque/__latex/cc6349dc26819c8790b47b6998ff2611.svg#card=math&code=q_t%5Ei&height=23&width=13">做出了正确的回答。我们注意到，这种设置与以往的一些深度知识追踪工作不同，这些工作往往忽略问题索引，将学习者的表现概括为<img src="https://cdn.nlark.com/yuque/__latex/5eae48953a2fcfcc31e937788a45997c.svg#card=math&code=%28c_t%5Ei%EF%BC%8Cr_t%5Ei%29&height=24&width=55">。此选择是为了避免过度参数化；有关详细分析，请参见第 3.3 节。在下面的讨论中，当我们讨论如何预测单个学习者的未来表现时，我们省略了上标 i。给定它们到时间 t−1 的过去历史为{(q1，c1，r1)，.…，(qt−1，ct−1，rt−1)}，我们的目标是预测他们在当前时间步长 t 对概念 ct 上的问题 qt 的反应 rt。</p><h2 id="2-1-Question-and-Response-Embeddings-问题和回答嵌入"><a href="#2-1-Question-and-Response-Embeddings-问题和回答嵌入" class="headerlink" title="2.1 Question and Response Embeddings 问题和回答嵌入"></a>2.1 Question and Response Embeddings 问题和回答嵌入</h2><p>在前人工作的基础上，我们使用实值嵌入向量 xt∈R 和 yt∈R 分别表示每个问题和每个问答对(qt，rt)。xt 表示关于问题的信息，yt 表示学习者通过回答问题获得的知识，分别对正确答案和错误答案进行了嵌入。D 表示这些嵌入的维度。因此，用 Q 表示问题的数量，总共有 Q 个问题嵌入向量和 2Q 个问题-回答嵌入向量。在大多数现实世界的教育设置中，题库比概念集大得多，许多问题被分配给很少的学习者。因此，现有的 KT 方法大多使用概念来索引问题，以避免过度参数化；覆盖同一概念的所有问题都被视为单个问题。在这种情况下，qt=ct，Q=C。</p><h1 id="3-THE-AKT-METHOD-AKT-方法"><a href="#3-THE-AKT-METHOD-AKT-方法" class="headerlink" title="3 THE AKT METHOD AKT 方法"></a>3 THE AKT METHOD AKT 方法</h1><p>AKT 方法由四个组件组成：两个自注意编码器，一个用于问题，一个用于知识获取，一个基于注意力的知识检索器，以及一个前馈响应预测模型；图 1 显示了 AKT 方法及其相关组件。<br>我们使用两个自我注意的编码器来学习问题和回答的上下文感知表示。我们将第一个编码器称为问题编码器，它根据学习者之前练习过的问题序列，生成每个问题的修改后的上下文表示形式。类似地，我们将第二个编码器称为知识编码器，它产生学习者在回答过去的问题时所获知识的修改的、上下文的表示。或者，我们可以使用与以前的工作类似的问题和回答的原始嵌入。我们发现，上下文感知表示在大多数数据集中表现得更好。我们将知识演化模型称为知识检索器，它使用注意力机制检索过去获得的与当前问题相关的知识。最后，响应预测模型使用检索到的知识预测学习者对当前问题的响应。<a href="">AKT</a>方法是由三种根植于认知科学和心理测量学的直觉驱动的；我们将在下面详细介绍这些直觉。<a href="#_msocom_2">[喻清尘2]</a></p><h2 id="3-1-Context-aware-Representations-and-The-Knowledge-Retriever-上下文感知表示和知识检索器"><a href="#3-1-Context-aware-Representations-and-The-Knowledge-Retriever-上下文感知表示和知识检索器" class="headerlink" title="3.1 Context-aware Representations and The Knowledge Retriever 上下文感知表示和知识检索器"></a>3.1 Context-aware Representations and The Knowledge Retriever 上下文感知表示和知识检索器</h2><p>如上所述，我们在模型中使用了两个编码器。问题编码器采用原始问题嵌入{x1，……，xt}作为输入，并输出使用单调注意机制(在下一小节中详细描述)的上下文感知问题嵌入序列{ˆx1，……ˆxt}。每个问题的上下文感知嵌入既取决于其本身，也取决于过去的问题，即       ˆxt=fenc(x1，……，xt)。类似地，知识编码器采用原始问题-答案嵌入{y1，……，yt−1}作为输入，并输出使用相同的单调注意机制所获取的实际知识序列{ˆy1，……，ˆyt−1}。所获知识的上下文感知嵌入取决于学习者对当前问题和过去问题的回答，即 ˆyt−1=fenc(y1，……，yt−1)。<br>（三大直觉）<a href="">选择使用上下文感知嵌入而不是原始嵌入反映了我们的第一个直觉：学习者在回答问题时理解和学习的方式取决于学习者。这些修改后的表征反映了每个学习者对问题的实际理解和他们实际获得的知识，给出了他们的个人反应历史。这种模式的选择是基于这样一种直觉，即对于两个过去回答顺序不同的学习者来说，他们理解同一问题的方式以及他们从练习中获得的知识可能会有所不同。</a><br>知识检索器将上下文感知问题和问题-回答对嵌入 ˆx：t 和 ˆy：t−1 作为输入，并输出当前问题的检索到的知识状态 ht。我们注意到，在 AKT 中，学习者的当前知识状态也是上下文感知的，因为它取决于他们正在回答的当前问题；这种模型选择与包括 DKT 在内的大多数现有方法不同。我们还注意到，知识检索器只能使用关于过去问题的信息、学习者对这些问题的反应以及当前问题的表示，而不能使用学习者对当前问题的反应，即 ht=f(ˆx1，……，ˆxt，      ˆy1，……，ˆyt−1)。响应预测模型使用检索到的知识来预测当前响应。</p><h2 id="3-2-The-Monotonic-Attention-Mechanism-单调注意机制"><a href="#3-2-The-Monotonic-Attention-Mechanism-单调注意机制" class="headerlink" title="3.2 The Monotonic Attention Mechanism 单调注意机制"></a>3.2 The Monotonic Attention Mechanism 单调注意机制</h2><p>对于编码器和知识检索器，我们使用一种改进的、单调版本的缩放点积注意机制(the scaled dot-product attention mechanism)。我们首先简要总结一下最初的缩放点积注意机制。在此框架下，每个编码器和知识检索器都有一个键、查询和值嵌入层，分别将输入映射到维度 D=D、D 和 D 的输出查询、键和值。设<img src="https://cdn.nlark.com/yuque/__latex/c319db38d76c50ae32b4076f179fc2d5.svg#card=math&code=qt%E2%88%88R%5E%7BDk%C3%971%7D&height=21&width=80">表示对应于学习者在时间 t 回答的问题的查询，使用 Softmax 函数[5]计算缩放的点积关注值<img src="https://cdn.nlark.com/yuque/__latex/f2cff72f44e635b4a2768db02c81ad5a.svg#card=math&code=%CE%B1_%7Bt%2C%CF%84%7D&height=16&width=24">：（单调注意机制相关公式）<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618245979161-f9f5ae77-484a-4aac-b4ed-dd964b986e03.png#align=left&display=inline&height=59&margin=%5Bobject%20Object%5D&name=image.png&originHeight=59&originWidth=295&size=5381&status=done&style=none&width=295" alt="image.png"><br>       然后，由<img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618245986422-16734589-e485-4a71-b31f-20b593e68085.png#align=left&display=inline&height=17&margin=%5Bobject%20Object%5D&name=image.png&originHeight=17&originWidth=113&size=1530&status=done&style=none&width=113" alt="image.png">出缩放的点积注意机制的输出。<img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618245996018-20d5a811-41ce-48b9-a60a-f6e5b24374b2.png#align=left&display=inline&height=20&margin=%5Bobject%20Object%5D&name=image.png&originHeight=20&originWidth=158&size=2058&status=done&style=none&width=158" alt="image.png">分别表示在时间步长 τ 的问题的键和值。根据特定组件的不同，输出要么取决于过去和当前(问题和知识编码器的 τ≤t)，要么仅取决于过去(知识检索器的 τ&lt;t)。<br>       两个编码器都使用自我注意机制，即使用相同的输入计算 qt、kt 和 vt；问题编码器使用{x1，……，xt}，而知识编码器使用{y1，……，yt−1}。另一方面，知识检索器不使用自我注意。如图 1 所示，在时间步，它使用 ˆxt(当前问题的修改嵌入)，{ˆx1，……，ˆxt−1}(过去问题的上下文感知嵌入)和{ˆy1，……，ˆyt−1}(过去问题-回答对的上下文感知嵌入)作为输入，以分别生成查询、键和值。我们注意到，SAKT 使用问题嵌入来映射查询，而响应嵌入来映射键和值。在我们的实验中，我们发现使用问题嵌入来映射查询和键要有效得多。<br>       然而，对于 KT 来说，这种基本的缩放点积注意机制可能是不够的。原因是学习是短暂的，记忆力会衰退[21]；当我们预测学习者对当前问题的反应时，他们在遥远过去的表现并不像最近的表现那样能提供信息。因此，我们开发了一种新的单调注意机制，它反映了我们的<a href="">第二直觉：当学习者面临一个新的问题时，</a>i)关于无关概念和 ii)来自太久以前的过去经验不太可能是高度相关的。具体地说，我们将乘法指数衰减项添加到注意力得分中，如下所示：<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618246011563-113971e6-5658-47ab-bf32-6c2f0afa3610.png#align=left&display=inline&height=43&margin=%5Bobject%20Object%5D&name=image.png&originHeight=43&originWidth=132&size=2371&status=done&style=none&width=132" alt="image.png"><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618246014822-4522e0e1-2100-42a6-b304-adecc0eba57f.png#align=left&display=inline&height=46&margin=%5Bobject%20Object%5D&name=image.png&originHeight=46&originWidth=182&size=2709&status=done&style=none&width=182" alt="image.png"><br>       其中 θ&gt;0 是可学习衰减率参数，d(t，τ)是时间步长 t 和 τ 之间的时间距离度量。换言之，当前问题对过去问题的关注度不仅取决于对应的查询和键之间的相似度，还取决于它们之间的相对时间步数。总而言之，我们的单调注意机制的基本形式是随着时间的推移呈指数衰减曲线，当过去的问题与现在的问题高度相似时，可能会在时间步长上出现峰值。我们注意到，我们对注意力权重应用指数衰减，而不是现有学习者模型中常见的潜在知识(参见[17，26])。<br>       我们注意到，还有许多其他可能的方法来表征注意力的时间动态。首先，在注意力网络擅长的语言任务中，可以使用加性位置嵌入(additive positional embeddings)或可学习嵌入对时间动态进行建模[29]。其次，在我们的单调注意机制中，我们还可以将指数衰减参数化为<img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618246024882-187961be-821b-427a-bdb7-d304e60aa4ba.png#align=left&display=inline&height=32&margin=%5Bobject%20Object%5D&name=image.png&originHeight=32&originWidth=134&size=2111&status=done&style=none&width=134" alt="image.png">。然而，这两个变化都不会带来与我们选择的模型设置相当的性能；在我们的实验中，我们将使用位置编码(而不是单调注意)来将 AKT 与其变体进行比较。<br>       情境感知的距离测量(A context-aware distance measure)。指数衰减函数决定了随着当前时间指数与之前时间指数之间的距离增加，注意权重衰减的速率。定义两个时间指标之间的距离的一种直接方法是它们的绝对值差，即 d(t,τ)= |t−τ|。然而，这种距离是不受上下文影响的，并且忽略了每个学习者的练习历史。例如，考虑以下两个序列的概念，学习者练习:维恩图(VD)1,VD2，…，VD8，素数(PN)9,PN10 和 PN1,VD2,VD3,… ,VD9,PN10, 其中符号“V D2”表示学习者在时间步长 2 时练习了维恩图的概念。在这个例子中，t = 10（即当前时间索引）时，学习者在这两个序列中都回答了一个质数的问题，在这个例子中，学习者在这两个序列中回答了关于 t=10 的质数(即当前时间索引)的问题，但是最近关于质数的过去练习来自不同的时间索引。由于维恩图和素数的概念关系不大，因此在预测当前习题的答案时，学习者以前关于素数的练习比最近关于维恩图的练习更适合我们。在这种情况下，使用直接的绝对值差异，指数衰减曲线将显著降低分配给 t=1 的素数练习的注意力权重。<br>       因此，对于指数衰减机制(在编码器中)，我们提出以下上下文感知的时间步长 d(t,τ)与 τ≤t 之间的距离度量：<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618246034182-1067fd47-f908-49f3-9df5-542cbdc3367e.png#align=left&display=inline&height=110&margin=%5Bobject%20Object%5D&name=image.png&originHeight=110&originWidth=231&size=6227&status=done&style=none&width=231" alt="image.png"><br>       对于知识检索器，我们将 τ＇≤t 替换为 τ&lt;t，将 t’≤t 替换为 t＇&lt;t。换言之，该上下文感知距离度量使用另一个 softmax 函数来根据过去练习的概念与当前概念的关系来调整连续时间索引之间的距离。实际上，在模型训练过程中的每一次迭代中，我们都使用当前的 AKT 模型参数来计算并固定修正后的距离度量，而不是通过距离度量传递梯度。<br>       多头注意和子层(Multi-head attention and sub-layers)。我们还结合了多头注意力，多头关注和子层。我们还加入了多头注意，这对在多个时间尺度[29]上注意过去的位置是有效的。因此，我们使用 H 个独立注意头，每个头都有自己的衰减率 θ，将最终输出连接成(Dv·H)×1 向量，并将其传递给下一层。这种模型设计使 AKT 能够在多个时间尺度上总结学习者过去的表现，这与多尺度上下文、DASH 和 DAS3H 模型中的多个时间窗口有一些相似之处[2，15，21]。我们还在每个编码器和知识检索器中使用几个子层，包括一个用于层归一化[14]，一个用于丢弃[27]，一个完全连接的前馈层，以及一个剩余连接层[6]。</p><h2 id="3-3-Response-Prediction-响应预测"><a href="#3-3-Response-Prediction-响应预测" class="headerlink" title="3.3 Response Prediction 响应预测"></a>3.3 Response Prediction 响应预测</h2><p>AKT 方法的最后一个组成部分是预测学习者对当前问题的反应。预测模型的输入是检索到的知识(知识检索器输出 ht)和嵌入当前问题 xt 的连接向量；该输入在最终通过 Sigmoid 函数[5]之前通过另一个完全连接的网络，以生成学习者正确回答当前问题的预测概率 ˆrt∈[0，1]。通过最小化所有学习者响应的二进制交叉熵损失，以端到端方式训练整个 AKT 方法中的所有可学习参数，即<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618246046765-d26fb85a-efaf-4b19-9459-893089747a9d.png#align=left&display=inline&height=37&margin=%5Bobject%20Object%5D&name=image.png&originHeight=37&originWidth=260&size=2797&status=done&style=none&width=260" alt="image.png"><br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618246052769-4db1bbb4-7c24-4402-a3ec-6f39eb67f28e.png#align=left&display=inline&height=336&margin=%5Bobject%20Object%5D&name=image.png&originHeight=336&originWidth=643&size=72509&status=done&style=none&width=643" alt="image.png"><br>图 1：AKT 方法概述。我们使用基于 Rasch 模型的嵌入作为问题和回答的原始嵌入。问题和知识编码器计算问题和回答对的上下文感知表示。知识检索器使用这些表示作为输入，并计算学习者的知识状态。为简单起见，我们没有展示编码器中的单调注意机制。我们也不显示子层。</p><h2 id="3-4-Rasch-Model-Based-Embeddings-基于-Rasch-模型的嵌入"><a href="#3-4-Rasch-Model-Based-Embeddings-基于-Rasch-模型的嵌入" class="headerlink" title="3.4 Rasch Model-Based Embeddings 基于 Rasch 模型的嵌入"></a>3.4 Rasch Model-Based Embeddings 基于 Rasch 模型的嵌入</h2><p>如前所述，现有的 KT 方法使用概念来索引问题，即设置 qt= ct。由于数据稀少，这种设置是必要的。设 Q 为问题总数，L 为学习者人数。<a href="">在大多数真实世界的学习者回答数据集中，学习者回答的数量与</a>CL 相当，但比 QL 少得多，因为许多问题分配给少数学习者。因此，使用概念对问题进行索引可以有效地避免过度参数化和过拟合。然而，这种基本设置忽略了覆盖同一概念的问题之间的个体差异，从而限制了 KT 方法的灵活性和它们的个性化潜力。<a href="#_msocom_3">[喻清尘3]</a><br>我们使用心理测量学中一个经典而强大的模型，Rasch 模型(也称为 1PL IRT 模型)[16，25]，来构建原始问题和知识嵌入。Rasch 模型使用两个标量来描述学习者正确回答问题的概率：问题的难度和学习者的能力。尽管它很简单，但在正式评估中，当知识是静态的时，它在学习者表现预测上取得了与更复杂的模型相当的性能[12，31]。具体地说，我们将来自概念 ct 的问题 qt 在时间步 t 的嵌入构造为<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618246066019-e3081deb-4daa-4330-a0f2-419e16e0cb68.png#align=left&display=inline&height=24&margin=%5Bobject%20Object%5D&name=image.png&originHeight=24&originWidth=128&size=1277&status=done&style=none&width=128" alt="image.png"><br>       其中，<img src="https://cdn.nlark.com/yuque/__latex/d9cd06829ce329a2bbe0bdb52030a7d1.svg#card=math&code=c_%7Bct%7D%E2%88%88R%5ED&height=21&width=63">是本问题涵盖的概念的嵌入，<img src="https://cdn.nlark.com/yuque/__latex/af3d0f493c9f9f2a12476aadda536c9c.svg#card=math&code=d_%7Bct%7D%E2%88%88R%5ED&height=21&width=64">是总结涉及此概念的问题的变化的向量，<img src="https://cdn.nlark.com/yuque/__latex/e087ff1ad2a484a1206a1410e8fbe2c8.svg#card=math&code=u_%7Bqt%7D%E2%88%88R&height=20&width=54">是控制此问题与其涵盖的概念的偏离程度的标量难度参数。使用标量难度参数对每个概念 ct 中的问题-回答对(qt，rt)进行类似地扩展：<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618246148995-cf3ddc11-2e61-4b60-ba0c-95dd6feb3ce1.png#align=left&display=inline&height=29&margin=%5Bobject%20Object%5D&name=image.png&originHeight=29&originWidth=156&size=1839&status=done&style=none&width=156" alt="image.png"><br>其中 e(ct，rt)∈R 和 f(ct，rt)∈R 是概念反应嵌入和变异向量。这种模式选择反映了我们的第三个直觉：被标记为涵盖相同概念的问题密切相关，但具有重要的个体差异，不应被忽视。这种模型选择在一定程度上受到了融合 KT 和 IRT 模型的另一项工作的启发[8]。<br>这些基于 Rasch 模型的嵌入在对单个问题差异建模和避免过度参数化之间取得了适当的平衡。对于问题嵌入，由于 C≪Q 和 D≫1，该模型的嵌入参数总数为 2CD+Q，略多于使用概念索引问题的模型(CD)，但远低于每个问题单独参数化的模型(QD)。我们进一步定义概念-回答嵌入为<img src="https://cdn.nlark.com/yuque/__latex/59d0080401e0f8364fece06e8e03f6ed.svg#card=math&code=e%28ct%EF%BC%8Crt%29%3Dc_%7Bct%7D%2Bg_%7Brt%7D&height=24&width=144">，其中 g1 和 g0 分别表示正确答案和错误答案(不考虑概念)的嵌入。因此，对于概念-反应嵌入，我们只引入了总共(C+2)D+Q 个新嵌入参数，而没有引入 2CD+Q 个新参数。我们注意到，我们的问题和问题-回答嵌入共享一组参数<img src="https://cdn.nlark.com/yuque/__latex/6f9e619b3a54feb0ac3227fbcf25bcb7.svg#card=math&code=%28c_%7Bct%7D%29&height=20&width=31">；这种设置不同于现有的基于神经网络的 KT 方法，在现有的 KT 方法中，两者是相互独立的。这些紧凑的嵌入表示法不仅显著减少了 AKT 中的参数数量，而且还显著减少了其他一些 KT 方法中的参数数量，从而提高了对未来学习者成绩预测的性能；有关详细信息，请参见表 5。<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618246265279-5c03ea92-b619-4992-bff8-b2766082ebeb.png#align=left&display=inline&height=100&margin=%5Bobject%20Object%5D&name=image.png&originHeight=100&originWidth=638&size=19217&status=done&style=none&width=638" alt="image.png"><br>表 5：基于 Rasch 模型的嵌入(有时非常重要)提高了 KT 方法的性能。</p><h1 id="4-EXPERIMENTAL-RESULTS-实验结果"><a href="#4-EXPERIMENTAL-RESULTS-实验结果" class="headerlink" title="4 EXPERIMENTAL RESULTS 实验结果"></a>4 EXPERIMENTAL RESULTS 实验结果</h1><p>在本节中，我们将详细介绍我们在几个真实数据集上进行的一系列实验。我们通过预测学习者未来的反应对 AKT 进行定量评估，并通过一系列可视化和案例研究对 AKT 进行定性评估。</p><h2 id="4-1-Experimental-Setup-实验设置"><a href="#4-1-Experimental-Setup-实验设置" class="headerlink" title="4.1 Experimental Setup 实验设置"></a>4.1 Experimental Setup 实验设置</h2><p><strong>数据集。</strong>我们使用四个基准数据集：ASSISTments2009、ASSISTments2015、ASSISTments2017 和 Statics2011 评估了 AKT 的性能和几个预测未来学习者反应的基线。ASSISTments 数据集是从一个在线教学平台收集的，特别是 ASSISTments2009 数据集在过去十年中一直是 KT 方法的标准基准。Statics2011 数据集是从一门大学级别的静力学工程课程中收集的。在所有这些数据集上，我们遵循文献中的一系列标准预处理步骤。对于 ASSISTments2009 数据集，我们删除与命名概念没有关联的所有交互。对于 ASSISTments2015 数据集，我们删除了“isGent”字段不是 0 或 1 的所有交互。我们在表 1 中列出了学习者、概念、问题和问题回答对的数量。在这些数据集中，只有 ASSISTments2009 和 ASSISTments2017 数据集包含问题 ID；因此，基于 Rasch 模型的嵌入仅适用于这两个数据集。<br>       <strong>基线方法和评估指标。</strong>我们将 AKT 与几种基线 KT 方法进行了比较，包括 BKT+[35]，DKT，DKT+(它是 DKT 的改进版本，具有预测一致性的正则化[34])，DKVMN[36]，以及最近提出的自关注 KT(SAKT)方法[18]，它使用了一种可以被视为 AKT 的特例的注意机制，而没有对问题和回答的上下文感知表示和单调注意机制。我们使用接收器操作特征曲线(AUC)下的面积作为度量来评估所有 KT 方法在预测二值未来学习者对问题的反应方面的性能。<br>       <strong>训练和测试。</strong>为了评估目的，我们对所有模型和所有数据集执行标准的 k 折交叉验证(k = 5)。因此，对于每一个 fold，使用 20%的学习者作为测试集，20%的学习者作为验证集，60%的学习者作为训练集。对于每一个 fold，我们使用验证集对每一个 KT 方法进行早期停止和调整参数。<br>       由于计算效率的原因，我们截断了超过 200 的学习者反应序列[23,36]。如果一个学习者有超过 200 个反应，我们就把他们的整个反应序列分解成多个更短的反应序列。我们使用 Adam 优化器训练所有模型[10]，批量大小为 24 个学习者，以确保我们的机器(配备了一个 NVIDIA Titan X GPU)能够容纳整个批处理。我们在 PyTorch 中实现了 AKT 的所有版本;我们还重新实现了 DKT、DKT+和 SAKT，因为包含问题 id 需要新的数据集分区，并导致新的实验结果。我们对 AKT, DKT, DKT+，和 SAKT 使用 Xavier 参数初始化方法[4];对于 DKVMN，我们遵循他们的工作，使用来自正态分布的样本来初始化参数[36]。我们不重新实现 BKT+;其在各种数据集上的性能均来自于[36]。对于大多数数据集和算法，一个训元需要少于 10 秒。我们将最大纪元数设置为 300。</p><h2 id="4-2-Results-and-Discussion-结果与讨论"><a href="#4-2-Results-and-Discussion-结果与讨论" class="headerlink" title="4.2 Results and Discussion 结果与讨论"></a>4.2 Results and Discussion 结果与讨论</h2><p>表 2 列出了所有 KT 方法在预测未来学习者反应方面在所有数据集中的表现;我们报告了五次测试的平均值和标准偏差。AKT- r 和 AKT- nr 分别表示基于 Rasch 模型的嵌入和不嵌入 AKT 模型的变体。我们看到 AKT(有时显著)在 ASSISTments 数据集上优于其他 KT 方法，而 DKT+在最小的 Statics2011 数据集上略微优于 AKT。总的来说，AKT 在更大的数据集上表现更好;这一结果表明，注意机制比递归神经网络更灵活，因此更有能力捕捉包含在大规模真实世界学习者反应数据集中的丰富信息。在 ASSISTments2015 和 ASSISTments2017 数据集上，AKT-NR 比最近的基线提高了 6%和 1%的 AUC。它在 Statics2011 和 ASSISTments2009 数据集上的性能与最佳基线不相上下。更重要的是，在有问题 id 的 ASSISTments2009 和 2017 数据集上，AKT-R 显著优于其他 KT 方法，分别比最近的基线高出 2%和 6%。我们注意到，在我们的实现中，DKT 的性能优于更高级的 DKVMN 方法。虽然我们能够使用相同的实验设置[36]复制 DKVMN 的性能，但我们发现 DKT 的性能比之前在该研究中报告的要好得多。DKT+与 DKT 的性能相当，对 Statics2011 数据集做了少许改进。我们还观察到，基于 rnn 的模型 DKT 在所有数据集上的性能都优于 SAKT。<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618246277993-134008cb-4407-4632-8c38-1a3c5831438b.png#align=left&display=inline&height=125&margin=%5Bobject%20Object%5D&name=image.png&originHeight=125&originWidth=643&size=38285&status=done&style=none&width=643" alt="image.png"><br>表 2:所有 KT 方法在所有数据集上预测未来学习者反应的表现。AKT(有时显著)在所有数据集上优于所有基线方法。最好的模特是粗体，第二好的模特是斜体。<br>       <strong>消融研究。</strong>为了证明 AKT 方法中的三个关键创新，即问题和回答的上下文感知表示、单调注意机制和基于 Rasch 模型的嵌入，我们进行了三个额外的消融实验，比较了 AKT 方法的几种变体。第一个实验比较了使用上下文感知问题和响应表示(使用问题和知识编码器)的 AKT-NR 和 AKT-R 与两个变体 AKTraw-NR 和 AKTraw-R；在这些变体中，我们使用原始的问题和响应嵌入作为它们的表示，而不是上下文感知表示(即，不通过编码器传递它们)。第二个实验比较了 AKT-NR 和几个没有单调注意机制的变体。这些变体包括 AKT-NRpos 和 AKT-NRfix，AKT-NRpos 使用(可学习的)位置编码来捕获学习者响应数据中的时间依赖性，AKT-NRfix 使用(固定)位置编码，使用不同频率的正弦和余弦函数[29]。第三个实验将 AKT-R 与 AKT-NR、DKT、DKT-R、DKT+、DKT+-R、DKVMN、DKVMN-R、SAKT 和 SAKT-R 在有问题 ID 的 ASSIST 2009 和 2017 数据集上进行比较；DKT-R、DKT+-R、DKVMN-R 和 SAKT-R 指的是 DKT、DKT+、DKT-R<br>       表 3 显示了上下文感知表示(即问题编码器和知识编码器)的第一次消融实验的结果(由于空间限制，仅测试折叠的平均值，而不是标准偏差)。在所有数据集上，AKT-R 和 AKTNR 的性能都优于 AKTraw-NR 和 AKTraw-R，后者只使用一种指数衰减的自我注意机制(即知识检索器)。这些结果表明，我们对问题和回答的语境感知表征在总结每个学习者的练习历史时是有效的。<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618246285439-c933f944-934a-4b01-89b5-b26b77b8dab4.png#align=left&display=inline&height=128&margin=%5Bobject%20Object%5D&name=image.png&originHeight=128&originWidth=365&size=17053&status=done&style=none&width=365" alt="image.png"><br>表 3：AKT 的表现优于不使用上下文感知问题和响应表示的变体。<br>       表 4 显示了单调注意机制的第二次消融实验结果。我们发现，AKT-NR 在所有数据集上的表现明显优于其他使用位置嵌入的注意机制，包括 SAKT，大约 1%到 6%。我们假设这一结果的原因是，与语言任务不同的是，在语言任务中，单词之间强烈的远程依赖更常见，未来学习者对过去表现的依赖被限制在更短的时间窗口内。因此，在注意力权重中使用不同指数衰减率的多头注意可以有效地捕捉不同时间尺度上对过去的短期依赖。<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618246290257-7623af58-ba82-46ac-b13d-69429da4c302.png#align=left&display=inline&height=129&margin=%5Bobject%20Object%5D&name=image.png&originHeight=129&originWidth=361&size=17248&status=done&style=none&width=361" alt="image.png"><br>表 4：AKT 显著优于不使用单调注意的变种。<br>      表 5 显示了基于 Rasch 模型的嵌入在两个 ASSISTments 数据集(其中有问题 ID)上的第三次消融实验结果。所有添加了基于 Rasch 模型嵌入的基线 KT 方法都优于它们的常规版本，特别是在 ASSISTments2017 数据集上。这些结果证实了我们的直觉，即将涵盖同一概念的所有问题视为一个问题是有问题的；只要可以避免过度参数化，这些问题之间的个体差异就不应该被忽视。<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618246297665-146073c5-30dd-4b0b-a998-95ddb8266354.png#align=left&display=inline&height=96&margin=%5Bobject%20Object%5D&name=image.png&originHeight=96&originWidth=640&size=19135&status=done&style=none&width=640" alt="image.png"><br>表 5：基于 Rasch 模型的嵌入(有时非常重要)提高了 KT 方法的性能。<br><strong>注意。</strong>我们的标准实验设置遵循[23，36]中使用的设置。在此设置中，对于带有多个概念的问题(在 ASSISTments2009 数据集中)，单个学习者的回答重复多次，每个概念一个。其他研究对这些问题使用了不同的实验设置；在[31]中，作者去掉了这些问题，结果，DKT 的成绩降到了 0.71。在[33]中，作者为共现的单个概念的每个组合建立了新的概念，结果，DKT 的性能下降到 0.73。因此，我们还在 ASSISTments2009 数据集上使用了另一种实验设置。对于带有多个概念的问题，我们平均相应的概念嵌入，并将其用作输入嵌入和响应预测。表 6 列出了此设置下 ASSISTments2009 数据集上所有 KT 方法的性能。使用平均嵌入时，DKT 的性能下降到 0.76，比[31，33]下的设置要好。与所有 KT 方法的标准实验设置相比，我们观察到类似的性能下降，而 AKT-R 仍然轻松地超过所有基线。<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618246304142-24afacee-bead-4b55-b9fa-1126a8274144.png#align=left&display=inline&height=100&margin=%5Bobject%20Object%5D&name=image.png&originHeight=100&originWidth=365&size=13531&status=done&style=none&width=365" alt="image.png"><br>表 6：AKT 在 ASSISTments2009 数据集上的性能仍然优于其他 KT 方法，在使用多个概念标记的问题的另一种实验设置下，AKT 仍然优于其他 KT 方法。</p><h2 id="4-3-Visualizing-Learned-AKT-Parameters-可视化学习的-AKT-参数"><a href="#4-3-Visualizing-Learned-AKT-Parameters-可视化学习的-AKT-参数" class="headerlink" title="4.3 Visualizing Learned AKT Parameters 可视化学习的 AKT 参数"></a>4.3 Visualizing Learned AKT Parameters 可视化学习的 AKT 参数</h2><p><strong>单调注意。</strong>图 2 显示了 AKT 使用 ASSISTments2009 数据集的单调注意机制提供的可解释性。图 2(A)以一个学习者为例，可视化了知识检索器中的注意力权重；我们绘制了用于预测他们在三个注意力头部的 20 个连续练习问题上的表现的注意力权重。我们看到，每个注意力头部都有自己的时间尺度：它们都有不同宽度的注意力窗口。例如，第二个头能够关注整个过去，最多 20 个时间点(在本例中)；相反，第三个头只能关注最近的过去，主要关注最后 3-5 个时间点。这一观察表明，过去的一些问题和回答包含了高度预测学习者对当前问题的反应的信息；这些信息可以被具有不同衰减率的多个注意力头脑有效地捕捉到。<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618246311226-32354069-9651-445a-82c2-2fca2446bce5.png#align=left&display=inline&height=365&margin=%5Bobject%20Object%5D&name=image.png&originHeight=365&originWidth=370&size=38538&status=done&style=none&width=370" alt="image.png"><br>图 2：对学习者来说，(A)AKT 解码器中三个注意力头部的注意力权重和(B)三个连续练习问题的注意力权重的可视化。概念相似度和新近度是控制注意力权重的关键因素。<br>       图 2(B)显示了知识检索器中针对单个学习者连续三个时间步的归一化注意力权重。在第一排，学习者在从 −10 到 T−5 练习了概念 30 之后，正在回答关于概念 30 的问题，然后休息一下练习概念 42，然后回到 TIMET−1 的概念 30。我们看到，AKT 预测他们对当前问题的反应是通过更多地关注以前对这个概念的练习(无论是在最近的过去还是更早的过去)，而不是对同样是在不久的过去的另一个概念的练习。在中间一排，学习者再次切换到练习概念 42。同样，在 T−2 和 T−1 的时候，AKT 学习将注意力集中在同一概念上的过去练习，而不是刚刚过去的不同概念上。在最下面的一排，学习者连续第二次练习概念 42，AKT 显示出与第一行相似的焦点模式，概念 30 和 42 的角色互换了。这些观察表明，AKT 的单调注意机制有可能通过将学习者当前的反应与他们过去的反应联系起来，向教师提供反馈；这些信息可能使教师能够选择他们已经练习过的特定问题，让他们在继续学习之前重新练习并清除误解。我们还注意到，AKT 使用数据驱动的方法，学习这些与现有 KT 方法中手工制作的特征相匹配的注意模式(例如，对该概念的总尝试次数和正确尝试次数)[15，22]。<br>       基于 Rasch 模型的嵌入。图 3 显示了使用 ASSISTments2009 数据集使用 t-SNE[28]对几个概念学习的基于 Rasch 模型的问题嵌入，以及它们对选定问题(学习者的正确答案的一部分)的经验困难。我们还强调了每个概念的最难和最简单的问题，这是基于它们的经验困难。我们看到，同一概念上的问题形成一条曲线，并按其难度级别排序：对于大多数概念，线段一端的问题很容易，而另一端的问题很难。这一结果证实了我们的直觉，即来自同一概念的问题不是完全相同的，而是密切相关的；这种关系可以通过 Rasch 模型的难度参数很好地捕捉到。<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618246318541-1be15a25-8c8c-4b03-886f-33d9aaf267f3.png#align=left&display=inline&height=225&margin=%5Bobject%20Object%5D&name=image.png&originHeight=225&originWidth=359&size=19853&status=done&style=none&width=359" alt="image.png"><br>图 3：可视化的学习问题嵌入，学习者对所选概念的部分正确回答。<br>       表 7 列出了针对三个不同概念（“正序十进制”，“单个事件的概率”和“分数到百分比的转换”）的样本问题，以及它们的学习难度参数。对于每个概念，我们显示三个问题：一个简单的问题，一个平均的问题和一个困难的问题。以“单个事件的概率”概念为例，学习的难度参数值（µq）对于简单事件为-0.0515，对于平均事件为 0.0088，对于困难事件为 0.0548。这些学习的难度级别与我们对这些问题的难度级别的理解相符。<br>       这些结果表明，AKT 有潜力应用于现实世界的教育环境中。使用估计的难度参数，计算机化学习平台可以 i)根据每个学习者过去的反应自动选择具有适当难度级别的问题，或者 ii)通过向教师提供关于从真实数据中学习的问题难度级别的反馈来支持教师调整课程计划。因此，AKT 不仅提供最先进的预测性能，而且表现出可解释性和个性化学习的潜力，从而改进了现有的 KT 方法。<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618246327348-e4305e88-5cb8-44e5-a10e-1c21c9147f30.png#align=left&display=inline&height=406&margin=%5Bobject%20Object%5D&name=image.png&originHeight=406&originWidth=352&size=52596&status=done&style=none&width=352" alt="image.png"><br>表 7：关于三个概念的选定问题的问题文本和学习难度参数(µQ)。习得的难度水平符合我们对这些问题难度的直觉。</p><h1 id="5-CONCLUSIONS-AND-FUTURE-WORK-结论和下一步工作"><a href="#5-CONCLUSIONS-AND-FUTURE-WORK-结论和下一步工作" class="headerlink" title="5 CONCLUSIONS AND FUTURE WORK 结论和下一步工作"></a>5 CONCLUSIONS AND FUTURE WORK 结论和下一步工作</h1><p>本文提出了一种新的完全依赖注意力网络的知识追踪方法–注意力知识追踪(Attensitive Knowledge Tracing)。我们的方法改进了现有的知识跟踪方法，通过建立问题和回答的上下文感知表示，使用单调的注意机制来总结过去学习者在正确的时间尺度上的表现，并使用 Rasch 模型来捕捉覆盖相同概念的问题之间的个体差异。在一系列基准真实学习者反应数据集上的实验结果表明，该方法的性能优于最先进的 KT 方法，并表现出良好的可解释性。未来工作的途径包括：i)纳入问题文本，以进一步增强问题和概念嵌入的可解释性；ii)测试我们的方法是否可以提高对发生记忆衰退的语言学习数据集的预测性能[26]。</p><hr><p><a href="#_msoanchor_1">[喻清尘1]</a>研究问题 1</p><p>1、 <a href="#_msoanchor_2">[喻清尘2]</a>选择使用上下文感知嵌入而不是原始嵌入反映了我们的第一个直觉：学习者在回答问题时理解和学习的方式取决于学习者。这些修改后的表征反映了每个学习者对问题的实际理解和他们实际获得的知识，给出了他们的个人反应历史。这种模式的选择是基于这样一种直觉，即对于两个过去回答顺序不同的学习者来说，他们理解同一问题的方式以及他们从练习中获得的知识可能会有所不同。<br>2、第二直觉：当学习者面临一个新的问题时，i)关于无关概念和 ii)来自太久以前的过去经验不太可能是高度相关的。<br>3、第三个直觉：被标记为涵盖相同概念的问题密切相关，但具有重要的个体差异，不应被忽视。</p><p><a href="#_msoanchor_3">[喻清尘3]</a>研究问题 2</p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;上下文感知的注意力集中的知识追踪&quot;&gt;&lt;a href=&quot;#上下文感知的注意力集中的知识追踪&quot; class=&quot;headerlink&quot; title=&quot;上下文感知的注意力集中的知识追踪&quot;&gt;&lt;/a&gt;上下文感知的注意力集中的知识追踪&lt;/h1&gt;&lt;h1 id=&quot;ABSTRACT-摘要&quot;&gt;&lt;a href=&quot;#ABSTRACT-摘要&quot; class=&quot;headerlink&quot; title=&quot;ABSTRACT 摘要&quot;&gt;&lt;/a&gt;ABSTRACT 摘要&lt;/h1&gt;&lt;p&gt;知识追踪(KT)指的是根据学习者过去在教育应用中的表现来预测其未来表现的问题。使用灵活的深度神经网络模型的 KT 的最新发展擅长于这一任务。然而，这些模式的可解释性往往有限，因此不足以满足个性化学习的需要。个性化学习需要使用可解释的反馈和可操作的建议来帮助学习者获得更好的学习结果。在本文中，我们提出了注意力知识追踪(AKT)，它将灵活的基于注意力的神经网络模型与一系列受认知和心理测量模型启发的新颖的、可解释的模型组件相结合。AKT 使用了一种新的单调注意机制，将学习者未来对评估问题的反应与他们过去的反应联系起来；除了问题之间的相似性外，还使用指数衰减和上下文感知的相对距离度量来计算注意力权重。此外，我们使用 Rasch 模型来规则化概念和问题嵌入，这些嵌入能够在不使用过多参数的情况下捕捉同一概念上问题之间的个体差异。我们在几个真实的基准数据集上进行了实验，结果表明，AKT 在预测未来学习者的反应方面优于现有的 KT 方法(在某些情况下 AUC 高达 6%)。我们还进行了几个案例研究，表明 AKT 表现出极好的可解释性，因此在现实世界的教育环境中具有自动反馈和个性化的潜力。&lt;/p&gt;</summary>
    
    
    
    <category term="知识追踪" scheme="https://ytno1.github.io/categories/%E7%9F%A5%E8%AF%86%E8%BF%BD%E8%B8%AA/"/>
    
    <category term="AKT" scheme="https://ytno1.github.io/categories/%E7%9F%A5%E8%AF%86%E8%BF%BD%E8%B8%AA/AKT/"/>
    
    
    <category term="知识追踪" scheme="https://ytno1.github.io/tags/%E7%9F%A5%E8%AF%86%E8%BF%BD%E8%B8%AA/"/>
    
    <category term="AKT" scheme="https://ytno1.github.io/tags/AKT/"/>
    
  </entry>
  
  <entry>
    <title>A Self-Attentive model for Knowledge Tracing</title>
    <link href="https://ytno1.github.io/archives/630d4ed4.html"/>
    <id>https://ytno1.github.io/archives/630d4ed4.html</id>
    <published>2021-04-12T14:58:32.000Z</published>
    <updated>2021-04-12T16:58:36.226Z</updated>
    
    <content type="html"><![CDATA[<h1 id="ABSTRACT-摘要"><a href="#ABSTRACT-摘要" class="headerlink" title="ABSTRACT 摘要"></a>ABSTRACT 摘要</h1><p>知识追踪是指当每个学生参与一系列学习活动时，对知识概念(KCs)的掌握情况进行建模的任务。每个学生的知识是通过评估学生在学习活动中的表现来建模的。为学生提供个性化的学习平台是一个重要的研究领域。近年来，基于递归神经网络(RNN)的方法，如深度知识追踪(DKT)和动态键值记忆网络(DKVMN)，由于能够捕获人类学习的复杂表示，因而优于所有传统方法。然而，这些方法在处理稀疏数据时面临的问题是不能很好地泛化【研究问题】，这是真实世界数据的情况下，学生与很少的 KCs 交互。为了解决这个问题，我们开发了一种方法，从学生过去的活动中识别与给定 KC 相关的 KC，并根据选择的相对较少的 KC 预测他/她的掌握情况。由于预测是基于相对较少的过去活动，它处理数据稀疏性问题比基于 RNN 的方法更好。为了识别 KCs 之间的相关性，我们提出了一种基于自我注意的方法——自我注意知识追踪(SAKT)。在各种真实世界数据集上的广泛实验表明，我们的模型在知识追踪方面优于最先进的模型，平均提高了 4.43%的 AUC。</p><p><strong>关键词：</strong>知识追踪，大规模网络开放课程，自我关注，顺序推荐</p><span id="more"></span><h1 id="1-INTRODUCTION-引言"><a href="#1-INTRODUCTION-引言" class="headerlink" title="1.   INTRODUCTION 引言"></a>1.   INTRODUCTION 引言</h1><p>学生关于他们知识概念(KC)的学习轨迹有海量数据集是可用的，其中 KC 可以是一种练习，一种技能或一个概念，吸引了数据挖掘者开发工具来预测学生的表现并给予适当的反馈[8]。在开发这种个人化学习平台的过程中，知识追踪(knowledge tracing, KT)被认为是一项重要的任务，它被定义为基于学生过去的学习活动对其知识状态进行追踪，这种知识状态代表了学生对 KCs 的掌握程度。KT 任务可以被形式化为一个监督序列学习任务-给定学生过去的练习互动 X = (x1,x2，…，xt)，预测他/她下次互动（回答）xt+1 的某个方面。在问答平台上，交互用 xt= (et, rt)表示，其中 et 是学生在时间戳 t 上尝试的练习，rt 是学生答案的正确性。KT 旨在预测学生是否能够正确回答下一个练习，即预测 p(rt+1= 1|et+1,X)。<br>最近，深度知识追踪(DKT)[6]及其变体[10]等深度学习模型使用递归神经网络(RNN)在一个总结的隐向量中对学生的知识状态进行建模。动态键值记忆网络(DKVMN)[11]为 KT 开发了记忆扩充神经网络[7]。它使用 Key 和 Value 两个矩阵，分别学习练习与潜在的 KC 和学生知识状态之间的关系。DKT 模型面临参数无法解释的问题[4]。DKVMN 比 DKT 更易于解释，因为它显式地维护 KC 表示矩阵(密钥)和知识状态表示矩阵(值)。然而，由于所有这些深度学习模型都是基于 RNN 的，它们在处理稀疏数据时都面临着不能泛化的问题[3]。<br>在本文中，我们建议使用一种纯粹基于注意机制的方法，即转换器（Transformer）[9]。在 KT 任务中，学生在经历一系列学习活动时所获得的技能是相互关联的，在特定练习中的表现取决于他在过去与该练习相关的练习中的表现。例如，在图 1 中，一个学生要解决属于知识概念“方程”的“二次方程”的练习(练习 5)，他需要知道如何找到“平方根”(练习 3)和“线性方程”(练习 4)。本文提出的 SAKT 首先从过去的互动中识别出相关的知识概念，然后根据学生在这些知识概念上的表现来预测学生的表现。为了预测学生在练习中的表现，我们使用练习作为 KC。正如我们稍后所展示的，SAKT 为前面回答的练习分配权重，同时预测学生在特定练习中的表现。在所有数据集上，SAKT 方法的性能明显优于现有的 KT 方法，在 AUC 上的性能平均提高了 4.43%。此外，SAKT 的主要成分(自我注意)适合于并行性，从而使我们的模型比基于 RNN 的模型快了一个数量级【SAKT 优势】。<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618243190838-289e5481-976c-41a8-9ab9-8211ff6f4544.png#align=left&display=inline&height=108&margin=%5Bobject%20Object%5D&name=image.png&originHeight=108&originWidth=380&size=17722&status=done&style=none&width=380" alt="image.png"><br>图 1：左下图显示了学生尝试的练习顺序，右下图显示了每个练习所属的知识概念。</p><h1 id="2-PROPOSED-METHOD-建议的方法"><a href="#2-PROPOSED-METHOD-建议的方法" class="headerlink" title="2. PROPOSED METHOD 建议的方法"></a>2. PROPOSED METHOD 建议的方法</h1><p>我们的模型根据一个学生之前的交互序列 X = x1,x2，…xt，预测他是否能够回答下一个练习 et+1。如图 2 所示，我们可以将问题转换为顺序建模问题。考虑输入为 x1,x2，…xt 且前面一个位置的练习序列 e2, e3，…，et 且输出为对练习响应的正确性 r2, r3，…rt 的模型是方便的。交互元组 xt= (et, rt)在模型中表示为数字 yt= et+ rt× E，其中 E 为练习总数。因此，交互序列中的元素可以接受的总值是 2E，而练习序列中的元素可以接受 E 个可能的值。<br>现在我们来描述我们体系结构的不同层。<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618243262538-16e7fabc-9155-4a0e-9fae-f2544e4bcf00.png#align=left&display=inline&height=285&margin=%5Bobject%20Object%5D&name=image.png&originHeight=285&originWidth=298&size=44411&status=done&style=none&width=298" alt="image.png"><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618243271344-68f30c9e-24c2-4548-b6dc-475efa17ba70.png#align=left&display=inline&height=142&margin=%5Bobject%20Object%5D&name=image.png&originHeight=142&originWidth=306&size=14789&status=done&style=none&width=306" alt="image.png"><br> (a)SAKT 网络。在每个时间戳上，仅为先前元素中的每一个估计关注度权重。从如下所示的嵌入层中提取键、值和查询。当第 j 个元素为查询且第 i 个元素为关键时，关注权重为 ai，j。<br>(b)嵌入层嵌入学生正在尝试的当前练习及其过去的交互。在每个时间戳 t+1 处，使用练习嵌入将当前问题 et+1 嵌入到查询空间中，并且使用交互嵌入将过去交互 xt 的元素嵌入到键和值空间中。<br>图 2：显示 SAKT 体系结构的示意图。<br><strong>嵌入层：</strong>对得到的输入序列 y=(y1，y2，…，yt)转化为 s=(s1，s2，…，sn)，其中 n 是模型可以处理的最大长度。由于该模型可以处理固定长度序列的输入，当序列长度 t 小于 n 时，我们在序列的左边重复添加一个问答对填充。然而，如果 t 大于 n，则我们将序列划分为长度为 n 的子序列。具体地说，当 t 大于 n 时，yt 被划分为 t/n 个子序列，每个子序列的长度为 n。所有这些子序列都用作模型的输入。我们训练一个交互嵌入矩阵 M∈R×d，其中 d 是潜在维数。该矩阵用于获得序列中每个元素 si 的嵌入 Msi。同样，我们训练练习嵌入矩阵 E∈R×d，使得集合 ei 中的每个练习嵌入到第 i 行。<br>位置编码：位置编码是自注意神经网络中用来对位置进行编码的那一层，这样我们就可以像卷积网络和递归神经网络一样对序列的顺序进行编码。这一层在知识追踪问题中尤为重要，因为学生的知识状态会随着时间的推移而逐渐稳定地演变。在某个特定时间实例的知识状态不应显示波状转变[10]。为了整合这一点，我们使用了一个参数，位置嵌入，P∈R×d，它是在训练时学习的。然后将第 i 行位置嵌入矩阵 Pi 加入到交互序列的第 i 个元素的交互嵌入向量。<br>嵌入层的输出是嵌入交互输入矩阵 Mˆ 和嵌入练习矩阵 Eˆ：<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618243357720-1758d9a4-a12e-44da-b991-996cf5c3c650.png#align=left&display=inline&height=92&margin=%5Bobject%20Object%5D&name=image.png&originHeight=92&originWidth=226&size=5029&status=done&style=none&width=226" alt="image.png"><br><strong>自我注意层：</strong>在我们的模型中，我们使用了按比例缩放的点积注意机制[9]。这一层找出与每个先前求解的练习相对应的相对权重，以预测当前练习的正确性。<br>我们使用以下公式获得查询和键-值对：<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618243365540-06d51e38-fc02-4357-87a9-6a0f663c83e0.png#align=left&display=inline&height=31&margin=%5Bobject%20Object%5D&name=image.png&originHeight=31&originWidth=238&size=2736&status=done&style=none&width=238" alt="image.png"><br>其中 W、W、W∈R×d 分别表示查询、关键字和值投影矩阵，它们将各自的向量线性投影到不同的空间[9]。使用注意力权重来确定先前的每个交互与当前练习的相关性。为了计算关注度权重，我们使用缩放的点积[9]，定义如下：<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618243372373-97bc6bac-3414-433b-8b0c-d4d52e83d553.png#align=left&display=inline&height=39&margin=%5Bobject%20Object%5D&name=image.png&originHeight=39&originWidth=255&size=4051&status=done&style=none&width=255" alt="image.png"><br>多头:为了共同处理来自不同代表性子空间的信息，我们使用不同的投影矩阵对查询、键和值进行线性投影 h 次。<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618243383333-45cc8361-eaca-4901-b8cc-723b233a90b9.png#align=left&display=inline&height=22&margin=%5Bobject%20Object%5D&name=image.png&originHeight=22&originWidth=305&size=3432&status=done&style=none&width=305" alt="image.png"><br>其中<img src="https://cdn.nlark.com/yuque/__latex/772b9537c10dd0f2f83ce0ef1fe1f43a.svg#card=math&code=head_i%3D%20Attention%28%5Chat%20E%20W%5EQ_i%EF%BC%8C%5Chat%20MW%5EK_i%EF%BC%8C%5Chat%20MW%5EV_i%29&height=26&width=317">和 W∈R×d。<br>因果关系：在我们的模型中，在预测第(t+1)次练习的结果时，我们应该只考虑前 t 次交互作用。因此，对于查询 Qi，不应考虑使得 j&gt;i 的键 Kj。我们使用因果层来掩蔽从未来交互关键字学习到的权重。<br><strong>前馈层：</strong>上面描述的自我注意层的结果是前面交互的值 Vi 的加权和。然而，从多头层获得的矩阵的行<img src="https://cdn.nlark.com/yuque/__latex/2fdbad74d3b3359634d48a2b1f988430.svg#card=math&code=S%3DMultiHead%28%5Chat%20M%EF%BC%8C%5Chat%20E%29&height=26&width=177">仍然是先前交互的值 Vi 的线性组合。为了在模型中加入非线性，并考虑不同潜在维度之间的相互作用，我们使用了前馈网络。<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618243674509-49bf487d-0415-4d9f-a29d-3f2e27d10c31.png#align=left&display=inline&height=30&margin=%5Bobject%20Object%5D&name=image.png&originHeight=30&originWidth=299&size=3464&status=done&style=none&width=299" alt="image.png"><br>其中 W∈R×d，W∈R×d，b∈R，b∈R 是在训练过程中学习到的参数。<br>残差连接：残差连接[2]用于将较低层的特征传播到较高层。因此，如果低层特征对预测很重要，残差连接将有助于将它们传播到执行预测的最终层。在 KT 的背景下，学生尝试用属于特定概念的练习来强化该概念。因此，残差连接可以帮助将最近求解的练习的嵌入传播到最后一层，使得模型更容易利用低层信息。在自我注意层和前馈层之后都应用了残差连接。<br>层归一化：文献[1]表明，归一化跨特征的输入有助于稳定和加速神经网络。出于同样的目的，我们在我们的体系结构中使用了层归一化。在自我注意层和前馈层也进行了层归一化。<br><strong>预测层：</strong>最后，将得到的矩阵 Fi 值的每一行通过 Sigmoid 激活函数的全连接网络对学生的表现进行预测。<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618243682620-a7630c13-6807-4f1a-9fea-8b4e3fa3591e.png#align=left&display=inline&height=32&margin=%5Bobject%20Object%5D&name=image.png&originHeight=32&originWidth=157&size=2119&status=done&style=none&width=157" alt="image.png"><br>其中 pi 是标量，表示学生对练习 ei 正确回答的概率，Fi 是 F 的第 i 行，Sigmoid(Z)=1/(1+e−z)<br>网络训练：训练的目标是最小化模型下观察到的学生反应序列的负对数似然率。通过最小化 pt 和 rt 之间的交叉熵损失来学习参数。<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618243689528-a0885321-691c-48e9-a2b0-c050a52bbbaa.png#align=left&display=inline&height=34&margin=%5Bobject%20Object%5D&name=image.png&originHeight=34&originWidth=248&size=2747&status=done&style=none&width=248" alt="image.png"></p><h1 id="3-EXPERIMENTAL-SETTINGS-实验环境"><a href="#3-EXPERIMENTAL-SETTINGS-实验环境" class="headerlink" title="3. EXPERIMENTAL SETTINGS 实验环境"></a>3. EXPERIMENTAL SETTINGS 实验环境</h1><h2 id="3-1-Datasets-数据集"><a href="#3-1-Datasets-数据集" class="headerlink" title="3.1 Datasets 数据集"></a>3.1 Datasets 数据集</h2><p>Synthetic：该数据集是通过模拟 4000 名虚拟学生的答题轨迹得到的。每个学生回答相同顺序的 50 个练习，这些练习取自 5 个难度不同的虚拟概念。<br>ASSISTment 2009(ASSIST2009)：此数据集由 ASSISTment 在线辅导平台提供，广泛用于 KT 任务。我们在更新后的“技能构建者”数据集上进行了实验。该数据集是稀疏的，因为该数据集的密度为 0.06，如表 2 所示。<br>ASSISTment 2015(ASSIST2015)：ASSISTment 2015 包含学生对百项技能的回答。共有 19917 名学生和 708631 个互动。虽然此数据集中的记录数量多于 ASSISTment 2009，但每个学生的平均记录数量较少，因为学生数量较多。此数据集是所有可用数据集中最稀疏的，密度为 0.05。<br>ASSISTment Challenges(ASSISTChall)：此数据来自 ASSISTment 2017 大赛。就互动次数而言，它是最丰富的数据集，有 942,816 个互动，686 名学生和 102 个技能。此数据集是所有可用数据集中密度最高的数据集，因为其密度为 0.81。<br>所有数据集的完整统计信息可以在表 2 中找到。<br>STATICS2011(Statics)：此数据集包含工程静力学课程的交互，包含 189,927 个交互、333 名学生和 1223 个技能标签。我们采用了文献[11]中经过处理的数据。它也是一个密度为 0.31 的密集数据集。<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618243696928-9f79ec27-b0cc-49fd-8d32-de2331cb79b8.png#align=left&display=inline&height=137&margin=%5Bobject%20Object%5D&name=image.png&originHeight=137&originWidth=338&size=16095&status=done&style=none&width=338" alt="image.png"><br>与#Users, #Skill tags 和#Interactions 对应的列分别代表学生数量、练习标签总数和记录数量。列密度 Density 表示每个数据集的密度,即， Density = #Unique Interactions/(#Users ×#Skill tags)       密度=#唯一交互/(#用户 ×#技能标签))。</p><h2 id="3-2-Evaluation-Methodology-评价方法"><a href="#3-2-Evaluation-Methodology-评价方法" class="headerlink" title="3.2 Evaluation Methodology 评价方法"></a>3.2 Evaluation Methodology 评价方法</h2><p><strong>Metrics 度量：</strong>预测任务在二进制分类设置中考虑，即正确或不正确地回答练习。因此，我们使用曲线下面积(AUC)度量来比较性能。<br><strong>Approaches 方法：</strong>我们将我们的模型与最先进的 KT 方法 DKT[6]、DKT+[10]和 DKVMN[11]进行比较。这些方法在引言中有介绍。<br><strong>Model Training and parameter selection 模型训练和参数选择：</strong>我们用 80%的数据集对模型进行训练，并在剩余的数据集上进行测试。对于所有的方法，我们尝试了隐藏状态维数 d={50,100,150,200}。对于相互竞争比较的方法，我们使用了与它们各自的论文中报告的相同的超参数。对于权重的初始化和优化，我们使用了与[10]类似的过程。我们使用 TensorFlow 实现了 SAKT，并使用了学习率为 0.001 的 ADAM[5]优化器。我们对 ASSISTChall 数据集使用批处理大小 256，对其他数据集使用批处理大小 128。对于记录数量较多的数据集，例如 ASSISTChall 和 ASSIST2015，我们使用的 dropout rate（丢弃率）为 0.2，而对于其余数据集，我们使用的 dropout rate 为 0.2。【论文中对于不同数据集使用的 dropout rate 描述似乎有错误】我们设置序列的最大长度，n 与每个学生的平均练习标签大致成正比。对于 ASSISTChall 和 Statics 数据集，我们使用 n=500；对于 ASSIST2009，n=100；对于合成数据集和 ASSIST2015 数据集，n 设置为 50。</p><h1 id="4-RESULTS-AND-DISCUSSION-结果与讨论"><a href="#4-RESULTS-AND-DISCUSSION-结果与讨论" class="headerlink" title="4. RESULTS AND DISCUSSION 结果与讨论"></a>4. RESULTS AND DISCUSSION 结果与讨论</h1><p><strong>Student Performance Prediction 学生表现预测：</strong>表 3 显示了 SAKT 与当前最先进方法的性能比较。在合成数据集上，SAKT 的表现优于竞争对手的方法，AUC 值为 0.832，而 DKT+的 AUC 值为 0.824。尽管合成数据集是密度最高的数据集，但 SAKT 的性能优于基于 RNN 的方法，这是因为生成合成数据所使用的方法。对于此数据集，每个单独的练习仅派生自一个概念。使用项目反应理论[8]确定学生正确回答该数据集中习题的概率为：<img src="https://cdn.nlark.com/yuque/__latex/9f4011ace72797ecb6c816d8a86bec54.svg#card=math&code=p%28correct%7C%CE%B1%EF%BC%8C%CE%B2%29%3Dc%20%2B%20%5Cfrac%20%7B1-c%7D%7B1%2Bexp%28%CE%B2-%CE%B1%29%7D&height=43&width=280">，其中 c 表示正确猜测的概率，α 和 β 是随机选择的数字，分别表示概念能力和练习难度。 因此，在此数据集中，属于同一概念的练习具有很强的相关性。与其他基准不同，SAKT 直接尝试识别属于同一概念的练习，因此比其他方法执行得更好。在 ASSIST2009 上，SAKT 的性能好于被用来比较的其他方法，比第二好的方法获得了 3.16%的性能提升。对于 ASSIST2015 数据集，SAKT 显示出 15.87%的令人印象深刻的改进。我们将这一收益归因于这样一个事实，即 SAKT 利用的注意力机制即使在数据集稀疏的情况下也能很好地学习和概括，ASSIST2015 就是这种情况，因为它的密度是其他数据集中最小的。对于 STATICS2011，与 DKT+相比，我们的方法获得了 2.16%的性能提升。对于 ASSISTChall，我们的方法的性能与 DKT 相当。这可以归因于 ASSISTChall 是所有实际数据集中密度最高的数据集。<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618244005161-f0c4fcc4-c77e-4267-9563-d03b3c9d4ce7.png#align=left&display=inline&height=190&margin=%5Bobject%20Object%5D&name=image.png&originHeight=190&originWidth=366&size=20422&status=done&style=none&width=366" alt="image.png"><br>1、粗体数字是最佳性能。2、通过对每个数据集分别进行最佳超参数选择来获得报告结果。<br><strong>Attention weights visualization 注意权重可视化：</strong>可视化过去交互的要素（用作键）和学生接下来要解决的练习（用作查询）之间的注意权重可以帮助理解过去的交互中的哪些练习与查询练习相关。以此动机为基础，我们可以计算所有运动对（e1，e2）在所有序列中的注意权重之和，其中 e1 作为查询，与运动 e2 的交互作为键。然后，我们对注意力权重进行归一化，以使每个查询的权重之和为 1。这将产生一个相关矩阵，其中每个元素（e1，e2）表示 e2 对 e1 的影响。我们对 Synthetic 进行分析，因为此数据集是用已知的隐藏概念生成的，因此关于不同练习的相关性的基本事实对我们来说是已知的。图 3a 显示了与 Synthetic 中练习的相关性矩阵相对应的热图。对于合成，所有序列均由相同的序列（从 1 到 50）中的所有运动标签组成。<br>为了构建练习标签之间的影响图，如图 3b 所示，我们使用相关性矩阵。首先，我们提取出属于每个隐含概念的序列中的第一个练习，并访问关联矩阵的每一行，并将该行对应的练习与按边权重排序的前两个练习相关联，边权重与两个练习之间的注意力权重成正比。我们可以看到，在注意力权重的基础上，我们能够实现基于隐含概念的运动标签的完美聚类。一个有趣的观察是，两个在序列中相隔很远但属于同一个概念的练习可以被 SAKT 识别出来。例如，如图 3b 所示，对练习 22 的查询将大部分权重分配给练习 5 的键，即使它们在序列中出现的距离很远。<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618244015512-705b0f4d-e9d2-46f3-98ae-28ae140d13de.png#align=left&display=inline&height=282&margin=%5Bobject%20Object%5D&name=image.png&originHeight=282&originWidth=304&size=40234&status=done&style=none&width=304" alt="image.png"><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618244021451-2e2b507b-7cd9-4b3f-b20d-25a802d6dfb6.png#align=left&display=inline&height=152&margin=%5Bobject%20Object%5D&name=image.png&originHeight=152&originWidth=305&size=39943&status=done&style=none&width=305" alt="image.png"><br>(a)每组练习之间的注意权重热图。注意，分配给 pair (i, j)的权重，其中 j &gt; i 总是 0，因为所有的序列都由相同的顺序的练习组成<br>(b)描绘练习之间相关性的图表。相关性是由使用 SAKT 在练习之间学习的注意力权重来确定的。我们观察到潜在概念的完美群集。<br>图 3：可视化合成数据集的注意力权重。<br>       两个相互关联的练习往往具有较高的注意力权重，因为其中一个练习的表现会影响另一个练习的表现。此外，在现实世界的场景中，按顺序发生的练习往往属于同一概念。因此，我们预计注意力权重偏向于交互序列中最近发生的练习。为了说明这一点，我们手动分析了 ASSIST2009 数据集，以可视化一些选定样本的注意力权重。表 4 显示了一些练习，以及过去的交互和分配给每个交互的注意力权重。<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618244030196-261ac239-836a-4a7b-8396-008de988a7ef.png#align=left&display=inline&height=137&margin=%5Bobject%20Object%5D&name=image.png&originHeight=137&originWidth=643&size=37821&status=done&style=none&width=643" alt="image.png"><br>1、  与练习标签对应的列是指查询(即，我们必须预测学生的表现的练习)，而过去的交互分别是指为该学生观察到的交互序列。<br>2、  右栏中的红色元素表示过去交互元素中最重要的元素。<br><strong>Ablation Study 消融研究：</strong>表 5 显示了默认 SAKT 架构和所有数据集(d=200)上的所有变体的性能。<br>没有位置编码 No Positional Encoding (PE):在这个默认架构的变体中，我们删除了位置编码。因此，用于预测学生在某一特定练习中的表现的注意权重仅取决于交互嵌入，而不受其在序列中的位置的影响。在 ASSIST2009 和 ASSIST2015 中，数据集是稀疏的，因此与 ASSISTChall 和 STATICS 等密集数据集相比，去除 PE 的影响不太明显。<br>无残差连接 No Residual Connection (RC):RCs 显示了低水平特征的重要性，即在进行预测时的交互嵌入。由于我们的架构不是很深入，RC 对模型的性能贡献不大。事实上，对于 ASSIST2015 数据集，删除残差连接可以提供比默认设置更好的性能。<br>No Dropout 无丢弃层：在神经网络中引入 Dropout，对模型进行正则化处理，使其具有更好的泛化能力。与模型参数数量相比，模型的过拟合问题对于记录数较少的数据集更为有效。因此，对于 ASSIST2009 数据集和 STATICS 数据集，Dropout 的作用更为有效。<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618244080687-4e571912-4f4b-444b-9197-c460dba0af41.png#align=left&display=inline&height=213&margin=%5Bobject%20Object%5D&name=image.png&originHeight=213&originWidth=362&size=20932&status=done&style=none&width=362" alt="image.png"><br>Single head 单头：我们尝试使用仅一个头的变体，而不是像默认体系结构那样使用 5 个头。多头有助于捕获不同子空间中的注意力权重。使用单头始终会降低 SAKT 在所有数据集上的性能。<br>No block 无注意力块：当不使用自我注意块时，对下一练习的预测仅取决于最后一次互动。可以看出，在没有注意阻塞的情况下，系统的性能比默认架构下的性能要差得多。<br>2 Blocks 2 个注意力模块：增加自我关注块的数量会增加模型的参数数量。然而，在我们的例子中，这种参数的增加被证明对改进性能没有用处。这是因为预测学生在习题中的表现的一个重要方面取决于他在过去相关习题中的表现。再增加一块自我关注会使模型变得更加复杂。<br><strong>Training efficiency 训练效率：</strong>图 4 根据训练阶段在 GPU 上的运行时间展示了各种方法的效率。对比计算效率，SAKT 在一个 epoch 中只花费 1.4 秒，比 DKT+(65 秒/epoch)所花费的时间少 46.42 秒，比 DKT(45 秒/epoch)所花费的时间少 32 倍，比 DKVMN(26 秒/epoch)所花费的时间少 17.33 倍。我们在单个 NVIDIA Titan V 型的 GPU 上进行了实验。<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618244089705-411525f1-a6a3-4a73-b6f9-47f5a79fb192.png#align=left&display=inline&height=209&margin=%5Bobject%20Object%5D&name=image.png&originHeight=209&originWidth=298&size=25763&status=done&style=none&width=298" alt="image.png"></p><h1 id="5-CONCLUSION-AND-FUTURE-WORK-结论与未来工作"><a href="#5-CONCLUSION-AND-FUTURE-WORK-结论与未来工作" class="headerlink" title="5. CONCLUSION AND FUTURE WORK 结论与未来工作"></a>5. CONCLUSION AND FUTURE WORK 结论与未来工作</h1><p>在本研究中，我们提出了一个基于自我注意的知识追踪模型——SAKT。它模拟学生的交互历史(不使用任何 RNN)，并通过考虑学生过去交互的相关练习来预测他在下一个练习中的表现。在各种真实世界的数据集上进行的大量实验表明，我们的模型可以优于最先进的方法，并且比基于 rnn 的方法快一个数量级。</p><h1 id="6-REFERENCES-参考文献"><a href="#6-REFERENCES-参考文献" class="headerlink" title="6. REFERENCES 参考文献"></a>6. REFERENCES 参考文献</h1><p>……</p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;ABSTRACT-摘要&quot;&gt;&lt;a href=&quot;#ABSTRACT-摘要&quot; class=&quot;headerlink&quot; title=&quot;ABSTRACT 摘要&quot;&gt;&lt;/a&gt;ABSTRACT 摘要&lt;/h1&gt;&lt;p&gt;知识追踪是指当每个学生参与一系列学习活动时，对知识概念(KCs)的掌握情况进行建模的任务。每个学生的知识是通过评估学生在学习活动中的表现来建模的。为学生提供个性化的学习平台是一个重要的研究领域。近年来，基于递归神经网络(RNN)的方法，如深度知识追踪(DKT)和动态键值记忆网络(DKVMN)，由于能够捕获人类学习的复杂表示，因而优于所有传统方法。然而，这些方法在处理稀疏数据时面临的问题是不能很好地泛化【研究问题】，这是真实世界数据的情况下，学生与很少的 KCs 交互。为了解决这个问题，我们开发了一种方法，从学生过去的活动中识别与给定 KC 相关的 KC，并根据选择的相对较少的 KC 预测他/她的掌握情况。由于预测是基于相对较少的过去活动，它处理数据稀疏性问题比基于 RNN 的方法更好。为了识别 KCs 之间的相关性，我们提出了一种基于自我注意的方法——自我注意知识追踪(SAKT)。在各种真实世界数据集上的广泛实验表明，我们的模型在知识追踪方面优于最先进的模型，平均提高了 4.43%的 AUC。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;关键词：&lt;/strong&gt;知识追踪，大规模网络开放课程，自我关注，顺序推荐&lt;/p&gt;</summary>
    
    
    
    <category term="知识追踪" scheme="https://ytno1.github.io/categories/%E7%9F%A5%E8%AF%86%E8%BF%BD%E8%B8%AA/"/>
    
    <category term="SAKT" scheme="https://ytno1.github.io/categories/%E7%9F%A5%E8%AF%86%E8%BF%BD%E8%B8%AA/SAKT/"/>
    
    
    <category term="知识追踪" scheme="https://ytno1.github.io/tags/%E7%9F%A5%E8%AF%86%E8%BF%BD%E8%B8%AA/"/>
    
    <category term="SAKT" scheme="https://ytno1.github.io/tags/SAKT/"/>
    
  </entry>
  
  <entry>
    <title>博客DIY清单</title>
    <link href="https://ytno1.github.io/archives/b46b0450.html"/>
    <id>https://ytno1.github.io/archives/b46b0450.html</id>
    <published>2021-04-10T15:07:41.000Z</published>
    <updated>2021-04-12T16:58:25.651Z</updated>
    
    <content type="html"><![CDATA[<h1 id="开始之前"><a href="#开始之前" class="headerlink" title="开始之前"></a>开始之前</h1><p>生命不息，折腾不止！尝试建一个属于自己个人博客，记录美好生活！</p><span id="more"></span><h1 id="配置项"><a href="#配置项" class="headerlink" title="配置项"></a>配置项</h1><ul><li><p>安装目录</p></li><li><p>宠物 or 看板娘</p></li><li><p>头像 旋转等效果</p></li><li><p>分页效果</p></li><li><p>404 页面 使用模板 or DIY</p></li><li><p>访问、阅读统计 （注：部署后正常显示统计数，仅测试时显示为不蒜子网站的统计数）</p></li><li><p>博客分类</p></li><li><p>评论系统</p></li><li><p>音乐、视频插件</p></li><li><p>sitemap 设置</p></li></ul><p>……</p><h1 id="优质教程"><a href="#优质教程" class="headerlink" title="优质教程"></a>优质教程</h1><ul><li>视频教程 <a href="https://www.bilibili.com/video/BV1Yb411a7ty?from=search&seid=6873047497118275353">https://www.bilibili.com/video/BV1Yb411a7ty?from=search&amp;seid=6873047497118275353</a></li><li>文字教程 <a href="https://segmentfault.com/a/1190000020382983">https://segmentfault.com/a/1190000020382983</a></li><li>模板 <a href="http://litten.me/">http://litten.me/</a></li><li>主题配置优化 <a href="http://dongshuyan.com/2019/05/24/hexo%25E5%258D%259A%25E5%25AE%25A2%25E6%25B3%25A8%25E6%2584%258F%25E4%25BA%258B%25E9%25A1%25B9/">http://dongshuyan.com/2019/05/24/hexo%E5%8D%9A%E5%AE%A2%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9/</a></li><li>语雀文章同步到 heox 博客 <a href="https://luan.ma/post/yuque2blog/">https://luan.ma/post/yuque2blog/</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;开始之前&quot;&gt;&lt;a href=&quot;#开始之前&quot; class=&quot;headerlink&quot; title=&quot;开始之前&quot;&gt;&lt;/a&gt;开始之前&lt;/h1&gt;&lt;p&gt;生命不息，折腾不止！尝试建一个属于自己个人博客，记录美好生活！&lt;/p&gt;</summary>
    
    
    
    <category term="经验帖" scheme="https://ytno1.github.io/categories/%E7%BB%8F%E9%AA%8C%E5%B8%96/"/>
    
    <category term="blog优化" scheme="https://ytno1.github.io/categories/%E7%BB%8F%E9%AA%8C%E5%B8%96/blog%E4%BC%98%E5%8C%96/"/>
    
    
    <category term="经验帖" scheme="https://ytno1.github.io/tags/%E7%BB%8F%E9%AA%8C%E5%B8%96/"/>
    
    <category term="blog优化" scheme="https://ytno1.github.io/tags/blog%E4%BC%98%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>教师资格证考试</title>
    <link href="https://ytno1.github.io/archives/5ddaa770.html"/>
    <id>https://ytno1.github.io/archives/5ddaa770.html</id>
    <published>2021-04-10T14:55:46.000Z</published>
    <updated>2021-04-12T16:58:25.683Z</updated>
    
    <content type="html"><![CDATA[<h1 id="开始之前"><a href="#开始之前" class="headerlink" title="开始之前"></a>开始之前</h1><p>打算从事教师职业的话，还是尽早在在校期间把教师资格证拿到手，要简单很多。</p><h1 id="笔试"><a href="#笔试" class="headerlink" title="笔试"></a>笔试</h1><h2 id="综合素质"><a href="#综合素质" class="headerlink" title="综合素质"></a>综合素质</h2><ul><li>选择题很重要，把题库刷完，⼤多挂在选择题上，可以⽤粉笔⽹等题库（三部分选择题加起来⼤约 2/3 千道选择题）软件（喜欢⽤纸质试卷刷题就⽤试卷），有很多，质量都可以</li><li>中公教育里的试卷或者网上整理的 pdf，应⽤题考的永远是那⼏个题，背下来就好了</li></ul><span id="more"></span><h2 id="教育知识与能力"><a href="#教育知识与能力" class="headerlink" title="教育知识与能力"></a>教育知识与能力</h2><ul><li>同样选择题很重要，将题库刷完</li><li>应⽤题有⼤致范围，可以准备⼀份资料去背，总之多写没错，⾔之有理就有分</li></ul><h2 id="学科知识与教学能⼒"><a href="#学科知识与教学能⼒" class="headerlink" title="学科知识与教学能⼒"></a>学科知识与教学能⼒</h2><ul><li>第三门真的好过，大部分⼈第三门课没复习都过了(都是本专业的，跨科的另讲)，反而前两门公共课没过，因此精力放在前两⻔，多刷选择题，背应⽤题</li></ul><h1 id="⾯试"><a href="#⾯试" class="headerlink" title="⾯试"></a>⾯试</h1><ul><li>学校老师⾯试，看你很亲切，不会卡你，⽐较好过，但也别掉以轻⼼</li><li>推荐粉笔⽹的⼀些⾯试教程，会有收获</li></ul><h1 id="资料分享"><a href="#资料分享" class="headerlink" title="资料分享"></a>资料分享</h1><ul><li>提纲版链接: <a href="https://pan.baidu.com/s/1fFjNq2UPQe9WhoPV0rD7dA">https://pan.baidu.com/s/1fFjNq2UPQe9WhoPV0rD7dA</a> 提取码: clfj</li><li>详细资料链接: <a href="https://pan.baidu.com/s/1kIR3c5j6V-">https://pan.baidu.com/s/1kIR3c5j6V-</a> SsdvhzK9rhkA 提取码: 64wg</li><li>粉笔网课程链接: <a href="https://pan.baidu.com/s/14QcXW99sXpXtHRMk2gULWQ">https://pan.baidu.com/s/14QcXW99sXpXtHRMk2gULWQ</a> 提取码: ddv7</li><li>信息技术教材链接: <a href="https://pan%20.baidu.com/s/">https://pan .baidu.com/s/</a> 1VazhG0F Gnwl8Wr22ydjJg 提取码: pvmz</li></ul>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;开始之前&quot;&gt;&lt;a href=&quot;#开始之前&quot; class=&quot;headerlink&quot; title=&quot;开始之前&quot;&gt;&lt;/a&gt;开始之前&lt;/h1&gt;&lt;p&gt;打算从事教师职业的话，还是尽早在在校期间把教师资格证拿到手，要简单很多。&lt;/p&gt;
&lt;h1 id=&quot;笔试&quot;&gt;&lt;a href=&quot;#笔试&quot; class=&quot;headerlink&quot; title=&quot;笔试&quot;&gt;&lt;/a&gt;笔试&lt;/h1&gt;&lt;h2 id=&quot;综合素质&quot;&gt;&lt;a href=&quot;#综合素质&quot; class=&quot;headerlink&quot; title=&quot;综合素质&quot;&gt;&lt;/a&gt;综合素质&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;选择题很重要，把题库刷完，⼤多挂在选择题上，可以⽤粉笔⽹等题库（三部分选择题加起来⼤约 2/3 千道选择题）软件（喜欢⽤纸质试卷刷题就⽤试卷），有很多，质量都可以&lt;/li&gt;
&lt;li&gt;中公教育里的试卷或者网上整理的 pdf，应⽤题考的永远是那⼏个题，背下来就好了&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    <category term="经验帖" scheme="https://ytno1.github.io/categories/%E7%BB%8F%E9%AA%8C%E5%B8%96/"/>
    
    <category term="教师资格证" scheme="https://ytno1.github.io/categories/%E7%BB%8F%E9%AA%8C%E5%B8%96/%E6%95%99%E5%B8%88%E8%B5%84%E6%A0%BC%E8%AF%81/"/>
    
    
    <category term="经验帖" scheme="https://ytno1.github.io/tags/%E7%BB%8F%E9%AA%8C%E5%B8%96/"/>
    
    <category term="教师资格证" scheme="https://ytno1.github.io/tags/%E6%95%99%E5%B8%88%E8%B5%84%E6%A0%BC%E8%AF%81/"/>
    
  </entry>
  
  <entry>
    <title>Linux常用命令记录</title>
    <link href="https://ytno1.github.io/archives/30a497b6.html"/>
    <id>https://ytno1.github.io/archives/30a497b6.html</id>
    <published>2021-04-10T14:14:34.000Z</published>
    <updated>2021-04-12T16:58:36.213Z</updated>
    
    <content type="html"><![CDATA[<h1 id="开始之前"><a href="#开始之前" class="headerlink" title="开始之前"></a>开始之前</h1><p>在当前的学习生涯中，Linux 服务器主要是用来跑 ML、DL 代码的，所以记录一下自己利用服务器跑实验经常需要用到的命令。</p><span id="more"></span><h1 id="虚拟环境"><a href="#虚拟环境" class="headerlink" title="虚拟环境"></a>虚拟环境</h1><p>好处：1、能够使不同开发环境独立，环境升级不影响其他应用、环境，可以防止系统中出现包管理混乱和版本的冲突。2、深度学习论文的源代码有环境、各种包的版本要求，利用虚拟环境配置一个代码所需的干净环境。</p><h2 id="方式-1-virtualenvwrapper-创建环境"><a href="#方式-1-virtualenvwrapper-创建环境" class="headerlink" title="方式 1 virtualenvwrapper 创建环境"></a>方式 1 virtualenvwrapper 创建环境</h2><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install virtualenvwrapper-win</span><br><span class="line">pip install virtualenvwrapper        <span class="comment"># linux环境</span></span><br></pre></td></tr></table></figure><p>创建虚拟环境 mkvirtualenv test1(虚拟环境名称)</p><h3 id="设置-WORK-HOME-环境变量"><a href="#设置-WORK-HOME-环境变量" class="headerlink" title="设置 WORK_HOME 环境变量"></a>设置 WORK_HOME 环境变量</h3><h3 id="创建、查看、激活等操作"><a href="#创建、查看、激活等操作" class="headerlink" title="创建、查看、激活等操作"></a>创建、查看、激活等操作</h3><ul><li>选择一个 python 解释器来搭建：mkvirtualenv env –python=python2.7</li><li>查看虚拟环境 lsvirtualenv 或者 workon</li><li>进入虚拟环境 workon test1(虚拟环境名称)</li><li>退出虚拟环境 deactivate</li><li>删除虚拟环境 rmvirtualenv test1(虚拟环境名称)</li><li>查看虚拟环境下的安装包 pip list</li></ul><h2 id="方式-2-conda-创建虚拟环境"><a href="#方式-2-conda-创建虚拟环境" class="headerlink" title="方式 2 conda 创建虚拟环境"></a>方式 2 conda 创建虚拟环境</h2><h3 id="安装、配置-Anaconda"><a href="#安装、配置-Anaconda" class="headerlink" title="安装、配置 Anaconda"></a>安装、配置 Anaconda</h3><p>网上教程很多，直接搜即可</p><h3 id="创建、激活虚拟环境等操纵"><a href="#创建、激活虚拟环境等操纵" class="headerlink" title="创建、激活虚拟环境等操纵"></a>创建、激活虚拟环境等操纵</h3><ul><li>查看安装的包 conda list</li><li>查看存在的虚拟环境 conda env list 或 conda info -e</li><li>检查更新 conda conda update conda</li><li>创建虚拟环境  conda create -n your_env_name python=X.X（2.7、3.6 等) 创建 python 版本为 X.X、名字为 your_env_name 的虚拟环境。your_env_name 文件可以在 Anaconda 安装目录 envs 文件下找到。</li><li>激活虚拟环境 conda activate yut</li><li>安装包 conda install [package]</li><li>关闭虚拟环境    Linux: source deactivate   Windows: deactivate</li><li>删除虚拟环境 conda remove -n your_env_name(虚拟环境名称) –all， 即可删除。</li><li>删除环境中的某个包 conda remove –name your_env_name  package_name 。</li></ul><h1 id="服务器中常用命令"><a href="#服务器中常用命令" class="headerlink" title="服务器中常用命令"></a>服务器中常用命令</h1><h2 id="命令行快捷操作"><a href="#命令行快捷操作" class="headerlink" title="命令行快捷操作"></a>命令行快捷操作</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">ctrl + e &#x2F;&#x2F; 光标回到行末</span><br><span class="line">ctrl + k &#x2F;&#x2F; 删除光标处到行尾的字符</span><br><span class="line">ctrl + u &#x2F;&#x2F; 删除命令行的整段命令</span><br><span class="line">ctrl + y &#x2F;&#x2F; 恢复上一次删除内容</span><br><span class="line">nvidia-smi  # 查看GPU使用情况</span><br><span class="line">ps aux  查看进程情况</span><br><span class="line">kill  -9 pid  # 关掉相应进程</span><br><span class="line"># 后台运行</span><br><span class="line">## nohup命令</span><br><span class="line">  nohup python -u RKT.py [Parameters] &gt; RKT.log 2&gt;&amp;1 &amp;  # 挂后台</span><br><span class="line">  tail -f RKT.log  # 查看输出情况</span><br><span class="line">## screen命令 开一个会话的同时时创建多个窗口处理不同的任务</span><br><span class="line">  screen -S test &#x2F;&#x2F;创建一个名为test的新窗口</span><br><span class="line">  ctrl + a + d 断开窗口的连接回到会话界面，注意：只是断开了窗口并未终止任务的运行</span><br><span class="line">  screen -ls &#x2F;&#x2F;显示所有窗口</span><br><span class="line">  screen -r test &#x2F;&#x2F;返回test窗口</span><br><span class="line">  ctrl + d  &#x2F;&#x2F; 断开某个窗口</span><br></pre></td></tr></table></figure><h2 id="vim-快捷操作"><a href="#vim-快捷操作" class="headerlink" title="vim 快捷操作"></a>vim 快捷操作</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">Esc &#x2F;&#x2F; 从当前模式转换到“普通模式”。</span><br><span class="line">i &#x2F;&#x2F; “插入模式”用于插入文字。</span><br><span class="line"></span><br><span class="line">:  &#x2F;&#x2F;“命令行模式” Vim 希望你输入类似于保存该文档命令的地方。例如：</span><br><span class="line">:q&#x2F;&#x2F; 退出 Vim，如果文件已被修改，将退出失败</span><br><span class="line">    :wq &#x2F;&#x2F; 保存文件并退出 Vim</span><br><span class="line"></span><br><span class="line">gg  &#x2F;&#x2F; 将光标移动到文档开头</span><br><span class="line">G  &#x2F;&#x2F; 将光标移动到文档末尾</span><br><span class="line">$  &#x2F;&#x2F; 将光标移动到本行尾</span><br><span class="line">0  &#x2F;&#x2F; 将光标移动到本行行首</span><br><span class="line">ndd  &#x2F;&#x2F; 删除n行(如10+dd)</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;开始之前&quot;&gt;&lt;a href=&quot;#开始之前&quot; class=&quot;headerlink&quot; title=&quot;开始之前&quot;&gt;&lt;/a&gt;开始之前&lt;/h1&gt;&lt;p&gt;在当前的学习生涯中，Linux 服务器主要是用来跑 ML、DL 代码的，所以记录一下自己利用服务器跑实验经常需要用到的命令。&lt;/p&gt;</summary>
    
    
    
    <category term="Linux" scheme="https://ytno1.github.io/categories/Linux/"/>
    
    
    <category term="Linux" scheme="https://ytno1.github.io/tags/Linux/"/>
    
    <category term="常用命令" scheme="https://ytno1.github.io/tags/%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"/>
    
  </entry>
  
  <entry>
    <title>服务器跑实验-有用的代码</title>
    <link href="https://ytno1.github.io/archives/65ba0542.html"/>
    <id>https://ytno1.github.io/archives/65ba0542.html</id>
    <published>2021-04-10T08:14:57.000Z</published>
    <updated>2021-04-12T16:58:25.804Z</updated>
    
    <content type="html"><![CDATA[<h1 id="开始之前"><a href="#开始之前" class="headerlink" title="开始之前"></a>开始之前</h1><p>后期跑实验经常需要同时跑几条命令，使用脚本可以简化操作。</p><h1 id="查看知识追踪领域数据集的情况"><a href="#查看知识追踪领域数据集的情况" class="headerlink" title="查看知识追踪领域数据集的情况"></a>查看知识追踪领域数据集的情况</h1><p>知识追踪数据集中的数据通常以三行形式呈现，每三行代表一个学生的做题情况，第一行表示做题数，第二行表示学生所做题目的 ID，第三行则表示学生的作答情况。如下方所示：<br>192            <br>742 504 1731 1955 559 2015 2944 2170 2051 1153 1046 607<br>0 0 0 0 0 0 0 0 0 1 0 0</p><span id="more"></span><h2 id="数据集问题总数以及不同的题数"><a href="#数据集问题总数以及不同的题数" class="headerlink" title="数据集问题总数以及不同的题数"></a>数据集问题总数以及不同的题数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> csv</span><br><span class="line">rows = csv.reader(<span class="built_in">open</span>(<span class="string">&#x27;./slepemapy/slepemapy/train_valid.csv&#x27;</span>), delimiter=<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">rows = [[<span class="built_in">int</span>(e) <span class="keyword">for</span> e <span class="keyword">in</span> row <span class="keyword">if</span> e != <span class="string">&#x27;&#x27;</span>] <span class="keyword">for</span> row <span class="keyword">in</span> rows]</span><br><span class="line"><span class="comment"># print(rows)</span></span><br><span class="line">q_num=<span class="number">0</span>, count=<span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> subq_num <span class="keyword">in</span> rows[<span class="number">0</span>::<span class="number">3</span>]:</span><br><span class="line">    <span class="comment"># print(subq_num[0])</span></span><br><span class="line">    q_num+=subq_num[<span class="number">0</span>]</span><br><span class="line">    count+=<span class="number">1</span></span><br><span class="line">print(<span class="string">&quot;训练验证集样本数：&quot;</span>,count,<span class="string">&quot;训练验证集问题总数：&quot;</span>,q_num)</span><br><span class="line"></span><br><span class="line">q_rows = []</span><br><span class="line">unique_qnum=<span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> q_row <span class="keyword">in</span> rows[<span class="number">1</span>::<span class="number">3</span>]:</span><br><span class="line">    <span class="comment">#  print(q_row)</span></span><br><span class="line">    q_rows.extend(q_row)</span><br><span class="line"><span class="comment">#     print(subqnum)</span></span><br><span class="line"><span class="comment">#     unique_qnum+=subqnum</span></span><br><span class="line"><span class="comment"># print(unique_qnum)</span></span><br><span class="line">print(<span class="string">&quot;训练验证集中不相同题目的数量：&quot;</span>,<span class="built_in">len</span>(<span class="built_in">set</span>(q_rows)))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">训练验证集样本数： 14535 训练验证集问题总数： 1060408</span><br><span class="line">训练验证集中不相同题目的数量： 1683</span><br></pre></td></tr></table></figure><h2 id="查看数据集的大致情况（前五行）"><a href="#查看数据集的大致情况（前五行）" class="headerlink" title="查看数据集的大致情况（前五行）"></a>查看数据集的大致情况（前五行）</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过读取 csv 文件创建 DataFrame</span></span><br><span class="line">df = pd.read_csv(<span class="string">&#x27;./train_valid.csv&#x27;</span>, header=<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看所有行</span></span><br><span class="line">print(df)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看前 5 行</span></span><br><span class="line">print(df.head())</span><br><span class="line">print(df.head(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看后 5 行</span></span><br><span class="line">print(df.tail())</span><br><span class="line">print(df.tail(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看行标签(index)，列标签(columns)和数据</span></span><br><span class="line">print(df.index)</span><br><span class="line">print(df.index.names)</span><br><span class="line">print(df.columns)</span><br><span class="line">print(df.values)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看基本统计信息</span></span><br><span class="line">print(df.info())</span><br><span class="line">print(df.describe())</span><br></pre></td></tr></table></figure><h1 id="conda-常用的命令。"><a href="#conda-常用的命令。" class="headerlink" title="conda 常用的命令。"></a>conda 常用的命令。</h1><p>1）conda list 查看安装了哪些包。<br>   2）conda env list 或 conda info -e 查看当前存在哪些虚拟环境<br>   3）conda update conda 检查更新当前 conda<br>使用 conda create -n your_env_name python=X.X（2.7、3.6 等)命令创建 python 版本为 X.X、名字为 your_env_name 的虚拟环境。your_env_name 文件可以在 Anaconda 安装目录 envs 文件下找到。</p><h1 id="shell-脚本实例"><a href="#shell-脚本实例" class="headerlink" title="shell 脚本实例"></a>shell 脚本实例</h1><p>#!/bin/bash         # ‘#!’是一个约定的标记，它告诉系统这个脚本需要什么解释器来执行，即使用哪一种 Shell。<br>echo “Hello World !”           # echo 命令用于向窗口输出文本。</p><h2 id="运行-Shell-脚本有两种方法："><a href="#运行-Shell-脚本有两种方法：" class="headerlink" title="运行 Shell 脚本有两种方法："></a>运行 Shell 脚本有两种方法：</h2><p>1、作为可执行程序<br>将上面的代码保存为 test.sh，并 cd 到相应目录：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">chmod +x .&#x2F;test.sh  #使脚本具有执行权限</span><br><span class="line">.&#x2F;test.sh  #执行脚本</span><br></pre></td></tr></table></figure><p>2、作为解释器参数<br>这种运行方式是，直接运行解释器，其参数就是 shell 脚本的文件名，如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;bin&#x2F;sh test.sh  # 执行shell脚本</span><br></pre></td></tr></table></figure><h1 id="后台运行命令"><a href="#后台运行命令" class="headerlink" title="后台运行命令"></a>后台运行命令</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup .&#x2F;test.sh &gt; myout.log 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure><p>解释：<br>&amp; 放在命令到结尾，表示后台运行，防止终端一直被某个进程占用，这样终端可以执行别到任务<br>nohup 放在命令的开头，表示不挂起（no hang up），也即，关闭终端或者退出某个账号，进程也继续保持运行状态<br>0 表示 stdin 标准输入<br>1 表示 stdout 标准输出<br>2 表示 stderr 标准错误<br>2&gt;&amp;1 也就表示将错误重定向到标准输出上</p><h1 id="Linux-查看-Nvidia-显卡信息及使用情况"><a href="#Linux-查看-Nvidia-显卡信息及使用情况" class="headerlink" title="Linux 查看 Nvidia 显卡信息及使用情况"></a>Linux 查看 Nvidia 显卡信息及使用情况</h1><p>Nvidia 自带一个命令行工具可以查看显存的使用情况：nvidia-smi<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618047738203-64b279b8-9d50-4ec4-9dd5-fecee11ca35a.png#align=left&display=inline&height=363&margin=%5Bobject%20Object%5D&name=image.png&originHeight=363&originWidth=641&size=32085&status=done&style=none&width=641" alt="image.png"><br><img src="https://cdn.nlark.com/yuque/0/2021/jpeg/12870328/1618048894016-633fef22-e1b7-4f07-b0bf-08dbf9b04f59.jpeg#align=left&display=inline&height=362&margin=%5Bobject%20Object%5D&originHeight=362&originWidth=600&size=0&status=done&style=none&width=600"><br>注：此图片来源于网络<br>表头释义：</p><p>Fan：显示风扇转速，数值在 0 到 100%之间，是计算机的期望转速，如果计算机不是通过风扇冷却或者风扇坏了，显示出来就是 N/A；  <br>Temp：显卡内部的温度，单位是摄氏度；<br>Perf：表征性能状态，从 P0 到 P12，P0 表示最大性能，P12 表示状态最小性能；<br>Pwr：能耗表示；上方的 Persistence-M：是持续模式的状态，持续模式虽然耗能大，但是在新的 GPU 应用启动时，花费的时间更少，这里显示的是 off 的状态。  <br>Bus-Id：涉及 GPU 总线的相关信息；格式为：domain:bus:device.function<br>Disp.A：是 Display Active 的意思，表示 GPU 的显示是否初始化；  <br>Memory Usage：显存的使用率；  <br>Volatile GPU-Util：浮动的 GPU 利用率；<br>ECC：是否开启错误检查和纠正技术；0/DISABLED, 1/ENABLED<br>Compute M.：计算模式；0/DEFAULT,1/EXCLUSIVE_PROCESS,2/PROHIBITED<br>下边的 Processes 显示每块 GPU 上每个进程所使用的显存情况。<br>注：<strong>显存占用和 GPU 占用是两个不一样的东西</strong>，显卡是由 GPU 和显存等组成的，显存和 GPU 的关系有点类似于内存和 CPU 的关系。</p><p>如果要周期性的输出显卡的使用情况，可以用 watch 指令实现：watch -n 10 nvidia-smi，命令行参数-n 后边跟的是执行命令的周期，以 s 为单位。</p><h1 id="脚本——并行跑几条命令"><a href="#脚本——并行跑几条命令" class="headerlink" title="脚本——并行跑几条命令"></a>脚本——并行跑几条命令</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">num=1</span><br><span class="line"><span class="keyword">for</span> hs <span class="keyword">in</span> 50 100 150 200;</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">for</span> bs <span class="keyword">in</span> 32 64 128 256;</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">for</span> ds <span class="keyword">in</span> assist2015 assist2017 slepemapy eanalyst;</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="comment"># echo &quot;$&#123;num&#125; $&#123;ds&#125;_hidden$&#123;hs&#125;_batch$&#123;bs&#125;&quot;</span></span><br><span class="line">            python SAKT.py --isServer 1 --train_dataset <span class="string">&#x27;$&#123;ds&#125;&#x27;</span> --state_size <span class="variable">$&#123;hs&#125;</span> --epoch 200 --batch_size <span class="variable">$&#123;bs&#125;</span> &gt;</span><br><span class="line">            <span class="variable">$&#123;ds&#125;</span>_hidden<span class="variable">$&#123;hs&#125;</span>_batch<span class="variable">$&#123;bs&#125;</span>.<span class="built_in">log</span> 2&gt;&amp;1 &amp;</span><br><span class="line"><span class="keyword">if</span> [ `expr <span class="variable">$&#123;num&#125;</span> % 2` == 0 ]</span><br><span class="line"><span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$&#123;num&#125;</span> 每两个停一下，并等待两秒！&quot;</span></span><br><span class="line">sleep 2</span><br><span class="line"><span class="built_in">wait</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line">num=`expr <span class="variable">$&#123;num&#125;</span> + 1`</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"><span class="built_in">wait</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">done</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;开始之前&quot;&gt;&lt;a href=&quot;#开始之前&quot; class=&quot;headerlink&quot; title=&quot;开始之前&quot;&gt;&lt;/a&gt;开始之前&lt;/h1&gt;&lt;p&gt;后期跑实验经常需要同时跑几条命令，使用脚本可以简化操作。&lt;/p&gt;
&lt;h1 id=&quot;查看知识追踪领域数据集的情况&quot;&gt;&lt;a href=&quot;#查看知识追踪领域数据集的情况&quot; class=&quot;headerlink&quot; title=&quot;查看知识追踪领域数据集的情况&quot;&gt;&lt;/a&gt;查看知识追踪领域数据集的情况&lt;/h1&gt;&lt;p&gt;知识追踪数据集中的数据通常以三行形式呈现，每三行代表一个学生的做题情况，第一行表示做题数，第二行表示学生所做题目的 ID，第三行则表示学生的作答情况。如下方所示：&lt;br&gt;192            &lt;br&gt;742 504 1731 1955 559 2015 2944 2170 2051 1153 1046 607&lt;br&gt;0 0 0 0 0 0 0 0 0 1 0 0&lt;/p&gt;</summary>
    
    
    
    <category term="知识追踪" scheme="https://ytno1.github.io/categories/%E7%9F%A5%E8%AF%86%E8%BF%BD%E8%B8%AA/"/>
    
    <category term="跑实验" scheme="https://ytno1.github.io/categories/%E7%9F%A5%E8%AF%86%E8%BF%BD%E8%B8%AA/%E8%B7%91%E5%AE%9E%E9%AA%8C/"/>
    
    
    <category term="知识追踪" scheme="https://ytno1.github.io/tags/%E7%9F%A5%E8%AF%86%E8%BF%BD%E8%B8%AA/"/>
    
    <category term="跑实验" scheme="https://ytno1.github.io/tags/%E8%B7%91%E5%AE%9E%E9%AA%8C/"/>
    
  </entry>
  
  <entry>
    <title>Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting</title>
    <link href="https://ytno1.github.io/archives/afacea32.html"/>
    <id>https://ytno1.github.io/archives/afacea32.html</id>
    <published>2021-04-06T06:03:18.000Z</published>
    <updated>2021-04-12T16:58:36.223Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>现实世界中的许多应用都需要对长序列时间序列进行预测，例如用电量规划。长序列时间序列预测(LSTF)对模型的预测能力提出了很高的要求，即能够有效地捕捉到输出和输入之间精确的长期依赖耦合。最近的研究表明，Transformer 具有提高预测能力的潜力。然而，Transformer 有几个严重的问题阻碍了它直接应用于 LSTF，例如二次时间复杂度、高内存使用率以及编码器-解码器体系结构的固有限制。针对这些问题，我们设计了一种高效的基于 Transformer 的 LSTF 模型 Informer，该模型具有三个显著的特点：(1)ProbSparse 自我注意机制，在时间复杂度和内存使用量上达到 O(LlogL)，在序列依赖比对方面具有相当的性能。(2)自我注意提取通过将级联层输入减半来突出主导注意力，并有效地处理超长输入序列。(3)生成式解码器虽然概念简单，但对长时间序列的预测采用一次正向运算，而不是分步预测，大大提高了长序列预测的推理速度。在四个大规模数据集上的大量实验表明，Informer 的性能明显优于现有方法，为 LSTF 问题提供了一种新的解决方案。</p><span id="more"></span><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>时间序列预测是许多领域的关键组成部分，例如传感器网络监测(Papadimitriou 和 Yu 2006)、能源和智能电网管理、经济和金融(Zhu 和 Shasha 2002)以及疾病传播分析(Matsubara 等人 2014 年)。在这些场景中，我们可以利用大量关于过去行为的时间序列数据来进行长期预测，即长序列时间序列预测(LSTF)。然而，现有的方法是在有限的问题背景下设计的，比如预测 48 点或更少(Hochreiter 和 Schmidhuber 1997；Li 等人 2018 年；Yu 等人 2017 年；Liu 等人 2019 年；Qin 等人 2017 年；Wen 等人 2017 年)。越来越长的序列使模型的预测能力变得紧张，以至于有人认为，这种趋势正在阻碍 LSTF 的研究。作为一个实证例子，图(1)显示了在实际数据集上的预测结果，其中 LSTM 网络预测了变电站从短期(12 点，0.5 天)到长期(480 点，20 天)的每小时温度。当预测长度大于 48 点(图(1(c)中的 ★)时，总体性能差距很大。MSE 分数上升到不能令人满意的程度，推理速度急剧下降，LSTM 模型失效。<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1617693447761-f06caf58-e477-4381-970f-ee1682c4f4ee.png#align=left&display=inline&height=302&margin=%5Bobject%20Object%5D&name=image.png&originHeight=302&originWidth=1065&size=69692&status=done&style=none&width=1065" alt="image.png"><br>图 1：(a)短序列预测只揭示不久的将来。(b)长序列时间序列预测可以涵盖更长的时期，以便更好地进行政策规划和投资保护。(c)现有方法的预测能力限制了长序列的性能，即从 length=48 开始，均方误差上升到不可接受的水平，推理速度迅速下降。</p><p>LSTF 面临的主要挑战是提高预测能力以满足日益增长的长序列需求，这要求(a)特别的长期比对能力和(b)对长序列输入和输出的有效操作。目前，与 RNN 模型相比，Transformer 模型在捕获长期依赖关系方面表现出更好的性能。自注意机制可以将网络信号传输路径的最大长度减少到理论上的最短 O(1)，并且避免了递归结构，从而使 Transformer 在 LSTF 问题上显示出很大的潜力。但另一方面，由于 L-quadratic 计算和 L 长度的输入/输出上的内存消耗，自我注意机制违反了要求(b)。一些大型 Transformer 模型倾注了大量资源，并在 NLP 任务上产生了令人印象深刻的结果(Brown 等人 2020)，但是在几十个 GPU 上的训练和昂贵的部署成本使得这些模型在现实世界的 LSTF 问题上负担不起。自我注意机制和 Transformer 框架的效率成为它们应用于 LSTF 问题的瓶颈。因此，在本文中，我们试图回答这样一个问题：能否改进 Transformer 模型以提高计算、存储和体系结构的效率，同时保持更高的预测能力？</p><p>Vanilla Transformer (Vaswani et al. 2017)在求解 LSTF 时有三个显著的局限性:</p><ol><li><strong>自我注意的平方计算。</strong>自我注意机制的原子运算（atom operation），即规范点积（canonical dot-product），使每层的时间复杂度和内存使用量为 O(L)。</li><li><strong>用于长输入的堆叠层中的内存瓶颈。</strong>由于 J 编码/解码层的堆栈使得总内存使用量为 O(J·L)，限制了模型在接收长序列输入时的可扩展性。</li><li><strong>预测长期输出的速度急转直下。</strong>Vanilla Transformer 的动态解码使得逐步推理的速度与基于 RNN 的模型一样慢，如图 1(c)所示。</li></ol><p>关于提高自我注意效率的研究已经有了一些前人的研究成果。The Sparse Transformer(Child et al. 2019)、LogSparse Transformer(Li et al. 2019)和 Longformer (Beltagy, Peters, and Cohan 2020)都使用启发式方法来处理限制 1，并将自我注意机制的复杂性降低到 O(LlogL)，其中它们的效率增益是有限的(Qiu et al. 2019)。Reformer (Kitaev, Kaiser，和 Levskaya 2019)也通过局部敏感的哈希自我注意实现 O(LlogL)，但它只适用于非常长的序列。最近，Linformer (Wang et al. 2020)声称其线性复杂度为 O(L)，但对于真实世界的长序列输入，投影矩阵不能被固定，可能会有退化到<img src="https://cdn.nlark.com/yuque/__latex/82cdf1c4698471cadb29df3d93dd0c00.svg#card=math&code=O%28L%5E2%29&height=23&width=45">的风险。Transformer- XL (Dai et al. 2019)和  Compressive Transformer (Rae et al. 2019)使用辅助隐藏状态捕获长期依赖，这可能放大限制 1，不利于打破效率瓶颈。所有的工作主要集中在限制 1，限制 2 和 3 仍然存在于 LSTF 问题中。为了提高预测能力，我们将解决所有这些问题，并在建议的 Informer 中实现超越效率的改进。</p><p>为此，我们的工作明确地探讨了这三个问题。我们研究了自我注意机制的稀疏性，对网络组件进行了改进，并进行了大量的实验。本文的研究成果概括如下：</p><ul><li>我们提出的 Informer 在 LSTF 问题中成功地提高了预测能力，验证了 Transformer-like 模型在捕捉长序列时间序列输出和输入之间的个体长期依赖方面的潜在价值。</li><li>提出了 ProbSparse 自我注意机制，有效地取代了规范的自我注意机制，达到了 O(LlogL)的时间复杂度和 O(LlogL)的内存使用量。</li><li>提出了在 J-堆叠层中以自我注意提取操作权限控制注意力得分，并将总空间复杂度大幅降低到 O((2−ε)LlogL)。</li><li>提出了生成式解码器(Generative Style Decoder)，只需一个前向步骤即可获得长序列输出，同时避免了推理阶段的累积误差扩散。</li></ul><h1 id="Preliminary"><a href="#Preliminary" class="headerlink" title="Preliminary"></a>Preliminary</h1><p>我们首先提供问题定义。在具有固定大小窗口的滚动预测设置下(Under the rolling forecasting setting with a fixed size window)，我们有 t 时刻的输入<img src="https://cdn.nlark.com/yuque/__latex/9a3ec5608206bf0b392ba4837af7695a.svg#card=math&code=X%5Et%3D%5C%7Bx_1%5Et%2C%20...%2C%20x_%7BL_x%7D%5Et%20%7C%20x_i%5Et%20%5Cin%20R%5E%7Bd_x%7D%5C%7D&height=26&width=208">，并且预测相应序列的输出为<img src="https://cdn.nlark.com/yuque/__latex/95de93009c975a6ace3d06f07ec3a10b.svg#card=math&code=Y%5Et%3D%5C%7By_1%5Et%2C%20...%2C%20y_%7BL_y%7D%5Et%20%7C%20y_i%5Et%20%5Cin%20R%5E%7Bd_y%7D%5C%7D&height=28&width=202">。LSTF 问题鼓励比以前研究更长的输出长度(Cho 等人 2014 年；Sutskever、Vinyals 和 Le 2014)，并且特征维度不限于单变量情况(dy≥1)。</p><p>**编码器-解码器体系结构: **许多流行的模型被设计成将输入表示<img src="https://cdn.nlark.com/yuque/__latex/83a07f217ed265eb856194ea9eccb9fe.svg#card=math&code=X%5Et&height=18&width=20">“编码”成隐藏状态表示<img src="https://cdn.nlark.com/yuque/__latex/d80edf292867aa1264cb3bd266fa3d7e.svg#card=math&code=H%5Et&height=18&width=20">，并从<img src="https://cdn.nlark.com/yuque/__latex/f41e2d8522d35440ce42f747681f4b75.svg#card=math&code=H%5Et%3D%5C%7Bh_1%5Et%2C%20...%2C%20h_%7BL_h%7D%5Et%5C%7D&height=25&width=141">解码出输出表示<img src="https://cdn.nlark.com/yuque/__latex/d0a0024d62b38e087f50d768a4d05734.svg#card=math&code=Y%5Et&height=16&width=20">。该推断涉及一种名为“动态解码”的逐步过程，其中解码器根据前一状态<img src="https://cdn.nlark.com/yuque/__latex/33cc88e5fbbd93f6d47379807ef64989.svg#card=math&code=h%5Et_k&height=23&width=17">和来自第 k 步的其他必要输出计算出新的隐藏状态<img src="https://cdn.nlark.com/yuque/__latex/61c9dabeefd1c3b43688e2848d96b571.svg#card=math&code=h%5Et_%7Bk%2B1%7D&height=24&width=32">，然后预测第(k+1)个序列<img src="https://cdn.nlark.com/yuque/__latex/b3bfc5eaa99db571971e015654237c98.svg#card=math&code=y%5Et_%7Bk%2B1%7D&height=24&width=31">。</p><p><strong>输入表示：</strong>给定统一的输入表示来增强时间序列输入的全局位置上下文和局部时间上下文。为了避免使描述变得琐碎，我们将详细信息放在附录 B 中。</p><h1 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h1><p>现有的时间序列预测方法大致可以分为两类。经典的时间序列模型是时间序列预测的可靠工具(Box et al. 2015; Ray 1990; Seeger et al. 2017; Seeger,Salinas, and Flunkert 2016)。深度学习技术主要是通过使用 RNN 及其变体来开发编码器-解码器预测范式  (Hochreiter and Schmidhuber 1997; Li et al. 2018; Yu et al. 2017)。我们提出的 Informer 在针对 LSTF 问题的同时保持了编码器-解码器的体系结构。请参见图(2)的概述和下列各节的详细内容。<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1617714623671-63538068-92e0-4615-b822-0e3cd3396356.png#align=left&display=inline&height=305&margin=%5Bobject%20Object%5D&name=image.png&originHeight=375&originWidth=485&size=47690&status=done&style=none&width=394" alt="image.png"><br>图 2：Informer 模型的总体图。左边的部分是编码器，它接收大量的长序列输入(绿色序列)。我们已经用提议的 ProbSparse 自我注意取代了规范的自我注意。蓝色梯形是一种取出主导注意力的自我注意提取操作，大大减小了网络规模。层堆叠副本提高了鲁棒性。对于右边的部分，解码器接收长序列输入，将目标元素填充为零，测量特征图（feature map）的加权注意力组成，并以生成式(generative style)即时预测输出元素(橙色序列)。<br><em>Due to the space limitation, a complete related work survey is provided in Appendix A.</em></p><h2 id="Efficient-Self-attention-Mechanism"><a href="#Efficient-Self-attention-Mechanism" class="headerlink" title="Efficient Self-attention Mechanism"></a>Efficient Self-attention Mechanism</h2><p>(Vaswani et al. 2017)中规范自我注意被定义为接收到元组输入(Query，Key，Value)，并执行缩放点积<img src="https://cdn.nlark.com/yuque/__latex/cbbdf1129700a05d610ee8996b44b42c.svg#card=math&code=A%28Q%2CK%2CV%29%3DSoftmax%28%5Cfrac%7BQK%5ET%7D%7B%5Csqrt%20d%7D%29V&height=48&width=242">, 其中<img src="https://cdn.nlark.com/yuque/__latex/0eda3de83072b77614694cd229209d80.svg#card=math&code=Q%20%5Cin%20R%5E%7BL_Q%C3%97d%7D&height=21&width=80">，<img src="https://cdn.nlark.com/yuque/__latex/14b529d7c4224b5ea360cb2c35192805.svg#card=math&code=K%20%5Cin%20R%5E%7BL_K%C3%97d%7D&height=19&width=83">，<img src="https://cdn.nlark.com/yuque/__latex/6f98b9bcd02f1cdb3e27208cae23c54b.svg#card=math&code=V%20%5Cin%20R%5E%7BL_V%C3%97d%7D&height=19&width=80">，d 是输入维度。为了进一步探讨自注意机制，让<img src="https://cdn.nlark.com/yuque/__latex/83071b249ba13d93d3ced06667fc4e1e.svg#card=math&code=q_i%2Ck_i%2Cv_i&height=18&width=57">分别代表 Q、K、V 中的第 i 行。根据(Tsai et al. 2019)的公式，第 i 个查询的注意被定义为一个概率形式的核平滑器:<br><img src="https://cdn.nlark.com/yuque/__latex/9f8d1210eae26c13687c607f504b5e6e.svg#card=math&code=A%28q_i%2CK%2CV%29%3D%5Cdisplaystyle%20%5Csum_j%7B%5Cfrac%7Bk%28q_i%2Ck_j%29%7D%7B%5Cdisplaystyle%20%5Csum_lk%28q_i%2Ck_l%29%7D%7Dv_j%3DE_%7Bp%28k_j%7Cq_i%29%7D%5Bv_j%5D%20%5Ctag%7B1%7D&height=66&width=724"><br>其中<img src="https://cdn.nlark.com/yuque/__latex/ec6a6f20b75534c52d2f570779c0712e.svg#card=math&code=p%28k_j%7Cq_i%29%3D%5Cfrac%7Bk%28q_i%2Ck_j%29%7D%7Bk%28q_i%2Ck_l%29%7D&height=47&width=142">，<img src="https://cdn.nlark.com/yuque/__latex/f0781803404fb99d9a46fb15686a03e3.svg#card=math&code=k%28q_i%2Ck_j%29&height=21&width=57">选取非对称指数核<img src="https://cdn.nlark.com/yuque/__latex/4a0334df7b8a92bca6a26dbfe05e1840.svg#card=math&code=%5Cfrac%7Bexp%28q_ik_j%5ET%29%7D%7B%5Csqrt%20d%7D&height=52&width=77">。自我注意基于计算概率<img src="https://cdn.nlark.com/yuque/__latex/ed60e0b604858580b9ca6e47b0d81bd7.svg#card=math&code=p%28k_j%7Cq_i%29&height=21&width=55">来组合这些值并获得输出。它需要二次点积计算( the quadratic times dot-product computation)和<img src="https://cdn.nlark.com/yuque/__latex/ef64551733b09ec71cdd03d2987dc987.svg#card=math&code=O%28L_QL_K%29&height=21&width=72">的内存使用，这是提高预测能力的主要缺点。</p><p>前人的一些尝试揭示了自我注意概率的分布具有潜在的稀疏性，并且在不显著影响性能的情况下设计了一些对所有<img src="https://cdn.nlark.com/yuque/__latex/ed60e0b604858580b9ca6e47b0d81bd7.svg#card=math&code=p%28k_j%7Cq_i%29&height=21&width=55">的“选择性”计数策略。The Sparse Transformer (Child et al. 2019)合并了行输出和列输入，其中稀疏性产生于分离的空间相关性。The LogSparse Transformer (Li et al. 2019)注意到自我关注的周期性模式，并迫使每个细胞以指数步长关注前一个细胞。 The Longformer (Beltagy, Peters, and Cohan 2020)  将前两个工作扩展到更复杂的稀疏配置。但是，他们都局限于理论分析，遵循启发式的方法，用相同的策略来解决每个多头自我注意问题，这就限制了其进一步的改进。</p><p>为了激发我们的方法，我们首先对习得的规范自注意的注意模式进行定性评估。“稀疏性”自我注意分数形成了一个长尾分布(a long tail distribution )(详见附录 C)，即少数几个点积对贡献了大部分注意力，其他的可以忽略不计。那么下一个问题是如何区分它们呢？</p><p><strong>Query Sparsity Measurement</strong> 公式(1)中，第 i 个 query 对所有 keys 的关注定义为概率<img src="https://cdn.nlark.com/yuque/__latex/ed60e0b604858580b9ca6e47b0d81bd7.svg#card=math&code=p%28k_j%7Cq_i%29&height=21&width=55">，输出是它与 values <strong>v</strong>的结合。主要的点积对使得相应查询的关注概率分布偏离均匀分布。如果<img src="https://cdn.nlark.com/yuque/__latex/ed60e0b604858580b9ca6e47b0d81bd7.svg#card=math&code=p%28k_j%7Cq_i%29&height=21&width=55">接近于均匀分布<img src="https://cdn.nlark.com/yuque/__latex/528362575be76004ba3dbc68f87adc5d.svg#card=math&code=q%28k_j%7Cq_i%29%3D%5Cfrac%20%7B1%7D%7BL_%7D&height=40&width=106">，则自我注意就成了 values <strong>V</strong>的 trivial sum 并且对 residential 输入是多余的。(the self-attention becomes a trivial sum of values V and is redundant to the residential input) 当然，分布 p 和 q 之间的“相似性”可以用来区分“重要的”查询。我们通过 Kullback-Leibler 发散度 <img src="https://cdn.nlark.com/yuque/__latex/4bdc0faf0d975ffa36928c45449f6555.svg#card=math&code=KL%28q%7C%7Cp%29%3D%5Cln%20%5Cdisplaystyle%20%5Csum_%7Bl%3D1%7D%5E%7BL_K%7D%7Be%5E%7B%5Cfrac%20%7Bq_ik_l%5ET%7D%7B%5Csqrt%20d%7D%7D%7D%20-%20%5Cfrac%20%7B1%7D%7BL_K%7D%5Cdisplaystyle%20%5Csum_%7Bj%3D1%7D%5E%7BL_K%7D%7Be%5E%7B%5Cfrac%20%7Bq_ik_j%5ET%7D%7B%5Csqrt%20d%7D%7D%7D%20-%20%5Cln%20L_K&height=61&width=346">来度量“相似性”。丢弃常量，我们将第 i 个查询的稀疏度量定义为<br><img src="https://cdn.nlark.com/yuque/__latex/26d8173abcc3aa5043764e1e2f567906.svg#card=math&code=M%28q_i%2CK%29%3D%5Cln%20%5Cdisplaystyle%20%5Csum_%7Bj%3D1%7D%5E%7BL_K%7D%7Be%5E%7B%5Cfrac%20%7Bq_ik_j%5ET%7D%7B%5Csqrt%20d%7D%7D%7D%20-%20%5Cfrac%20%7B1%7D%7BL_K%7D%5Cdisplaystyle%20%5Csum_%7Bj%3D1%7D%5E%7BL_K%7D%7Be%5E%7B%5Cfrac%20%7Bq_ik_j%5ET%7D%7B%5Csqrt%20d%7D%7D%7D%20%5Ctag%7B2%7D&height=61&width=724"><br>其中第一项是<img src="https://cdn.nlark.com/yuque/__latex/da326f7200e158a864695985b2e2f095.svg#card=math&code=q_i&height=14&width=13">在所有 keys 上的 Log-Sum-Exp (LSE)，第二项是它们的算术平均值。如果第 i 个查询获得较大的<img src="https://cdn.nlark.com/yuque/__latex/d3403442a08d603369651c2d598521c1.svg#card=math&code=M%28q_i%2CK%29&height=20&width=66">，则其注意概率 p 更加“多样化”，。并且很有可能在长尾自我注意分布的头字段中包含主导点积对。</p><p>**ProbSparse Self-attention **基于所提出的度量，我们通过允许每个键只关注 u 个主要查询来实现 ProbSparse Self-attention<br><img src="https://cdn.nlark.com/yuque/__latex/def20e0097aa66f75e5c8c438b605182.svg#card=math&code=A%28Q%2CK%2CV%29%20%3D%20Softmax%28%5Cfrac%20%7B%5Cstackrel%7B-%7D%7BQ%7DK%5ET%7D%7B%5Csqrt%20d%7D%29V%20%5Ctag%7B3%7D&height=57&width=724"><br>其中<img src="https://cdn.nlark.com/yuque/__latex/36348243501ca146ad458dd1e58a8a26.svg#card=math&code=%5Cstackrel%20%7B-%7D%7BQ%7D&height=30&width=13">是与 q 相同大小的稀疏矩阵，并且它仅包含稀疏度量 M(q，K)下的 Top-u 个查询。在固定采样因子 c 的控制下，设置<img src="https://cdn.nlark.com/yuque/__latex/d72f57a8a4d68a8a3c23fd17f6876a13.svg#card=math&code=u%3Dc%C2%B7%5Cln%20L_Q&height=20&width=90">，这使得 ProbSparse self-attention 对于每个 query-key lookup 只需要计算<img src="https://cdn.nlark.com/yuque/__latex/1c81cef27f82c3fc88af055843ba7f30.svg#card=math&code=O%28%5Cln%20L_Q%29&height=21&width=65">次点积，而层内存使用量保持<img src="https://cdn.nlark.com/yuque/__latex/f363c5b10f408e3f29e6301fcd56bab8.svg#card=math&code=O%28L_K%20%5Cln%20L_Q%29&height=21&width=91">。</p><p>然而，遍历度量<img src="https://cdn.nlark.com/yuque/__latex/5dad588034daa07346617e947d27e725.svg#card=math&code=M%28q_i%EF%BC%8CK%29&height=24&width=74">的所有查询需要计算每个点积对，即二次<img src="https://cdn.nlark.com/yuque/__latex/ef64551733b09ec71cdd03d2987dc987.svg#card=math&code=O%28L_QL_K%29&height=21&width=72">，并且 LSE 操作存在潜在的数值稳定性问题。受此启发，我们提出了一种近似的查询稀疏度量方法。<br>**Lemma 1. **对于每个查询<img src="https://cdn.nlark.com/yuque/__latex/0d4332cabd6bc5c54d07563cf16e52b0.svg#card=math&code=q_i%20%5Cin%20R%5Ed&height=21&width=54">和 keys set <strong>K</strong>中的<img src="https://cdn.nlark.com/yuque/__latex/5aaebcfdb0f68f658c917c5fa1c07e9c.svg#card=math&code=k_j%20%5Cin%20R%5Ed&height=24&width=56">，我们有界为<img src="https://cdn.nlark.com/yuque/__latex/b149f5a4389dcb6ddb1b0c21838a2004.svg#card=math&code=%5Cln%20L_k%20%5Cleq%20M%28q_i%EF%BC%8CK%29%20%5Cleq%20max_j%20%5C%7B%5Cfrac%20%7Bq_ik_j%5ET%7D%7B%5Csqrt%20d%7D%20%5C%7D%E2%88%92%5Cfrac%20%7B1%7D%7BL_K%7D%5Cdisplaystyle%20%5Csum_%7Bj%3D1%7D%5E%7BL_K%7D%7B%5Cfrac%20%7Bq_ik_j%5ET%7D%7B%5Csqrt%20d%7D%7D%2B%5Cln%20L_k&height=57&width=425">。当<img src="https://cdn.nlark.com/yuque/__latex/0bfb2ba27c27cbc9ea5813a8dd2363e0.svg#card=math&code=q_i%20%5Cin%20K&height=18&width=49">时，它也成立。</p><p>从 Lemma 1(证明见附录 D.1)出发，我们提出最大均值度量为<br><img src="https://cdn.nlark.com/yuque/__latex/166f39efa7a7f0ce37d4eafe40661f32.svg#card=math&code=%5Cstackrel%20%7B-%7D%7BM%7D%28q_i%2CK%29%20%3D%20%5Cunderset%20%7Bj%7D%7Bmax%7D%20%5C%7B%5Cfrac%20%7Bq_ik_j%5ET%7D%7B%5Csqrt%20d%7D%20%5C%7D%E2%88%92%5Cfrac%20%7B1%7D%7BL_K%7D%5Cdisplaystyle%20%5Csum_%7Bj%3D1%7D%5E%7BL_K%7D%7B%5Cfrac%20%7Bq_ik_j%5ET%7D%7B%5Csqrt%20d%7D%7D%20%5Ctag%7B4%7D&height=57&width=724"><br>Top-u 的顺序与命题 1 的边界松弛一致(The order of Top-u holds in the boundary relaxation with Proposition 1)(参见附录 D.2 中的证明)。在长尾分布下，我们只需要随机抽样<img src="https://cdn.nlark.com/yuque/__latex/ca820907a47584f81e89ebe59c46db6b.svg#card=math&code=U%3DL_Q%20%5Cln%20L_K&height=20&width=101">个点-积对即可计算<img src="https://cdn.nlark.com/yuque/__latex/f8ab6fa2dff07c8ea8f64eca6e62635e.svg#card=math&code=%5Coverset%20%7B-%7DM%28q_i%EF%BC%8CK%29&height=33&width=74">，即用零填充其他对。我们从中选择稀疏的 Top-u 个作为<img src="https://cdn.nlark.com/yuque/__latex/2a73fd27a287cdcc25060ac855e9b257.svg#card=math&code=%5Coverset%20%7B-%7DQ&height=30&width=13">。<img src="https://cdn.nlark.com/yuque/__latex/f8ab6fa2dff07c8ea8f64eca6e62635e.svg#card=math&code=%5Coverset%20%7B-%7DM%28q_i%EF%BC%8CK%29&height=33&width=74">中的最大算子对零值的敏感性较低，并且数值稳定。在实际应用中，查询和关键字的输入长度通常相等，即<img src="https://cdn.nlark.com/yuque/__latex/2e611ae927b924cf8b78fcfbec4aecd1.svg#card=math&code=L_Q%20%3D%20L_K%20%3D%20L&height=20&width=102">，使得 ProbSparse self-attention 的总时间复杂度和总空间复杂度为<img src="https://cdn.nlark.com/yuque/__latex/b175a4e1504a866e623f8d44d98d8dd6.svg#card=math&code=O%28L%20%5Cln%20L%29&height=20&width=68">。</p><p>**Proposition 1. ** 假设<img src="https://cdn.nlark.com/yuque/__latex/858acc91197e9de2eb105af50a23595e.svg#card=math&code=k_j%E2%88%BCN%28%C2%B5%EF%BC%8C%CE%A3%29&height=24&width=103">，令<img src="https://cdn.nlark.com/yuque/__latex/93316bd9d4398e340a779923f509bb9e.svg#card=math&code=qk_i&height=18&width=22">表示集合<img src="https://cdn.nlark.com/yuque/__latex/1bd3d4c663cc9a50ee25e0eed7b157a6.svg#card=math&code=%5C%7B%28q_ik_j%5ET%29%2F%20%5Csqrt%20d%20%7C%20j%3D1%2C%20%5Cdots%20%2CL_K%20%5C%7D&height=26&width=197">, <img src="https://cdn.nlark.com/yuque/__latex/7ee5e17d7f1888929fa933ecad9a476b.svg#card=math&code=%E2%88%80M_m%3Dmax_iM%28q_i%EF%BC%8CK%29&height=24&width=173">存在 κ&gt;0 使得：在区间<img src="https://cdn.nlark.com/yuque/__latex/be7a5f4a35c20c5b998e0a9986024910.svg#card=math&code=%E2%88%80q_1%EF%BC%8Cq_2%E2%88%88%20%5C%7Bq%7CM%28q%EF%BC%8CK%29%E2%88%88%28M_m%EF%BC%8CM_m%E2%88%92%CE%BA%29%20%5C%7D&height=24&width=310">中，如果<img src="https://cdn.nlark.com/yuque/__latex/7b9d80322ab8f9b42d2d3ec212097d51.svg#card=math&code=%5Coverset%20%7B-%7DM%28q_1%2CK%29%20%3E%20%5Coverset%20%7B-%7D%20M%28q_2%2CK%29%E5%92%8CVar%28qk_1%29%20%3E%20Var%28qk_2%29&height=33&width=330">，则有<img src="https://cdn.nlark.com/yuque/__latex/7df1fd8f604ec56a31f8c4d404ac7e24.svg#card=math&code=M%28q_1%2CK%29%20%3E%20M%28q_2%2CK%29&height=20&width=159">的概率很大。为简化起见，在证明中给出了概率的估计。</p><h2 id="Encoder-Allowing-for-processing-longer-sequential-inputs-under-the-memory-usage-limitation"><a href="#Encoder-Allowing-for-processing-longer-sequential-inputs-under-the-memory-usage-limitation" class="headerlink" title="Encoder: Allowing for processing longer sequential inputs under the memory usage limitation"></a>Encoder: Allowing for processing longer sequential inputs under the memory usage limitation</h2><p>该编码器的设计目的是提取长序列输入的鲁棒长期依赖。在输入表示之后，第 t 个序列输入 X 已被整形为矩阵<img src="https://cdn.nlark.com/yuque/__latex/3f0f083d96baa317e8ba48c93089be80.svg#card=math&code=X%5Et_%7Bfeed%20%5C_%20en%7D%E2%88%88R%5E%7BL_x%C3%97d_%7Bmodel%7D%7D&height=26&width=150">。为了清楚起见，我们在图(3)中给出了编码器的草图。<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1617803444294-d9f9250f-e082-494f-9b78-6cdea07e2e15.png#align=left&display=inline&height=529&margin=%5Bobject%20Object%5D&name=image.png&originHeight=529&originWidth=1083&size=125397&status=done&style=none&width=1083" alt="image.png"><br>图 3：Informer 编码器的体系结构。(1)每个水平堆栈代表图(2)中一个单独的编码器副本；(2)上层堆栈是主堆栈，接收整个输入序列，第二层堆栈接收输入的半个切片；(3)红色层是自我注意机制的点积矩阵，通过对每一层进行自我注意提取得到级联递减；(4)将两层堆栈的特征映射连接起来作为编码器的输出。</p><p><strong>自注意提取</strong> 作为 ProbSparse Self-attention 机制的自然结果，编码器的特征映射存在价值 V 的冗余组合，我们利用提取操作将主要特征赋予上级特征(privilege the superior ones with dominating features )，在下一层生成一个聚焦的自我注意特征映射。它大幅削减了输入的时间维数，如图(3)所示的注意块的 n 个头(n-heads)的权值矩阵(红色方块重叠)。受到膨胀卷积的启发(Y u, Koltun, and Funkhouser 2017;Gupta and Rush 2017)，我们的“提取”过程从第 j 层向前推进到第(j+1)层，如下所示<br><img src="https://cdn.nlark.com/yuque/__latex/5aa5d9ced7745e0c572ad3cdc580f6b7.svg#card=math&code=X%5Et_%7Bj%2B1%7D%3D%20MaxPool%28ELU%28%20Conv1d%28%5BX%5Et_j%5D_%7BAB%7D%29%20%29%29%5Ctag%20%7B5%7D&height=25&width=724"><br>其中<img src="https://cdn.nlark.com/yuque/__latex/5ba33469774feec9f6a1fbd18fc4fa42.svg#card=math&code=%5B%C2%B7%5D_%7BAB%7D&height=20&width=33">包含多头 ProbSparse 自注意和注意块中的基本操作，而 Conv1d(·)利用 ELU(·)激活函数在时间维度上执行一维卷积滤波(核宽度=3)(Clevert，Unterthiner 和 Hochreiter 2016)。在堆叠一个层之后，我们增加一个步长为 2 的最大池化层，并将 X 下采样到其半片中，从而将整个内存使用量减少到<img src="https://cdn.nlark.com/yuque/__latex/8018e1b72044fc523c1c42759d9bf2a7.svg#card=math&code=O%28%282%E2%88%92%CE%B5%29LlogL%29&height=20&width=119">，其中 ε 是一个很小的数字。为了增强提取操作的健壮性，我们建立了主栈的一半副本，并通过一次丢弃一层来逐步减少自注意提取层的数量，就像图(3)中的金字塔(pyramid)，从而使它们的输出维度对齐。因此，我们将所有堆栈的输出连接起来，得到编码器的最终隐藏表示。</p><h2 id="Decoder-Generating-long-sequential-outputs-through-one-forward-procedure"><a href="#Decoder-Generating-long-sequential-outputs-through-one-forward-procedure" class="headerlink" title="Decoder: Generating long sequential outputs through one forward procedure"></a>Decoder: Generating long sequential outputs through one forward procedure</h2><p>我们使用标准的解码器结构(Vaswani et al. 2017)，它由 2 个相同的多头注意层堆叠而成。然而，在长期预测中，采用了产生式推理(generative inference)来缓解速度骤降。我们向解码器提供如下矢量<br><img src="https://cdn.nlark.com/yuque/__latex/584de8041575fc95e41423862910a735.svg#card=math&code=X%5Et_%7Bfeed%5C_de%7D%20%3D%20Concat%28X%5Et_%7Btoken%7D%2CX%5Et_0%29%20%20%5Cin%20R%5E%7B%28L_%7Btoken%2BLy%29%7D%C3%97d_%7Bmodel%7D%7D%20%5Ctag%7B6%7D&height=28&width=724"><br>其中<img src="https://cdn.nlark.com/yuque/__latex/f66b73474a3bdf385c833edf6f191e72.svg#card=math&code=X%5Et_%7Btoken%7D%20%5Cin%20R%5E%7BL_%7Btoken%7D%C3%97d_%7Bmodel%7D%7D&height=24&width=154">l 是开始令牌，<img src="https://cdn.nlark.com/yuque/__latex/ba7129e4bed3022b6423d1a6c63d7f76.svg#card=math&code=X%5Et_0%20%5Cin%20R%5E%7BL_y%C3%97d_%7Bmodel%7D%7D%20&height=24&width=112">是目标序列的占位符(将标量设置为 0)。通过将掩码的点积设置为 −∞，将掩码多头注意应用于 ProbSparse 自注意计算。它防止每个位置关注即将到来的位置，从而避免自回归(auto-regressive)。一个全连接层获得最终输出，它的大小 d 取决于我们执行的是单变量预测还是多变量预测。</p><p><strong>Generative Inference</strong> 起始令牌(start token)是 NLP“动态解码”中的一种有效技术(Devlin et al. 2018 年)，我们将其扩展为一种生成性的方式。我们不选择特定的标志作为令牌，而是在输入序列中采样一个 L 大小的长序列，它是输出序列之前的较早片段。以图(2(b))中预测 168 个点为例(7 天温度预测)，我们将目标序列之前的已知 5 天作为“start token”，并用<img src="https://cdn.nlark.com/yuque/__latex/e2af5fd6d65fc49e0da596265fe56dae.svg#card=math&code=X_%7Bfeed%5C_de%7D%3D%5C%7BX_%7B5d%7D%EF%BC%8CX_0%5C%7D&height=24&width=161">喂给生成式推理(generative-style inference)解码器。X 包含目标序列的时间戳，即目标周的上下文。注意，我们提出的解码器通过一个前向过程来预测所有的输出，并且在简单的编码器-解码器体系结构中没有耗时的“动态解码”事务。在 computation efficiency 部分给出了详细的性能比较。</p><p>**Loss function ** 我们选择 MSE 损失函数对目标序列进行预测，损失从解码器的输出传回整个模型。</p><h1 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h1><h2 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h2><p>我们在四个数据集上进行了实验，其中包括为 LSTF 收集的两个真实数据集和两个公共基准数据集。</p><p><strong>ETT</strong> (Electricity Transformer Temperature): ETT 是电力长期运行的重要指标。我们从中国两个不同的县收集了两年的数据。为了探索 LSTF 问题的粒度，我们为 1 小时级别的创建了单独的数据集<img src="https://cdn.nlark.com/yuque/__latex/398bd490294d541e7484540cd99b910a.svg#card=math&code=%5C%7B%20ETT_%7Bh_1%7D%EF%BC%8CETT_%7Bh_2%7D%20%5C%7D&height=24&width=130">，为 15 分钟级别的创建了<img src="https://cdn.nlark.com/yuque/__latex/2299dcb30d8be21361545e1589cace5a.svg#card=math&code=ETT_%7Bm_1%7D&height=20&width=52">。每个数据点由目标值“油温”和 6 个电力负荷特征组成。The train/val/test  是 12/4/4 个月。<br><strong>ECL</strong>(Electricity Consuming Load)：收集 321 个用户的用电量(千瓦时)。由于数据缺失(Li et al. 2019 年)，我们将数据集转换为 2 年的小时用电量，并将‘MT_320’设置为目标值。The train/val/test  是 15/3/4 个月。<br><strong>Weather</strong>：该数据集包含美国近 1600 个地点的当地气候数据，从 2010 年到 2013 年的 4 年间，每 1 小时收集一次数据点。每个数据点由目标值“wet bulb”和 11 个气候特征组成。The train/val/test 为 28/10/10 个月。<br>We collected the ETT dataset and published it at <a href="https://github.com/zhouhaoyi/ETDataset">https://github.com/zhouhaoyi/ETDataset</a>.<br>ECL dataset was acquired at <a href="https://archive.ics.uci.edu/ml/">https://archive.ics.uci.edu/ml/</a>datasets/ElectricityLoadDiagrams20112014<br>Weather dataset was acquired at <a href="https://www.ncdc.noaa.gov/">https://www.ncdc.noaa.gov/</a>orders/qclcd/</p><h2 id="Experimental-Details"><a href="#Experimental-Details" class="headerlink" title="Experimental Details"></a>Experimental Details</h2><p>我们简要总结了基本信息，有关网络组件和设置的更多信息请参见附录 E。</p><p>**Baselines: **附录 E.1 提供了网络组件的详细信息。我们选择了 5 种时间序列预测方法作为比较，包括 ARIMA (Ariyo, Adewumi, and Ayo 2014)、Prophet (Taylor and Letham 2018)、LSTMa (Bahdanau, Cho, and Bengio 2015)、LSTnet (Lai et al. 2018)和 DeepAR (Flunkert, Salinas, and Gasthaus 2017)。为了更好地探索 ProbSparse 自我注意在我们所提出的 Informer 的表现，我们在实验中纳入了规范的自我注意变体(Informer)、高效的变体 Reformer(Kitaev、Kaiser and Levskaya 2019)和最相关的研究 LogSparse self-attention(Li et al. 2019)。</p><p><strong>Hyper-parameter tuning:</strong> 我们对超参数进行网格搜索，详细范围见附录 E.3。Informer 包含一个 3 层堆栈和一个 2 层堆栈(1/4 输入)的编码器以及 2 层解码器。我们提出的方法采用 Adam 优化器进行优化，其学习速率从 1e 开始，每 2 个 epochs 衰减 10 倍，总 epochs 为 10。我们按照推荐设置了比较方法，batch size 为 32。**Setup: <strong>对每个数据集的输入进行零均值归一化。在 LSTF 设置下，我们逐步延长预测窗口大小，即{ETTh, ECL, Weather}中的{1d, 2d, 7d, 14d, 30d, 40d}， ETTm 中的{6h, 12h, 24h, 72h, 168h}。</strong>Metrics: <strong>我们在每个预测窗口上使用了两个评估指标，包括<img src="https://cdn.nlark.com/yuque/__latex/19223ec8b238f0398716f34b24148bbb.svg#card=math&code=MSE%20%3D%20%5Cfrac%20%7B1%7D%20%7Bn%7D%20%5Csum%20%5En_%7Bi%3D1%7D%28y%E2%88%92%20%5Chat%20y%29%5E2&height=49&width=166">和<img src="https://cdn.nlark.com/yuque/__latex/df28b48511ab5d72c76a87ba2f647463.svg#card=math&code=MAE%20%3D%5Cfrac%20%7B1%7D%7Bn%7D%20%5Csum%5En_%7Bi%3D1%7D%7Cy%E2%88%92%20%5Chaty%7C&height=49&width=159">(多变量预测的平均值)，并以 stride =1 滚动整个集合。</strong>Platform: ** 所有型号都在单个 Nvidia V100 32GB GPU 图形处理器上进行训练/测试。</p><h2 id="Results-and-Analysis"><a href="#Results-and-Analysis" class="headerlink" title="Results and Analysis"></a>Results and Analysis</h2><p>表 1 和表 2 总结了所有方法在 4 个数据集上的单变量/多变量评估结果。随着对预测能力的更高要求，我们逐渐延长了预测范围。为了进行公平的比较，我们精确控制了问题设置，使每种方法的 LSTF 都可以在单个 GPU 上处理。最佳结果以粗体突出显示。<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618031574340-ac61ff4f-b897-45cd-852b-e49756ebadbf.png#align=left&display=inline&height=545&margin=%5Bobject%20Object%5D&name=image.png&originHeight=545&originWidth=891&size=187874&status=done&style=none&width=891" alt="image.png"></p><p><strong>Univariate Time-series Forecasting：</strong>在这种设置下，每种方法都可以在一段时间内实现单个变量的预测。从表 1 中我们观察到：(1)所提出的模型 Informer 极大地提高了所有数据集的推理性能(最后一列的获胜计数)，并且它们的预测误差在不断增长的预测范围内平稳而缓慢地上升。这表明 Informer 在提高 LSTF 问题的预测能力方面取得了成功。(2)Informer 主要在获胜计数上优于规范降级的 Informer†，即 28&gt;14，这支持了查询稀疏性假设，提供了可比较的注意力特征映射。我们提出的方法也比最相关的研究 LogTrans 和 Reformer 性能更好。我们注意到，Reformer 保持动态解码，在 LSTF 中表现不佳，而其他方法则受益于生成式解码器作为非自回归预测器。(3)Informer 模型的预测结果明显优于递归神经网络 LSTMa。我们的方法的均方误差分别降低了 41.5%(at 168), 60.7% (at 336) and 60.7% (at 720)。这表明在自我注意机制中，较短的网络路径比基于 RNN 的模型具有更好的预测能力。(4)与 DeepAR、ARIMA 和 Prophet 相比，本文提出的方法在 MSE 上的平均降幅分别为 20.9% (at 168), 61.2% (at 336), and 51.3% (at 720) 。在 ECL 数据集上，DeepAR 在较短的范围(≤336)上表现得更好，而我们的方法在较长的范围上表现得更好。我们将这归因于一个具体的例子，在这个例子中，问题的可伸缩性反映了预测能力的有效性。</p><p><strong>Multivariate Time-series Forecasting:</strong> 在这种情况下，一些单变量方法是不合适的，而 LSTnet 是最先进的基线。相反，通过调整最终的 FCN 层，我们提出的 Informer 很容易从单变量预测转变为多变量预测。从表 2 中，我们观察到：(1)所提出的模型 Informer 的性能大大优于其他方法，并且在单变量设置下的结果 1 和 2 在多变量时间序列中仍然成立。(2)Informer 模型比基于 RNN 的 LSTMa 模型和基于 CNN 的 LSTnet 模型具有更好的预测效果，平均 MSE 分别下降了 9.5% (at 168), 2.1% (at 336), 13.8% (at 720)。与单变量结果相比，压倒性的性能有所降低，这种现象可能是由于特征维度预测能力的各向异性(anisotropy)造成的。这超出了本文的范围，我们将在今后的工作中对其进行探讨。</p><p>**LSTF with Granularity Consideration: ** 我们进行了额外的比较，试图探索不同粒度下的性能。将  {96, 288, 672} of ETTm1(minutes-level)与{24, 48, 168}<br>of ETTh1(hour-level)比对。即使序列的粒度级别不同，所提出的 Informer 的性能也优于其他基线。</p><h2 id="Parameter-Sensitivity"><a href="#Parameter-Sensitivity" class="headerlink" title="Parameter Sensitivity"></a>Parameter Sensitivity</h2><p>在单变量设置下，我们对提出的 Informer 模型在 ETTh1 上进行了敏感度分析。<strong>Input Length:</strong> 在图(4(a))中，当预测短序列(如 48)时，最初增加编码器/解码器的输入长度会降低性能，但进一步增加会导致 MSE 下降，因为这会带来重复的短期模式(repeat short-term patterns)。然而，在预测长序列(如 168)时，输入越长，MSE 越低。因为较长的编码器输入可能包含更多的依赖性，并且较长的解码器令牌具有丰富的本地信息。<strong>Sampling Factor:</strong> 采样因子控制公式(3)中 ProbSparse 自注意的信息带宽(bandwidth)。我们从小的因素(=3)开始, 到大的因素，在图(4(b))中，总体性能略有提高，最终趋于稳定。验证了我们的查询稀疏性假设，即自我注意机制中存在冗余的点积对，在实践中我们设样本因子=5(红线)。**The Number of Layer Stacking: ** 层的副本对于自我注意的提取是互补的，我们在图(4(c))中研究了每个堆栈{L，L/2，L/4}的行为。堆栈越长，对输入越敏感，部分原因是接收的长期信息更多。我们方法的选择(红线)，即结合 L 和 L/4，是最稳健的策略。<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618036128494-16054ce7-2cd1-4a46-9488-dc9c2f1b45fa.png#align=left&display=inline&height=413&margin=%5Bobject%20Object%5D&name=image.png&originHeight=471&originWidth=961&size=100871&status=done&style=none&width=842" alt="image.png"></p><h2 id="Ablation-Study-How-Informer-works"><a href="#Ablation-Study-How-Informer-works" class="headerlink" title="Ablation Study: How Informer works?"></a>Ablation Study: How Informer works?</h2><p>考虑到消融，我们还对 ETTh1 进行了额外的实验。</p><p><strong>The performance of ProbSparse self-attention mechanism：</strong>在表 1 和 2 的总体结果中，我们对问题设置进行了限制，以使规范自我注意的内存使用是可行的。在本研究中，我们将我们的方法与 LogTrans 和 Reformer 进行了比较，并深入探讨了它们的极限性能。为了隔离内存效率问题，我们首先将设置减少为{Batch Size=8，Heads=8，Dim=64}，并在单变量情况下保持其他设置。在表 3 中，ProbSparse 自我注意显示出比对应自我注意更好的性能。LogTrans 在极端情况下获得 OOM(out of memory)，因为它的公共实现是完全注意的掩码(mask of the full-attention)，它仍然有 O(L)的内存使用量。我们提出的 ProbSparse 自我注意避免了公式(4)中的查询稀疏性假设带来的简单性，参考了附录 E.2 中的伪代码，并且达到了更小的内存使用量。</p><p><strong>The performance of self-attention distilling：</strong>在本研究中，我们使用 Informer†作为基准来消除 ProbSparse 自我注意的额外影响。另一种实验设置与单变量时间序列的设置一致。从表 4 可以看出，Informer†已经完成了所有实验，并且在利用长序列输入后取得了更好的性能。相比较的方法 Informer‡则去除了提取操作，并使用更长的输入(&gt;720 时)达到 OOM。关于 LSTF 问题中长序列输入的好处，我们得出结论：自我注意提取是值得采用的，特别是当需要更长的预测时。</p><p><strong>The performance of generative style decoder：</strong>在本研究中，我们证明了我们的解码器在获得“生成性”结果方面的潜在价值。与已有方法不同的是，在训练和推理过程中，标签和输出是强制对齐的，我们提出的解码器的预测仅依赖于时间戳，时间戳可以用偏移量进行预测。从表 5 可以看出，Informer‡的总体预测性能随着偏移量的增加而下降，而对应的 Informer§的动态解码失败。它证明了解码器能够捕捉任意输出之间的单个长期依赖关系，并避免了推理中的错误累积。<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618039545504-375e79cb-f8fb-44c5-91ef-597233fe16a0.png#align=left&display=inline&height=634&margin=%5Bobject%20Object%5D&name=image.png&originHeight=634&originWidth=494&size=84713&status=done&style=none&width=494" alt="image.png"></p><h2 id="Computation-Efficiency"><a href="#Computation-Efficiency" class="headerlink" title="Computation Efficiency"></a>Computation Efficiency</h2><p>在使用多变量设置和每种方法当前最好的实现下，我们在图(5)中执行了严格的运行时间比较。在训练阶段，在基于 Transformer 的方法中，Informer(红线)的训练效率最高。在测试阶段，我们的方法比其他生成式解码方法要快得多。表 6 总结了理论时间复杂度和内存使用量的比较，Informer 的性能与运行时间实验一致。请注意，LogTrans 侧重于自我注意机制，我们在 LogTrans 中应用了我们提出的解码器，以进行公平的比较(the ★ in Table 6)。<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618039980736-5e3ebdaa-d9db-4203-ad47-0e56d6cc46d0.png#align=left&display=inline&height=490&margin=%5Bobject%20Object%5D&name=image.png&originHeight=490&originWidth=629&size=83772&status=done&style=none&width=629" alt="image.png"></p><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>本文研究了长序列时间序列的预测问题，提出了一种用于预测长序列的 Informer 模型。具体地说，我们设计了 ProbSparse 自我注意机制和提取操作，以应对 Vanilla Transformer 中的二次方时间复杂度和二次方内存使用的挑战。此外，精心设计的生成式解码器缓解了传统编码器-解码器体系结构的局限性。在真实数据上的实验表明，Informer 在提高 LSTF 问题预测能力方面是有效的。</p><h1 id="Appendices"><a href="#Appendices" class="headerlink" title="Appendices"></a>Appendices</h1><h2 id="Appendix-A-Related-Work"><a href="#Appendix-A-Related-Work" class="headerlink" title="Appendix A Related Work"></a>Appendix A Related Work</h2><p>下面我们提供长序列时间序列预测(LSTF)问题的文献综述。</p><p><strong>Time-series Forecasting</strong> 现有的时间序列预测方法大致可以分为两类：经典模型和基于深度学习的方法。经典时间序列模型作为时间序列预测的可靠工具，具有诸如可解释性和理论保证等吸引人的特性  (Box et al. 2015; Ray 1990)。现在的扩展模型包括对支持丢失数据(Seeger et al. 2017)  和多种数据类型(Seeger, Salinas, and Flunkert 2016)。基于深度学习的方法主要通过使用 RNN 及其变体开发序列到序列预测范式，取得突破性的性能(Hochreiter and Schmidhuber 1997; Li et al. 2018; Y u et al. 2017)。尽管已经取得了实质性的进展，但现有的算法仍然不能以令人满意的精度预测长序列时间序列。典型的最先进的方法  (Seeger et al. 2017; Seeger, Salinas, and Flunkert 2016)，特别是深度学习方法  (Y u et al. 2017; Qin et al. 2017; Flunkert, Salinas, and Gasthaus 2017; Mukherjee et al. 2018; Wen et al. 2017)，仍然是一种循序渐进的序列到序列的预测范式，具有以下局限性：(i)即使它们可以实现一步前向的准确预测，它们也经常遭受动态解码的累积误差，导致 LSTF 问题的较大误差 (Liu et al. 2019; Qin et al. 2017)。预测精度随着预测序列长度的增加而下降。(ii)由于梯度消失和内存约束的问题(Sutskever, Vinyals, and Le 2014)，现有的大多数方法不能学习时间序列整个历史的过去行为。在我们的工作中，Informer 就是为了解决这两个限制而设计的。</p><h2 id="Appendix-B-The-Uniform-Input-Representation"><a href="#Appendix-B-The-Uniform-Input-Representation" class="headerlink" title="Appendix B The Uniform Input Representation"></a>Appendix B The Uniform Input Representation</h2><h2 id="Appendix-C-The-long-tail-distribution-in-self-attention-feature-map"><a href="#Appendix-C-The-long-tail-distribution-in-self-attention-feature-map" class="headerlink" title="Appendix C The long tail distribution in self- attention feature map"></a>Appendix C The long tail distribution in self- attention feature map</h2><h2 id="Appendix-D-Details-of-the-proof"><a href="#Appendix-D-Details-of-the-proof" class="headerlink" title="Appendix D Details of the proof"></a>Appendix D Details of the proof</h2><h3 id="Proof-of-Lemma-1"><a href="#Proof-of-Lemma-1" class="headerlink" title="Proof of Lemma 1"></a>Proof of Lemma 1</h3><h3 id="Proof-of-Proposition-1"><a href="#Proof-of-Proposition-1" class="headerlink" title="Proof of Proposition 1"></a>Proof of Proposition 1</h3><h2 id="Appendix-E-Reproducbility"><a href="#Appendix-E-Reproducbility" class="headerlink" title="Appendix E Reproducbility"></a>Appendix E Reproducbility</h2><h3 id="Details-of-the-experiments"><a href="#Details-of-the-experiments" class="headerlink" title="Details of the experiments"></a>Details of the experiments</h3><h3 id="lmplement-of-the-ProbSparse-self-attention"><a href="#lmplement-of-the-ProbSparse-self-attention" class="headerlink" title="lmplement of the ProbSparse self- attention"></a>lmplement of the ProbSparse self- attention</h3><h3 id="The-hyperparameter-tuning-range"><a href="#The-hyperparameter-tuning-range" class="headerlink" title="The hyperparameter tuning range"></a>The hyperparameter tuning range</h3><h3 id="Appendix-F-Extra-experimental-results"><a href="#Appendix-F-Extra-experimental-results" class="headerlink" title="Appendix F Extra experimental results"></a>Appendix F Extra experimental results</h3><h2 id="Appendix-G-Computing-Infrastructure"><a href="#Appendix-G-Computing-Infrastructure" class="headerlink" title="Appendix G Computing Infrastructure"></a>Appendix G Computing Infrastructure</h2>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;Abstract&quot;&gt;&lt;a href=&quot;#Abstract&quot; class=&quot;headerlink&quot; title=&quot;Abstract&quot;&gt;&lt;/a&gt;Abstract&lt;/h1&gt;&lt;p&gt;现实世界中的许多应用都需要对长序列时间序列进行预测，例如用电量规划。长序列时间序列预测(LSTF)对模型的预测能力提出了很高的要求，即能够有效地捕捉到输出和输入之间精确的长期依赖耦合。最近的研究表明，Transformer 具有提高预测能力的潜力。然而，Transformer 有几个严重的问题阻碍了它直接应用于 LSTF，例如二次时间复杂度、高内存使用率以及编码器-解码器体系结构的固有限制。针对这些问题，我们设计了一种高效的基于 Transformer 的 LSTF 模型 Informer，该模型具有三个显著的特点：(1)ProbSparse 自我注意机制，在时间复杂度和内存使用量上达到 O(LlogL)，在序列依赖比对方面具有相当的性能。(2)自我注意提取通过将级联层输入减半来突出主导注意力，并有效地处理超长输入序列。(3)生成式解码器虽然概念简单，但对长时间序列的预测采用一次正向运算，而不是分步预测，大大提高了长序列预测的推理速度。在四个大规模数据集上的大量实验表明，Informer 的性能明显优于现有方法，为 LSTF 问题提供了一种新的解决方案。&lt;/p&gt;</summary>
    
    
    
    <category term="知识追踪" scheme="https://ytno1.github.io/categories/%E7%9F%A5%E8%AF%86%E8%BF%BD%E8%B8%AA/"/>
    
    <category term="Informer" scheme="https://ytno1.github.io/categories/%E7%9F%A5%E8%AF%86%E8%BF%BD%E8%B8%AA/Informer/"/>
    
    
    <category term="知识追踪" scheme="https://ytno1.github.io/tags/%E7%9F%A5%E8%AF%86%E8%BF%BD%E8%B8%AA/"/>
    
    <category term="Informer" scheme="https://ytno1.github.io/tags/Informer/"/>
    
  </entry>
  
  <entry>
    <title>plt.contourf()用法</title>
    <link href="https://ytno1.github.io/archives/e01bb831.html"/>
    <id>https://ytno1.github.io/archives/e01bb831.html</id>
    <published>2021-04-02T14:17:27.000Z</published>
    <updated>2021-04-12T16:58:26.282Z</updated>
    
    <content type="html"><![CDATA[<h1 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h1><p>contourf([X, Y,] Z, [levels], **kwargs)</p><h1 id="功能"><a href="#功能" class="headerlink" title="功能"></a>功能</h1><p>contour()和 contourf()分别绘制等高线和填充等高线。除非另有说明，两个版本的函数签名和返回值是相同的。</p><span id="more"></span><h1 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h1><p>**X, Y: **<strong>array-like, optional</strong><br>Z 中值的坐标。X 和 Y 必须是与 Z 相同形状的 2D 数据(例如，通过 numpy.meshgrid 创建)，或者它们必须是 1-D，这样 len(X) == M 是 Z 中的列数，len(Y) == N 是 Z 中的行数。<br>如果没有给出，则假设它们是整数索引，即 X = range(M)， Y = range(N)。<br><strong>Z: (N, M) array-like</strong><br>绘制等高线的高度值。<br>**levels: int or array-like, optional**<br>确定轮廓线/区域的数量和位置。如果为 int n，则使用 MaxNLocator，它会尝试在 vmin 和 vmax 之间自动选择不超过 n + 1 个等高线 levels。<br>如果是类数组，则在指定的 levels 上绘制等高线。这些值必须按递增顺序排列。</p><h1 id="返回"><a href="#返回" class="headerlink" title="返回"></a>返回</h1><p>QuadContourSet</p><h1 id="例-1-基础用法"><a href="#例-1-基础用法" class="headerlink" title="例 1 基础用法"></a>例 1 基础用法</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment"># 计算x,y坐标对应的高度值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span>(<span class="params">x, y</span>):</span></span><br><span class="line"> <span class="keyword">return</span> (<span class="number">1</span>-x/<span class="number">2</span>+x**<span class="number">3</span>+y**<span class="number">5</span>) * np.exp(-x**<span class="number">2</span>-y**<span class="number">2</span>)</span><br><span class="line"><span class="comment"># 生成x,y的数据</span></span><br><span class="line">x = np.linspace(-<span class="number">3</span>, <span class="number">3</span>, <span class="number">251</span>)</span><br><span class="line">y = np.linspace(-<span class="number">3</span>, <span class="number">3</span>, <span class="number">201</span>)</span><br><span class="line"><span class="comment"># 把x,y数据生成mesh网格状的数据，因为等高线的显示是在网格的基础上添加上高度值</span></span><br><span class="line">X, Y = np.meshgrid(x, y)</span><br><span class="line">f(X,Y).shape</span><br><span class="line"><span class="comment"># 填充等高线</span></span><br><span class="line">plt.contourf(X, Y, f(X, Y))</span><br><span class="line"><span class="comment"># 显示图表</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1617375888460-d12bc6f3-83d0-415a-8700-d31239eec9d4.png#align=left&display=inline&height=179&margin=%5Bobject%20Object%5D&name=image.png&originHeight=249&originWidth=372&size=7864&status=done&style=none&width=267" alt="image.png"></p><h1 id="例-2-levels-的用法"><a href="#例-2-levels-的用法" class="headerlink" title="例 2 levels 的用法"></a>例 2 levels 的用法</h1><p>当为整数的时候, 如<strong>levels=3</strong>, 他会将数值分成三份, 设置为 10 就是分为 10 份, 可以看下面的例子.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">x, y = np.meshgrid(np.arange(<span class="number">10</span>),np.arange(<span class="number">10</span>))</span><br><span class="line">z = np.sqrt(x**<span class="number">2</span> + y**<span class="number">2</span>)</span><br><span class="line">cs = plt.contourf(x,y,z,levels=<span class="number">10</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1617376189393-5924ce60-d1c1-4d9b-a0eb-b5899bc1f1c1.png#align=left&display=inline&height=181&margin=%5Bobject%20Object%5D&name=image.png&originHeight=247&originWidth=365&size=8133&status=done&style=none&width=268" alt="image.png"><br>当然 levels 后面也是可以跟 lists, 注意 lists 中的数字必须从小到大. 这个数字表示的是 a0-a1 是一部分, a1-a2 是一部分. 比如 levels=[2,3,4,6], 就表示 2&lt;x&lt;3, 3&lt;x&lt;4, 4&lt;x&lt;5, 5&lt;x&lt;6, 最终的结果如下图所示.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">x, y = np.meshgrid(np.arange(<span class="number">10</span>),np.arange(<span class="number">10</span>))</span><br><span class="line">z = np.sqrt(x**<span class="number">2</span> + y**<span class="number">2</span>)</span><br><span class="line">cs = plt.contourf(x,y,z,levels=[<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">6</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1617376382976-cb88e250-0446-48a2-87a3-0e3b6088d7f9.png#align=left&display=inline&height=185&margin=%5Bobject%20Object%5D&name=image.png&originHeight=248&originWidth=360&size=7006&status=done&style=none&width=269" alt="image.png"></p><h1 id="例-3-自定义颜色-color-参数"><a href="#例-3-自定义颜色-color-参数" class="headerlink" title="例 3 自定义颜色 color 参数"></a>例 3 自定义颜色 color 参数</h1><p>自定义颜色需要配合 levels 来进行使用, 比如我们指定了 levels 对应着三个范围, 我们就可以对这三个范围依次设置颜色, 如下所示.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x, y = np.meshgrid(np.arange(<span class="number">10</span>),np.arange(<span class="number">10</span>))</span><br><span class="line">z = np.sqrt(x**<span class="number">2</span> + y**<span class="number">2</span>)</span><br><span class="line">cs = plt.contourf(x,y,z,levels=[<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">6</span>], colors=[<span class="string">&#x27;lightgreen&#x27;</span>, <span class="string">&#x27;royalblue&#x27;</span>, <span class="string">&#x27;gold&#x27;</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1617376634347-6f92d7e8-1d03-48cd-8b05-c5ebf57ba0f5.png#align=left&display=inline&height=195&margin=%5Bobject%20Object%5D&name=image.png&originHeight=245&originWidth=359&size=6884&status=done&style=none&width=286" alt="image.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">x = np.arange(<span class="number">1</span>, <span class="number">10</span>)</span><br><span class="line">x</span><br><span class="line">y = x.reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">y</span><br><span class="line">h = x * y</span><br><span class="line">cs = plt.contourf(h, levels=[<span class="number">10</span>, <span class="number">30</span>, <span class="number">50</span>], colors=[<span class="string">&#x27;#808080&#x27;</span>, <span class="string">&#x27;#A0A0A0&#x27;</span>, <span class="string">&#x27;#C0C0C0&#x27;</span>], extend=<span class="string">&#x27;both&#x27;</span>)</span><br><span class="line">cs.cmap.set_over(<span class="string">&#x27;red&#x27;</span>)</span><br><span class="line">cs.cmap.set_under(<span class="string">&#x27;blue&#x27;</span>)</span><br><span class="line">cs.changed()</span><br><span class="line"><span class="comment"># 避免图片显示不完全</span></span><br><span class="line"><span class="comment"># plt.tight_layout()</span></span><br><span class="line"><span class="comment"># Label a contour plot.显示出等高线的值，以及指定颜色</span></span><br><span class="line">plt.clabel(cs,fontsize=<span class="number">8</span>,colors=[<span class="string">&#x27;red&#x27;</span>,<span class="string">&#x27;red&#x27;</span>,<span class="string">&#x27;blue&#x27;</span>])</span><br><span class="line">plt.show</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>])</span><br><span class="line">array([[<span class="number">1</span>],</span><br><span class="line">       [<span class="number">2</span>],</span><br><span class="line">       [<span class="number">3</span>],</span><br><span class="line">       [<span class="number">4</span>],</span><br><span class="line">       [<span class="number">5</span>],</span><br><span class="line">       [<span class="number">6</span>],</span><br><span class="line">       [<span class="number">7</span>],</span><br><span class="line">       [<span class="number">8</span>],</span><br><span class="line">       [<span class="number">9</span>]])</span><br><span class="line">&lt;a <span class="built_in">list</span> of <span class="number">3</span> text.Text objects&gt;</span><br></pre></td></tr></table></figure><p><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1617377578956-be674f7e-e1ed-497d-814d-0b1d138ccb5a.png#align=left&display=inline&height=181&margin=%5Bobject%20Object%5D&name=image.png&originHeight=240&originWidth=356&size=8239&status=done&style=none&width=269" alt="image.png"></p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;语法&quot;&gt;&lt;a href=&quot;#语法&quot; class=&quot;headerlink&quot; title=&quot;语法&quot;&gt;&lt;/a&gt;语法&lt;/h1&gt;&lt;p&gt;contourf([X, Y,] Z, [levels], **kwargs)&lt;/p&gt;
&lt;h1 id=&quot;功能&quot;&gt;&lt;a href=&quot;#功能&quot; class=&quot;headerlink&quot; title=&quot;功能&quot;&gt;&lt;/a&gt;功能&lt;/h1&gt;&lt;p&gt;contour()和 contourf()分别绘制等高线和填充等高线。除非另有说明，两个版本的函数签名和返回值是相同的。&lt;/p&gt;</summary>
    
    
    
    <category term="笔记" scheme="https://ytno1.github.io/categories/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="Matplotlib" scheme="https://ytno1.github.io/categories/%E7%AC%94%E8%AE%B0/Matplotlib/"/>
    
    <category term="contourf()" scheme="https://ytno1.github.io/categories/%E7%AC%94%E8%AE%B0/Matplotlib/contourf/"/>
    
    
    <category term="笔记" scheme="https://ytno1.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="Matplotlib" scheme="https://ytno1.github.io/tags/Matplotlib/"/>
    
    <category term="contourf()" scheme="https://ytno1.github.io/tags/contourf/"/>
    
  </entry>
  
  <entry>
    <title>np.meshgrid()用法</title>
    <link href="https://ytno1.github.io/archives/e3ef8953.html"/>
    <id>https://ytno1.github.io/archives/e3ef8953.html</id>
    <published>2021-04-02T13:31:48.000Z</published>
    <updated>2021-04-12T16:58:26.342Z</updated>
    
    <content type="html"><![CDATA[<h1 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h1><p>numpy.meshgrid(*xi, copy=True, sparse=False, indexing=’xy’)</p><h1 id="功能"><a href="#功能" class="headerlink" title="功能"></a>功能</h1><p>当传入的参数是两个的时候，meshgrid 函数就是用两个坐标轴上的点在平面上画网格。当然我们可以指定多个参数，比如三个参数，那么就是用用三个一维的坐标轴上的点在三维平面上画网格。</p><span id="more"></span><h1 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h1><p><strong>x1, x2,…, xn: array_like</strong><br>一维数组，表示网格的坐标。这里可以传入多个一维数组的值。但是这里需要注意的就是如果我们给传入的是一个矩阵(多维数组)的话，他会自动把这个矩阵转换成一维数组。<br><strong>indexing: {‘xy’, ‘ij’}, optional</strong><br>输出的形状基于笛卡尔(‘ xy ‘，默认)或矩阵(‘ ij ‘)索引。详情请看下面的“<strong>注意</strong>”。<br><strong>sparse: bool, optional</strong><br>如果为 True，则返回一个稀疏网格以节省内存。 默认值为 False。<br><strong>copy: bool, optional</strong><br>如果为 False，则返回原始数组的视图以节省内存。默认是正确的。请注意，sparse=False, copy=False 可能会返回不连续的数组。此外，广播数组的多个元素可以引用单个内存位置。如果需要写入数组，请先复制。</p><h1 id="返回"><a href="#返回" class="headerlink" title="返回"></a>返回</h1><p><strong>X1, X2,…, XN: ndarray</strong><br>对于长度为 Ni=len(xi)的向量 x1, x2，…，’ xn ‘，如果 index = ‘ ij ‘则返回(N1, N2, N3，…Nn)形状的数组，如果 index = ‘ xy ‘则返回(N2, N1, N3，…Nn)形状的数组，用对应 xi 的元素重复填充矩阵的第一个维度 x1，第二个维度 x2，以此类推。</p><h1 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h1><p>该函数通过 indexing 关键字参数支持这两种索引约定。给定字符串’ ij ‘将返回一个带有矩阵索引的网格，而’ xy ‘将返回一个带有笛卡尔索引的网格。在输入长度为 M 和 N 的二维情况下，输出为 xy 索引的形状(N, M)和 ij 索引的形状(M, N)。在输入长度为 M, N 和 P 的 3-D 情况下，输出为 xy 索引的形状(N, M, P)和 ij 索引的形状(M, N, P)。在 1 维和 0 维情况下，索引和稀疏两个参数的设置则没有影响。</p><h1 id="用法"><a href="#用法" class="headerlink" title="用法"></a>用法</h1><ol><li> xv,yv = meshgrid(x,y)</li><li> xv,yv = meshgrid(x)与 xv,yv = meshgrid(x,x)是等同的</li><li> xv,yv,zv = meshgrid(x,y,z)生成三维数组，可用来计算三变量的函数和绘制三维立体图</li></ol><h1 id="例-1-indexing-参数"><a href="#例-1-indexing-参数" class="headerlink" title="例 1 indexing 参数"></a>例 1 indexing 参数</h1><p>indexing 参数有两个值’xy’和’ij’，默认值为’xy’。其中’xy’代表的是笛卡尔，’ij’代表的是矩阵。通过下面代码查看两者的区别：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">x = np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">y = np.array([<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>])</span><br><span class="line">xv,yv = np.meshgrid(x,y,indexing = <span class="string">&#x27;xy&#x27;</span>)</span><br><span class="line">xv2,yv2 = np.meshgrid(x,y,indexing = <span class="string">&#x27;ij&#x27;</span>)</span><br><span class="line">print(<span class="string">&#x27;-----向量的形状-----&#x27;</span>)</span><br><span class="line">print(x.shape)</span><br><span class="line">print(y.shape)</span><br><span class="line"></span><br><span class="line">print(<span class="string">&#x27;-----xy-----&#x27;</span>)</span><br><span class="line">print(xv.shape)</span><br><span class="line">print(yv.shape)</span><br><span class="line"></span><br><span class="line">print(<span class="string">&#x27;-----ij-----&#x27;</span>)</span><br><span class="line">print(xv2.shape)</span><br><span class="line">print(yv2.shape)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">-----向量的形状-----</span><br><span class="line">(<span class="number">3</span>,)</span><br><span class="line">(<span class="number">4</span>,)</span><br><span class="line">-----xy-----</span><br><span class="line">(<span class="number">4</span>, <span class="number">3</span>)</span><br><span class="line">(<span class="number">4</span>, <span class="number">3</span>)</span><br><span class="line">-----ij-----</span><br><span class="line">(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">(<span class="number">3</span>, <span class="number">4</span>)</span><br></pre></td></tr></table></figure><p>总结一下,对于二维数组来说，对于两个长度为 3 和 4 的一维数组，我们设 N = 3 ,M = 4。对于我们 indexing = ‘xy’(默认)来说，得到 xv 以及 yv 矩阵的形状是(M,N)也就是(4,3)；那对于 indexing = ‘ij’我们的 xv 以及 yv 矩阵的形状是(N,M)也就是(3,4)。</p><p>那对于三维来说，参数是三个一维数组，并且一维数组的形状分别是 N，M，P，那么如果 indexing = ‘xy’的话返回的三个矩阵 xv,yv,zv 的形状都是(M,N,P)；如果 indexing = ‘ij’的话返回的是三个矩阵 xv,yv,zv 的形状都是(N,M,P)。</p><p>另外,其实不管用’xy’还是’ij’, 生成的网格坐标都是同样的</p><h1 id="例-2-np-meshgrid-plt-contourf"><a href="#例-2-np-meshgrid-plt-contourf" class="headerlink" title="例 2 np.meshgrid()+plt.contourf"></a>例 2 np.meshgrid()+plt.contourf</h1><p>np.meshgrid()生成网格坐标之后,可以配合 plt.contourf()生成填充的等高线图.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">nx, ny = (<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">x = np.linspace(<span class="number">0</span>, <span class="number">1</span>, nx)</span><br><span class="line">y = np.linspace(<span class="number">0</span>, <span class="number">1</span>, ny)</span><br><span class="line">xv, yv = np.meshgrid(x, y)</span><br><span class="line">xv</span><br><span class="line">array([[<span class="number">0.</span> , <span class="number">0.5</span>, <span class="number">1.</span> ],</span><br><span class="line">       [<span class="number">0.</span> , <span class="number">0.5</span>, <span class="number">1.</span> ]])</span><br><span class="line">yv</span><br><span class="line">array([[<span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>],</span><br><span class="line">       [<span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>]])</span><br><span class="line">xv, yv = np.meshgrid(x, y, sparse=<span class="literal">True</span>)  <span class="comment"># make sparse output arrays</span></span><br><span class="line">xv</span><br><span class="line">array([[<span class="number">0.</span> ,  <span class="number">0.5</span>,  <span class="number">1.</span> ]])</span><br><span class="line">yv</span><br><span class="line">array([[<span class="number">0.</span>],</span><br><span class="line">       [<span class="number">1.</span>]])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">x = np.arange(-<span class="number">5</span>, <span class="number">5</span>, <span class="number">0.1</span>)</span><br><span class="line">y = np.arange(-<span class="number">5</span>, <span class="number">5</span>, <span class="number">0.1</span>)</span><br><span class="line">xx, yy = np.meshgrid(x, y, sparse=<span class="literal">True</span>)</span><br><span class="line">z = np.sin(xx**<span class="number">2</span> + yy**<span class="number">2</span>) / (xx**<span class="number">2</span> + yy**<span class="number">2</span>)</span><br><span class="line">h = plt.contourf(x,y,z)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1617372156310-4fe4a051-8a6f-4814-915d-87637c43fb66.png#align=left&display=inline&height=177&margin=%5Bobject%20Object%5D&name=image.png&originHeight=225&originWidth=378&size=14430&status=done&style=none&width=297" alt="image.png"></p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;语法&quot;&gt;&lt;a href=&quot;#语法&quot; class=&quot;headerlink&quot; title=&quot;语法&quot;&gt;&lt;/a&gt;语法&lt;/h1&gt;&lt;p&gt;numpy.meshgrid(*xi, copy=True, sparse=False, indexing=’xy’)&lt;/p&gt;
&lt;h1 id=&quot;功能&quot;&gt;&lt;a href=&quot;#功能&quot; class=&quot;headerlink&quot; title=&quot;功能&quot;&gt;&lt;/a&gt;功能&lt;/h1&gt;&lt;p&gt;当传入的参数是两个的时候，meshgrid 函数就是用两个坐标轴上的点在平面上画网格。当然我们可以指定多个参数，比如三个参数，那么就是用用三个一维的坐标轴上的点在三维平面上画网格。&lt;/p&gt;</summary>
    
    
    
    <category term="笔记" scheme="https://ytno1.github.io/categories/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="Numpy" scheme="https://ytno1.github.io/categories/%E7%AC%94%E8%AE%B0/Numpy/"/>
    
    <category term="np.meshgrid()" scheme="https://ytno1.github.io/categories/%E7%AC%94%E8%AE%B0/Numpy/np-meshgrid/"/>
    
    
    <category term="笔记" scheme="https://ytno1.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="Numpy" scheme="https://ytno1.github.io/tags/Numpy/"/>
    
    <category term="np.meshgrid()" scheme="https://ytno1.github.io/tags/np-meshgrid/"/>
    
  </entry>
  
  <entry>
    <title>Matplotlib操作笔记</title>
    <link href="https://ytno1.github.io/archives/bd1550fb.html"/>
    <id>https://ytno1.github.io/archives/bd1550fb.html</id>
    <published>2021-03-30T06:44:03.000Z</published>
    <updated>2021-04-12T16:58:26.373Z</updated>
    
    <content type="html"><![CDATA[<h1 id="开始之前"><a href="#开始之前" class="headerlink" title="开始之前"></a>开始之前</h1><p>学习 Matplotlib 绘图其实就是学习绘图函数中的参数！将参数活学活用，不同的参数搭配会产生不同的化学效应！</p><h1 id="相关配置"><a href="#相关配置" class="headerlink" title="相关配置"></a>相关配置</h1><span id="more"></span><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment">#设置单元格全部行的输出结果</span></span><br><span class="line"><span class="keyword">from</span> IPython.core.interactiveshell <span class="keyword">import</span> InteractiveShell</span><br><span class="line">InteractiveShell.ast_node_interactivity = <span class="string">&quot;all&quot;</span></span><br></pre></td></tr></table></figure><h1 id="plt-scatter-用法"><a href="#plt-scatter-用法" class="headerlink" title="plt.scatter()用法"></a>plt.scatter()用法</h1><p>功能：用于散点图的绘制。</p><p>参数<br>x, y → 散点的坐标<br>s → 散点大小，是一个标量或者是一个 shape 大小为(n,)的数组，可选，默认 20。<br>c → 散点的颜色（默认值为蓝色，’b’，其余颜色同 plt.plot( )）<br>marker → 散点样式（默认值为实心圆，’o’，其余样式同 plt.plot( )）<br>alpha → 散点透明度（[0, 1]之间的数，0 表示完全透明，1 则表示完全不透明）<br>linewidths → 散点的边缘线宽<br>edgecolors → 散点的边缘颜色<br>cmap → 指的是 matplotlib.colors.Colormap，相当于多个调色盘的合集<br>norm、vmin、vmax → 散点颜色亮度设置</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">n = <span class="number">10</span> <span class="comment"># 用于生成十个点</span></span><br><span class="line">x = np.random.rand(n)</span><br><span class="line">y = np.random.rand(n)</span><br><span class="line"></span><br><span class="line">plt.scatter(x, y, s=<span class="number">100</span>, c=<span class="string">&#x27;r&#x27;</span>, marker=<span class="string">&#x27;*&#x27;</span>,alpha=<span class="number">0.65</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1617086987094-fcad9a5f-6fd3-4485-9e15-4929ea72cd7c.png#align=left&display=inline&height=174&margin=%5Bobject%20Object%5D&name=image.png&originHeight=242&originWidth=368&size=6617&status=done&style=none&width=265" alt="image.png"></p><p>从 cmap 中选取了一个叫做’viridis’的调色盘，其作用是，将参数 c 中获取到的数值，映射到“色盘”中已经对应好的颜色上</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">rng = np.random.RandomState(<span class="number">0</span>)</span><br><span class="line">x = rng.randn(<span class="number">50</span>)  <span class="comment"># 随机产生50个X轴坐标</span></span><br><span class="line">y = rng.randn(<span class="number">50</span>)  <span class="comment"># 随机产生50个Y轴坐标</span></span><br><span class="line">colors = rng.rand(<span class="number">50</span>)  <span class="comment"># 随机产生50个用于颜色映射的数值</span></span><br><span class="line">sizes = <span class="number">700</span> * rng.rand(<span class="number">50</span>)  <span class="comment"># 随机产生50个用于改变散点面积的数值</span></span><br><span class="line">plt.scatter(x, y, c=colors, s=sizes, alpha=<span class="number">0.3</span>, cmap=<span class="string">&#x27;viridis&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1617087014246-68b4ea38-5310-4a2a-aff7-58625e2d98f5.png#align=left&display=inline&height=179&margin=%5Bobject%20Object%5D&name=image.png&originHeight=242&originWidth=370&size=26707&status=done&style=none&width=273" alt="image.png"></p><h2 id="cmap-plt-cm-Spectral-用法"><a href="#cmap-plt-cm-Spectral-用法" class="headerlink" title="cmap = plt.cm.Spectral 用法"></a>cmap = plt.cm.Spectral 用法</h2><p>例子 1<br>代码 plt.scatter(X, Y, c = label, s = 40, cmap = plt.cm.Spectral)中的 cmap = plt.cm.Spectral 是给定 label 中不同数值以不同的颜色。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">1</span>) <span class="comment"># 产生相同的随机数</span></span><br><span class="line">X = np.random.randn(<span class="number">1</span>, <span class="number">10</span>)</span><br><span class="line">Y = np.random.randn(<span class="number">1</span>, <span class="number">10</span>)</span><br><span class="line">label = np.array([<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>])</span><br><span class="line">plt.scatter(X, Y, c = label, s = <span class="number">180</span>, cmap = plt.cm.Spectral);</span><br></pre></td></tr></table></figure><p><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1617087090590-54f8811c-1d77-43f6-a73d-76f95044f656.png#align=left&display=inline&height=191&margin=%5Bobject%20Object%5D&name=image.png&originHeight=244&originWidth=374&size=9955&status=done&style=none&width=293" alt="image.png"></p><p>例子 2<br>3 个不同的类别被赋予了 3 种不同的颜色。原理是通过 plt.cm.Spectral(parameters)中的 parameters 来指定生成的颜色种类，例如 plt.cm.Spectral(np.arange(5))将生成 5 中不同的颜色，而在例子 2 中，c = label 代替了参数 parameter。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># label有三种不同的取值</span></span><br><span class="line">X = np.array(<span class="built_in">range</span>(<span class="number">1</span>, <span class="number">7</span>))</span><br><span class="line">Y = np.array(<span class="built_in">range</span>(<span class="number">1</span>, <span class="number">7</span>))</span><br><span class="line">label = (<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,-<span class="number">1</span>,-<span class="number">1</span>)</span><br><span class="line">plt.scatter(X, Y, c = label, s = <span class="number">180</span>, cmap = plt.cm.Spectral)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;matplotlib.collections.PathCollection at 0x13fda3da240&gt;</span><br></pre></td></tr></table></figure><p><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1617087068164-9b8992d0-e5af-462c-a8fa-bc90e6f81aad.png#align=left&display=inline&height=197&margin=%5Bobject%20Object%5D&name=image.png&originHeight=246&originWidth=354&size=6507&status=done&style=none&width=284" alt="image.png"></p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;开始之前&quot;&gt;&lt;a href=&quot;#开始之前&quot; class=&quot;headerlink&quot; title=&quot;开始之前&quot;&gt;&lt;/a&gt;开始之前&lt;/h1&gt;&lt;p&gt;学习 Matplotlib 绘图其实就是学习绘图函数中的参数！将参数活学活用，不同的参数搭配会产生不同的化学效应！&lt;/p&gt;
&lt;h1 id=&quot;相关配置&quot;&gt;&lt;a href=&quot;#相关配置&quot; class=&quot;headerlink&quot; title=&quot;相关配置&quot;&gt;&lt;/a&gt;相关配置&lt;/h1&gt;</summary>
    
    
    
    <category term="笔记" scheme="https://ytno1.github.io/categories/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="Matplotlib" scheme="https://ytno1.github.io/categories/%E7%AC%94%E8%AE%B0/Matplotlib/"/>
    
    
    <category term="笔记" scheme="https://ytno1.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="Matplotlib" scheme="https://ytno1.github.io/tags/Matplotlib/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch操作笔记</title>
    <link href="https://ytno1.github.io/archives/39ef698d.html"/>
    <id>https://ytno1.github.io/archives/39ef698d.html</id>
    <published>2021-03-30T06:43:20.000Z</published>
    <updated>2021-04-12T16:58:26.573Z</updated>
    
    <content type="html"><![CDATA[<p>tags: [笔记, Pytorch]<br>categories: [笔记, Pytorch]</p><h1 id="Pytorch-用法教程"><a href="#Pytorch-用法教程" class="headerlink" title="Pytorch 用法教程"></a>Pytorch 用法教程</h1><h2 id="相关配置"><a href="#相关配置" class="headerlink" title="相关配置"></a>相关配置</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#导入相关包</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment">#设置单元格全部行的输出结果</span></span><br><span class="line"><span class="keyword">from</span> IPython.core.interactiveshell <span class="keyword">import</span> InteractiveShell</span><br><span class="line">InteractiveShell.ast_node_interactivity = <span class="string">&quot;all&quot;</span></span><br></pre></td></tr></table></figure><span id="more"></span><h2 id="torch-cuda-Event"><a href="#torch-cuda-Event" class="headerlink" title="torch.cuda.Event()"></a>torch.cuda.Event()</h2><p>默认情况下，GPU 操作是异步的。当您调用使用 GPU 的函数时，操作将排入特定设备，但不一定要在以后执行。这允许我们并行执行更多计算，包括在 CPU 或其他 GPU 上的操作。</p><p>通常，异步计算的效果对于调用者是不可见的，因为 (1) 每个设备按照它们排队的顺序执行操作，以及 (2) PyTorch 在 CPU 和 GPU 之间或两个 GPU 之间复制数据时自动执行必要的同步。因此，计算将如同每个操作同步执行一样进行。</p><p>您可以通过设置环境变量强制进行同步计算 CUDA_LAUNCH_BLOCKING=1。这在 GPU 上发生错误时非常方便。(使用异步执行时，直到实际执行操作后才会报告此类错误，因此堆栈跟踪不会显示请求的位置。）</p><p>异步计算的结果是没有同步的时间测量是不精确的。要获得精确的测量结果，应该在测量之前调用 torch.cuda.synchronize()，或者使用 torch.cuda.Event 记录时间如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">start_event = torch.cuda.Event(enable_timing=<span class="literal">True</span>)</span><br><span class="line">end_event = torch.cuda.Event(enable_timing=<span class="literal">True</span>)</span><br><span class="line">start_event.record()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在这里执行一些操作</span></span><br><span class="line"></span><br><span class="line">end_event.record()</span><br><span class="line">torch.cuda.synchronize()  <span class="comment"># Wait for the events to be recorded!</span></span><br><span class="line">elapsed_time_ms = start_event.elapsed_time(end_event)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">---------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line">NameError                                 Traceback (most recent call last)</span><br><span class="line"></span><br><span class="line">&lt;ipython-input-1-ec7600a08e47&gt; in &lt;module&gt;</span><br><span class="line">----&gt; 1 start_event &#x3D; torch.cuda.Event(enable_timing&#x3D;True)</span><br><span class="line">      2 end_event &#x3D; torch.cuda.Event(enable_timing&#x3D;True)</span><br><span class="line">      3 start_event.record()</span><br><span class="line">      4</span><br><span class="line">      5 # 在这里执行一些操作</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">NameError: name &#39;torch&#39; is not defined</span><br></pre></td></tr></table></figure><h2 id="torch-clamp-操作用法"><a href="#torch-clamp-操作用法" class="headerlink" title="torch.clamp()操作用法"></a>torch.clamp()操作用法</h2><p>torch.clamp(input, min, max, out=None) → Tensor</p><p>使得输入的所有元素夹到[min, max]范围内。让 min_value 和 max_value 分别为 min 和 max，这会返回:y_i =min(max(x_i, min_value), max_value)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">4</span>)</span><br><span class="line">a</span><br><span class="line">torch.clamp(a, <span class="built_in">min</span>=-<span class="number">0.5</span>, <span class="built_in">max</span>=<span class="number">0.5</span>)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tensor([-0.5281, -1.0857,  0.4811, -0.2452])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tensor([-0.5000, -0.5000,  0.4811, -0.2452])</span><br></pre></td></tr></table></figure><h2 id="tirch-Tensor-cuda-用法"><a href="#tirch-Tensor-cuda-用法" class="headerlink" title="tirch.Tensor.cuda()用法"></a>tirch.Tensor.cuda()用法</h2><p>CPU tensor 转 GPU tensor</p><h3 id="torch-Tensor-的属性值-is-cuda"><a href="#torch-Tensor-的属性值-is-cuda" class="headerlink" title="torch.Tensor 的属性值 is_cuda"></a>torch.Tensor 的属性值 is_cuda</h3><p>is_cuda：Is True if the Tensor is stored on the GPU, False otherwise.</p><h2 id="torch-中的掩码"><a href="#torch-中的掩码" class="headerlink" title="torch 中的掩码"></a>torch 中的掩码</h2><h3 id="masked-fill"><a href="#masked-fill" class="headerlink" title="masked_fill()"></a>masked_fill()</h3><p>masked<em>fill</em>(mask, value)</p><p>Fills elements of self tensor with value where mask is True. The shape of mask must be broadcastable with the shape of the underlying tensor.在 mask 值为 1 的位置处用 value 填充。</p><p>参数： - mask (ByteTensor)-二进制掩码 - value (Tensor)-用来填充的值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a = torch.Tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line">a = a.masked_fill(mask = torch.LongTensor([<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>]).<span class="built_in">type</span>(torch.<span class="built_in">bool</span>), value=-np.inf)</span><br><span class="line"></span><br><span class="line">print(a)</span><br><span class="line">b = F.softmax(a,dim=<span class="number">0</span>)</span><br><span class="line">print(b)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([-inf, -inf, 3., 4.])</span><br><span class="line">tensor([0.0000, 0.0000, 0.2689, 0.7311])</span><br></pre></td></tr></table></figure><h2 id="torch-from-numpy-用法"><a href="#torch-from-numpy-用法" class="headerlink" title="torch.from_numpy()用法"></a>torch.from_numpy()用法</h2><p>Creates a Tensor from a numpy.ndarray.</p><p>用 torch.from_numpy 这个方法将 numpy 类转换成 tensor 类</p><h2 id="Pytorch-Tensor-的索引与切片"><a href="#Pytorch-Tensor-的索引与切片" class="headerlink" title="Pytorch Tensor 的索引与切片"></a>Pytorch Tensor 的索引与切片</h2><h3 id="一般索引"><a href="#一般索引" class="headerlink" title="一般索引"></a>一般索引</h3><p>根据 Tensor 的 shape，从前往后索引，依次在每个维度上做索引。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">a = torch.rand(<span class="number">4</span>, <span class="number">3</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line">print(a[<span class="number">0</span>].shape) <span class="comment">#取到第一个维度</span></span><br><span class="line">print(a[<span class="number">0</span>, <span class="number">0</span>].shape) <span class="comment"># 取到二个维度</span></span><br><span class="line">print(a[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">4</span>])  <span class="comment"># 具体到某个元素</span></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">  创建了一个shape=[4, 3, 28, 28]的Tensor，我们可以理解为4张图片，每张图片有3个通道，每个通道是28x28的图像数据。</span></span><br><span class="line"><span class="string">  a代表这个Tensor，a后面跟着的列表[]表示对Tensor进行索引，</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([3, 28, 28])</span><br><span class="line">torch.Size([28, 28])</span><br><span class="line">tensor(0.3142)</span><br></pre></td></tr></table></figure><h3 id="普通的切片索引"><a href="#普通的切片索引" class="headerlink" title="普通的切片索引"></a>普通的切片索引</h3><p>注意负值的索引即表示倒数第几个元素，-2 就是倒数第二个元素。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 譬如：4张图片，每张三个通道，每个通道28行28列的像素</span></span><br><span class="line">a = torch.rand(<span class="number">4</span>, <span class="number">3</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在第一个维度上取后0和1，等同于取第一、第二张图片</span></span><br><span class="line">print(a[:<span class="number">2</span>].shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在第一个维度上取0和1,在第二个维度上取0，</span></span><br><span class="line"><span class="comment"># 等同于取第一、第二张图片中的第一个通道</span></span><br><span class="line">print(a[:<span class="number">2</span>, :<span class="number">1</span>, :, :].shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在第一个维度上取0和1,在第二个维度上取1,2，</span></span><br><span class="line"><span class="comment"># 等同于取第一、第二张图片中的第二个通道与第三个通道</span></span><br><span class="line">print(a[:<span class="number">2</span>, <span class="number">1</span>:, :, :].shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在第一个维度上取0和1,在第二个维度上取1,2，</span></span><br><span class="line"><span class="comment"># 等同于取第一、第二张图片中的第二个通道与第三个通道</span></span><br><span class="line">print(a[:<span class="number">2</span>, -<span class="number">2</span>:, :, :].shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用step隔行采样</span></span><br><span class="line"><span class="comment"># 在第一、第二维度取所有元素，在第三、第四维度隔行采样</span></span><br><span class="line"><span class="comment"># 等同于所有图片所有通道的行列每个一行或者一列采样</span></span><br><span class="line"><span class="comment"># 注意：下面的代码不包括28</span></span><br><span class="line">print(a[:, :, <span class="number">0</span>:<span class="number">28</span>:<span class="number">2</span>, <span class="number">0</span>:<span class="number">28</span>:<span class="number">2</span>].shape)</span><br><span class="line">print(a[:, :, ::<span class="number">2</span>, ::<span class="number">2</span>].shape)  <span class="comment"># 等同于上面语句</span></span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([2, 3, 28, 28])</span><br><span class="line">torch.Size([2, 1, 28, 28])</span><br><span class="line">torch.Size([2, 2, 28, 28])</span><br><span class="line">torch.Size([2, 2, 28, 28])</span><br><span class="line">torch.Size([4, 3, 14, 14])</span><br><span class="line">torch.Size([4, 3, 14, 14])</span><br></pre></td></tr></table></figure><h3 id="index-select（）选择特定索引"><a href="#index-select（）选择特定索引" class="headerlink" title="index_select（）选择特定索引"></a>index_select（）选择特定索引</h3><p>选择特定下标有时候很有用，比如上面的 a 这个 Tensor 可以看作 4 张 RGB（3 通道）的 MNIST 图像，长宽都是 28px。那么在第一维度上可以选择特定的图片，在第二维度上选择特定的通道。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 选择第一张和第三张图</span></span><br><span class="line">print(a.index_select(<span class="number">0</span>, torch.tensor([<span class="number">0</span>, <span class="number">2</span>])).shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 选择R通道和B通道</span></span><br><span class="line">print(a.index_select(<span class="number">1</span>, torch.tensor([<span class="number">0</span>, <span class="number">2</span>])).shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 选择图像的0~8行</span></span><br><span class="line">print(a.index_select(<span class="number">2</span>, torch.arange(<span class="number">8</span>)).shape)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([2, 3, 28, 28])</span><br><span class="line">torch.Size([4, 2, 28, 28])</span><br><span class="line">torch.Size([4, 3, 8, 28])</span><br></pre></td></tr></table></figure><p>注意：index_select（）的第二个索引参数必须是 Tensor 类型</p><h3 id="任意多的维度"><a href="#任意多的维度" class="headerlink" title="任意多的维度"></a>任意多的维度</h3><p>在索引中使用…可以表示任意多的维度。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">a = torch.rand(<span class="number">4</span>, <span class="number">3</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 等与a</span></span><br><span class="line">print(a[...].shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第一张图片的所有维度</span></span><br><span class="line">print(a[<span class="number">0</span>, ...].shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 所有图片第二通道的所有维度</span></span><br><span class="line">print(a[:, <span class="number">1</span>, ...].shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 所有图像所有通道所有行的第一、第二列</span></span><br><span class="line">print(a[..., :<span class="number">2</span>].shape)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([4, 3, 28, 28])</span><br><span class="line">torch.Size([3, 28, 28])</span><br><span class="line">torch.Size([4, 28, 28])</span><br><span class="line">torch.Size([4, 3, 28, 2])</span><br></pre></td></tr></table></figure><h3 id="mask-索引"><a href="#mask-索引" class="headerlink" title="mask 索引"></a>mask 索引</h3><p>可以获取满足一些条件的值的位置索引，然后用这个索引去取出这些位置的元素。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">a = torch.randn(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">print(a)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成a这个Tensor中大于0.5的元素的掩码</span></span><br><span class="line">mask = a.ge(<span class="number">0.5</span>)</span><br><span class="line">print(mask)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.取出a这个Tensor中大于0.5的元素</span></span><br><span class="line">val = torch.masked_select(a, mask)</span><br><span class="line">print(val)</span><br><span class="line">print(val.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.or直接用mask索引</span></span><br><span class="line">print(a[mask])</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tensor([[-0.3195, -0.3623, -0.6834,  0.5384],</span><br><span class="line">        [ 0.8215,  0.4409,  1.2699,  1.6882],</span><br><span class="line">        [ 0.7391,  2.2220, -0.7828,  0.8776]])</span><br><span class="line">tensor([[False, False, False,  True],</span><br><span class="line">        [ True, False,  True,  True],</span><br><span class="line">        [ True,  True, False,  True]])</span><br><span class="line">tensor([0.5384, 0.8215, 1.2699, 1.6882, 0.7391, 2.2220, 0.8776])</span><br><span class="line">torch.Size([7])</span><br><span class="line">tensor([0.5384, 0.8215, 1.2699, 1.6882, 0.7391, 2.2220, 0.8776])</span><br></pre></td></tr></table></figure><p>注意：最后取出的 大于 0.5 的 Tensor 的 shape 已经被摊平(变为一维)。</p><h3 id="take-索引"><a href="#take-索引" class="headerlink" title="take 索引"></a>take 索引</h3><p>take 索引是基于目标 Tensor 的 flatten 形式下的，即摊平后的 Tensor 的索引。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">a = torch.tensor([[<span class="number">3</span>, <span class="number">7</span>, <span class="number">2</span>], [<span class="number">2</span>, <span class="number">8</span>, <span class="number">3</span>]])</span><br><span class="line">print(a)</span><br><span class="line">print(torch.take(a, torch.tensor([<span class="number">0</span>, <span class="number">1</span>, <span class="number">5</span>])))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor([[3, 7, 2],</span><br><span class="line">        [2, 8, 3]])</span><br><span class="line">tensor([3, 7, 3])</span><br></pre></td></tr></table></figure><h2 id="pytotch-中类似于-numpy-的花式索引-即以任意维-tensor-作为索引"><a href="#pytotch-中类似于-numpy-的花式索引-即以任意维-tensor-作为索引" class="headerlink" title="pytotch 中类似于 numpy 的花式索引,即以任意维 tensor 作为索引"></a>pytotch 中类似于 numpy 的花式索引,即以任意维 tensor 作为索引</h2><p>需整理，参考<a href="https://blog.csdn.net/xpy870663266/article/details/101597144">https://blog.csdn.net/xpy870663266/article/details/101597144</a></p><h3 id="一维-Tensor-作为索引"><a href="#一维-Tensor-作为索引" class="headerlink" title="一维 Tensor 作为索引"></a>一维 Tensor 作为索引</h3><p>在 Numpy 中，我们可以传入数组作为索引，称为花式索引。这里只演示使用两个一维 List 的例子。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a=np.arange(<span class="number">18</span>).reshape(<span class="number">6</span>,<span class="number">3</span>)</span><br><span class="line">a</span><br><span class="line">a[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>]]  <span class="comment"># 相当于选择了下标分别为[1,0], [2,1], [3,2]的元素</span></span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">array([[ 0,  1,  2],</span><br><span class="line">       [ 3,  4,  5],</span><br><span class="line">       [ 6,  7,  8],</span><br><span class="line">       [ 9, 10, 11],</span><br><span class="line">       [12, 13, 14],</span><br><span class="line">       [15, 16, 17]])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">array([ 3,  7, 11])</span><br></pre></td></tr></table></figure><p>而在 PyTorch 中，如果使用两个整数 List/一维 Tensor 作为索引，所起的作用是相同的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">w=torch.arange(<span class="number">18</span>).view(<span class="number">6</span>,<span class="number">3</span>)</span><br><span class="line">w</span><br><span class="line">w[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>]]</span><br><span class="line">w[torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]),torch.tensor([<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>])]</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ 0,  1,  2],</span><br><span class="line">        [ 3,  4,  5],</span><br><span class="line">        [ 6,  7,  8],</span><br><span class="line">        [ 9, 10, 11],</span><br><span class="line">        [12, 13, 14],</span><br><span class="line">        [15, 16, 17]])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tensor([ 3,  7, 11])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tensor([ 3,  7, 11])</span><br></pre></td></tr></table></figure><h3 id="二维-Tensor-作为索引"><a href="#二维-Tensor-作为索引" class="headerlink" title="二维 Tensor 作为索引"></a>二维 Tensor 作为索引</h3><p>下面的例子使用了二维 Tensor 作为索引，注意把[[1,2,3],[0,1,2]]和上一小节的两个一维 Tensor[1,2,3],[0,1,2]区分开。通过下面的例子可以发现，二维 Tensor 作为索引时，每个索引中的元素都作为 w 的第一维度的下标（即行号）用于选择 w 中第一维的元素。例如二维索引[[1,2,3],[0,1,2]]中的 3 选出了 w 的第四行[ 9, 10, 11]。 下面例子中，索引形状为[2,3]，将索引中的每个元素用被索引的 Tensor 中对应行号的行替换之后，由于每一行有三列，故得到了[2,3,3]的结果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">w</span><br><span class="line">w[torch.LongTensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>]])]</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ 0,  1,  2],</span><br><span class="line">        [ 3,  4,  5],</span><br><span class="line">        [ 6,  7,  8],</span><br><span class="line">        [ 9, 10, 11],</span><br><span class="line">        [12, 13, 14],</span><br><span class="line">        [15, 16, 17]])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tensor([[[ 3,  4,  5],</span><br><span class="line">         [ 6,  7,  8],</span><br><span class="line">         [ 9, 10, 11]],</span><br><span class="line"></span><br><span class="line">        [[ 0,  1,  2],</span><br><span class="line">         [ 3,  4,  5],</span><br><span class="line">         [ 6,  7,  8]]])</span><br></pre></td></tr></table></figure><h3 id="使用-Tensor-作为-List-的索引"><a href="#使用-Tensor-作为-List-的索引" class="headerlink" title="使用 Tensor 作为 List 的索引"></a>使用 Tensor 作为 List 的索引</h3><p>当 Tensor 仅含有一个整数时，可以作为 List 的索引，相当于取出该整数作为索引。若含有多个整数，则报错。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">a=[x <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>)]</span><br><span class="line">a</span><br><span class="line"></span><br><span class="line">a[torch.tensor([[<span class="number">1</span>]])]  <span class="comment"># 相当于a[1]</span></span><br><span class="line"></span><br><span class="line">a[torch.tensor([[[<span class="number">5</span>]]])]  <span class="comment"># 相当于a[5]</span></span><br><span class="line"></span><br><span class="line">a[torch.tensor([[<span class="number">1</span>,<span class="number">2</span>]])]  <span class="comment"># 多于1个整数，报错</span></span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">5</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">---------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line">TypeError                                 Traceback (most recent call last)</span><br><span class="line"></span><br><span class="line">&lt;ipython-input-15-ac7aa2680cfc&gt; in &lt;module&gt;</span><br><span class="line">      7 a[torch.tensor([[[5]]])]  # 相当于a[5]</span><br><span class="line">      8</span><br><span class="line">----&gt; 9 a[torch.tensor([[1,2]])]  # 多于1个整数，报错</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">TypeError: only integer tensors of a single element can be converted to an index</span><br></pre></td></tr></table></figure><h2 id="torch-cat-用法"><a href="#torch-cat-用法" class="headerlink" title="torch.cat()用法"></a>torch.cat()用法</h2><p>在 pytorch 中，常见的拼接函数主要是两个，分别是：stack()、cat()。</p><p>一般 torch.cat()是为了把函数 torch.stack()得到的 tensor 进行拼接而存在的。参考链接<a href="https://blog.csdn.net/xinjieyuan/article/details/105205326">torch.stack()</a>， 但是本文主要说 cat()。</p><p>torch.cat() 和 python 中的内置函数 cat()， 在使用和目的上，是没有区别的。</p><p>torch.cat(): 在给定维度上对输入的张量序列 seq 进行拼接操作。</p><p>语法：outputs = torch.cat(inputs, dim=0) → Tensor</p><p>参数<br>inputs : 待连接的张量序列，可以是任意相同 Tensor 类型的 python 序列<br>dim : 选择的扩维, 必须在 0 到 len(inputs[0])之间，沿着此维连接张量序列。</p><p><strong>注意</strong><br>输入数据必须是序列，序列中数据是任意相同的 shape 的同类型 tensor。<br>维度不可以超过输入数据的任一个张量的维度</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 准备数据，每个的shape都是[2,3]</span></span><br><span class="line"><span class="comment"># x1</span></span><br><span class="line">x1 = torch.tensor([[<span class="number">11</span>,<span class="number">21</span>,<span class="number">31</span>],[<span class="number">21</span>,<span class="number">31</span>,<span class="number">41</span>]],dtype=torch.<span class="built_in">int</span>)</span><br><span class="line">x1.shape <span class="comment"># torch.Size([2, 3])</span></span><br><span class="line"><span class="comment"># x2</span></span><br><span class="line">x2 = torch.tensor([[<span class="number">12</span>,<span class="number">22</span>,<span class="number">32</span>],[<span class="number">22</span>,<span class="number">32</span>,<span class="number">42</span>]],dtype=torch.<span class="built_in">int</span>)</span><br><span class="line">x2.shape  <span class="comment"># torch.Size([2, 3])</span></span><br><span class="line"><span class="comment"># 合成inputs</span></span><br><span class="line"><span class="string">&#x27;inputs为２个形状为[2 , 3]的矩阵 &#x27;</span></span><br><span class="line">inputs = [x1, x2]</span><br><span class="line">print(inputs)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看结果, 测试不同的dim拼接结果</span></span><br><span class="line">torch.cat(inputs, dim=<span class="number">0</span>).shape</span><br><span class="line"></span><br><span class="line">torch.cat(inputs, dim=<span class="number">1</span>).shape</span><br><span class="line"></span><br><span class="line">torch.cat(inputs, dim=<span class="number">2</span>).shape</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([2, 3])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">torch.Size([2, 3])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#39;inputs为２个形状为[2 , 3]的矩阵 &#39;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[tensor([[11, 21, 31],</span><br><span class="line">        [21, 31, 41]], dtype&#x3D;torch.int32), tensor([[12, 22, 32],</span><br><span class="line">        [22, 32, 42]], dtype&#x3D;torch.int32)]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">torch.Size([4, 3])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">torch.Size([2, 6])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">---------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line">IndexError                                Traceback (most recent call last)</span><br><span class="line"></span><br><span class="line">&lt;ipython-input-17-166e5a9372e2&gt; in &lt;module&gt;</span><br><span class="line">     16 torch.cat(inputs, dim&#x3D;1).shape</span><br><span class="line">     17</span><br><span class="line">---&gt; 18 torch.cat(inputs, dim&#x3D;2).shape</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">IndexError: Dimension out of range (expected to be in range of [-2, 1], but got 2)</span><br></pre></td></tr></table></figure><h2 id="nn-Embedding-用法"><a href="#nn-Embedding-用法" class="headerlink" title="nn.Embedding()用法"></a>nn.Embedding()用法</h2><p>个人理解：这是一个矩阵类，里面初始化了一个随机矩阵，矩阵行数是字典的大小，列宽是用来表示字典中每个元素的特征向量，向量的维度根据你想要表示的元素的复杂度而定。类实例化之后可以根据字典中元素的下标来查找元素对应的向量。</p><p>参数<br>num_embeddings (python:int) – 词典的大小尺寸，比如总共出现 5000 个词，那就输入 5000。此时 index 为（0-4999）<br>embedding_dim (python:int) – 嵌入向量的维度，即用多少维来表示一个元素。<br>padding_idx (python:int, optional) – 填充 id，如果给定，则填充时遇到 padding_idx 时，用（初始化为零）的嵌入向量来填充。<br>max_norm (python:float, optional) – 最大范数，如果嵌入向量的范数超过了这个界限，就要进行再归一化。<br>norm_type (python:float, optional) – 指定利用什么范数计算，并用于对比 max_norm，默认为 2 范数。<br>scale_grad_by_freq (boolean, optional) – 根据单词在 mini-batch 中出现的频率，对梯度进行放缩。默认为 False.<br>sparse (bool, optional) – 若为 True,则与权重矩阵相关的梯度转变为稀疏张量。</p><p>输入:(*)，嵌入矩阵，IntTensor or LongTensor</p><p>输出:(<em>，H)，其中</em>是输入形状，H = embeddding_dim</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># an Embedding module containing 10 tensors of size 3</span></span><br><span class="line">embedding = torch.nn.Embedding(<span class="number">10</span>, <span class="number">3</span>)</span><br><span class="line"><span class="comment"># a batch of 2 samples of 4 indices each</span></span><br><span class="line"><span class="built_in">input</span> = torch.LongTensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">4</span>,<span class="number">5</span>],[<span class="number">4</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">9</span>]])</span><br><span class="line">embedding(<span class="built_in">input</span>)</span><br><span class="line"><span class="comment"># example with padding_idx</span></span><br><span class="line">embedding = torch.nn.Embedding(<span class="number">10</span>, <span class="number">3</span>, padding_idx=<span class="number">5</span>)</span><br><span class="line"><span class="built_in">input</span> = torch.LongTensor([[<span class="number">0</span>,<span class="number">2</span>,<span class="number">0</span>,<span class="number">5</span>]])</span><br><span class="line">embedding(<span class="built_in">input</span>)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[ 0.8953, -0.4839, -0.6158],</span><br><span class="line">         [ 0.0080,  1.8228,  1.5201],</span><br><span class="line">         [ 1.9881,  1.6257,  0.3809],</span><br><span class="line">         [ 0.9000, -0.0256, -0.6423]],</span><br><span class="line"></span><br><span class="line">        [[ 1.9881,  1.6257,  0.3809],</span><br><span class="line">         [-2.5488, -0.0816, -1.1412],</span><br><span class="line">         [ 0.0080,  1.8228,  1.5201],</span><br><span class="line">         [ 1.5346, -0.1341,  0.7297]]], grad_fn&#x3D;&lt;EmbeddingBackward&gt;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tensor([[[ 0.9047, -1.0269, -1.8386],</span><br><span class="line">         [ 1.8361, -0.9710,  1.1253],</span><br><span class="line">         [ 0.9047, -1.0269, -1.8386],</span><br><span class="line">         [ 0.0000,  0.0000,  0.0000]]], grad_fn&#x3D;&lt;EmbeddingBackward&gt;)</span><br></pre></td></tr></table></figure><h2 id="nn-Linear-用法"><a href="#nn-Linear-用法" class="headerlink" title="nn.Linear()用法"></a>nn.Linear()用法</h2><p>语法：CLASS  torch.nn.Linear(in_features, out_features, bias=True)</p><p>参数</p><p>in_features -每个输入样本的大小</p><p>out_features -每个输出样本的大小</p><p>bias-如果设置为 False，层将不会学习附加偏差。默认值:真正的</p><p>Shape:<br>Input: (N, _, H_in) where _ means any number of additional dimensions and H_in = in_features</p><p>Output: (N, *, H_out) where all but the last dimension are the same shape as the input and H_out = out_features .</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">m = torch.nn.Linear(<span class="number">20</span>, <span class="number">30</span>)</span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">128</span>, <span class="number">20</span>)</span><br><span class="line">output = m(<span class="built_in">input</span>)</span><br><span class="line">print(output.size())</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([128, 30])</span><br></pre></td></tr></table></figure><h2 id="nn-Parameter-用法"><a href="#nn-Parameter-用法" class="headerlink" title="nn.Parameter()用法"></a>nn.Parameter()用法</h2><p>首先可以把这个函数理解为类型转换函数，将一个不可训练的类型 Tensor 转换成可以训练的类型 parameter 并将这个 parameter 绑定到这个 module 里面(net.parameter()中就有这个绑定的 parameter，所以在参数优化的时候可以进行优化的)，所以经过类型转换这个 Tensor 就成为了模型中根据训练可以改动的参数了。使用这个函数的目的也是想让某些变量在学习的过程中不断的修改其值以达到最优化。</p><h2 id="torch-repeat-sizes-gt-Tensor"><a href="#torch-repeat-sizes-gt-Tensor" class="headerlink" title="torch.repeat(*sizes) -&gt; Tensor"></a>torch.repeat(*sizes) -&gt; Tensor</h2><p>沿着指定的维度重复这个张量。</p><p>参数:sizes (torch.Size or int…),沿每个维度重复这个张量的次数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">x.repeat(<span class="number">4</span>, <span class="number">2</span>)</span><br><span class="line">x.repeat(<span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>).size()</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">tensor([[1, 2, 3, 1, 2, 3],</span><br><span class="line">        [1, 2, 3, 1, 2, 3],</span><br><span class="line">        [1, 2, 3, 1, 2, 3],</span><br><span class="line">        [1, 2, 3, 1, 2, 3]])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">torch.Size([4, 2, 3])</span><br></pre></td></tr></table></figure><h2 id="Tensors-操作"><a href="#Tensors-操作" class="headerlink" title="Tensors 操作"></a>Tensors 操作</h2><h2 id="torch-nn-utils-rnn-pad-sequence-用法"><a href="#torch-nn-utils-rnn-pad-sequence-用法" class="headerlink" title="torch.nn.utils.rnn.pad_sequence()用法"></a>torch.nn.utils.rnn.pad_sequence()用法</h2><p>torch.nn.utils.rnn.pad_sequence(sequences, batch_first=False, padding_value=0.0)</p><p>使用 padding_value 填充可变长度 Tensor 的列表</p><p>pad*sequence 沿新维度堆叠张量列表，并将它们填充为相等的长度。 例如，如果输入是大小为 L x <em>的序列列表，并且 batch</em>first 为 False，否则为 T x B x *。</p><p>B 是批次大小。它等于序列中元素的数量。T 是最长序列的长度。L 是序列的长度。*是任意数量的尾部维度，包括 None。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.nn.utils.rnn <span class="keyword">import</span> pad_sequence</span><br><span class="line">a = torch.ones(<span class="number">25</span>, <span class="number">300</span>)</span><br><span class="line">b = torch.ones(<span class="number">22</span>, <span class="number">300</span>)</span><br><span class="line">c = torch.ones(<span class="number">15</span>, <span class="number">300</span>)</span><br><span class="line">pad_sequence([a, b, c],batch_first=<span class="literal">True</span>).size()</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([3, 25, 300])</span><br></pre></td></tr></table></figure><h2 id="自动微分"><a href="#自动微分" class="headerlink" title="自动微分"></a>自动微分</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.ones(<span class="number">2</span>, <span class="number">2</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([[1., 1.],</span><br><span class="line">        [1., 1.]], requires_grad&#x3D;True)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y = x + <span class="number">2</span></span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([[3., 3.],</span><br><span class="line">        [3., 3.]], grad_fn&#x3D;&lt;AddBackward0&gt;)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">z = y * y * <span class="number">3</span></span><br><span class="line">out = z.mean()</span><br><span class="line"></span><br><span class="line">print(z, out)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([[27., 27.],</span><br><span class="line">        [27., 27.]], grad_fn&#x3D;&lt;MulBackward0&gt;) tensor(27., grad_fn&#x3D;&lt;MeanBackward0&gt;)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">out.backward()</span><br><span class="line"><span class="comment">#打印梯度 d(out)/dx</span></span><br><span class="line">print(x.grad)</span><br><span class="line"><span class="comment"># print(out.grad)</span></span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([[22.5000, 22.5000],</span><br><span class="line">        [22.5000, 22.5000]])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#现在让我们看一个雅可比向量积的例子：</span></span><br><span class="line">x = torch.randn(<span class="number">3</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = x * <span class="number">2</span></span><br><span class="line"><span class="keyword">while</span> y.data.norm() &lt; <span class="number">1000</span>:</span><br><span class="line">    y = y * <span class="number">2</span></span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([  -80.3993,   178.1791, -1355.1229], grad_fn&#x3D;&lt;MulBackward0&gt;)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">v = torch.tensor([<span class="number">0.1</span>, <span class="number">1.0</span>, <span class="number">0.0001</span>], dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">y.backward(v)</span><br><span class="line">print(x.grad)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([5.1200e+01, 5.1200e+02, 5.1200e-02])</span><br></pre></td></tr></table></figure><h2 id="torch-diag-用法"><a href="#torch-diag-用法" class="headerlink" title="torch.diag()用法"></a>torch.diag()用法</h2><p>torch.diag(input, diagonal=0, *, out=None) → Tensor<br>If input is a vector (1-D tensor), then returns a 2-D square tensor with the elements of input as the diagonal.<br>If input is a matrix (2-D tensor), then returns a 1-D tensor with the diagonal elements of input.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Get the square matrix where the input vector is the diagonal</span></span><br><span class="line">a = torch.randn(<span class="number">3</span>)</span><br><span class="line">a</span><br><span class="line">torch.diag(a)</span><br><span class="line">torch.diag(a, <span class="number">1</span>)</span><br><span class="line"><span class="comment">#Get the k-th diagonal of a given matrix</span></span><br><span class="line">a = torch.randn(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">a</span><br><span class="line">torch.diag(a, <span class="number">0</span>)</span><br><span class="line">torch.diag(a, <span class="number">1</span>)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">tensor([ 0.4590,  0.8841, -0.6945])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tensor([[ 0.4590,  0.0000,  0.0000],</span><br><span class="line">        [ 0.0000,  0.8841,  0.0000],</span><br><span class="line">        [ 0.0000,  0.0000, -0.6945]])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tensor([[ 0.0000,  0.4590,  0.0000,  0.0000],</span><br><span class="line">        [ 0.0000,  0.0000,  0.8841,  0.0000],</span><br><span class="line">        [ 0.0000,  0.0000,  0.0000, -0.6945],</span><br><span class="line">        [ 0.0000,  0.0000,  0.0000,  0.0000]])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tensor([[ 0.3824, -0.6560,  1.8126],</span><br><span class="line">        [ 0.0241, -0.8781, -1.3364],</span><br><span class="line">        [-0.5124,  2.1753,  0.1158]])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tensor([ 0.3824, -0.8781,  0.1158])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tensor([-0.6560, -1.3364])</span><br></pre></td></tr></table></figure><h2 id="torch-sign-用法"><a href="#torch-sign-用法" class="headerlink" title="torch.sign()用法"></a>torch.sign()用法</h2><p>torch.sign(input, *, out=None) → Tensor<br>Returns a new tensor with the signs of the elements of input.即输出 input 通过 sign 函数后的张量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = torch.tensor([<span class="number">0.7</span>, -<span class="number">1.2</span>, <span class="number">0.</span>, <span class="number">2.3</span>])</span><br><span class="line">print(a)</span><br><span class="line">torch.sign(a)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tensor([ 0.7000, -1.2000,  0.0000,  2.3000])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tensor([ 1., -1.,  0.,  1.])</span><br></pre></td></tr></table></figure><h2 id="torch-t-用法"><a href="#torch-t-用法" class="headerlink" title="torch.t()用法"></a>torch.t()用法</h2><p>torch.t(input) → Tensor<br>Expects input to be &lt;= 2-D tensor and transposes dimensions 0 and 1.<br>0-D and 1-D tensors are returned as is. When input is a 2-D tensor this is equivalent to transpose(input, 0, 1).</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(())</span><br><span class="line">x</span><br><span class="line">torch.t(x)</span><br><span class="line">x = torch.randn(<span class="number">3</span>)</span><br><span class="line">x</span><br><span class="line">torch.t(x)</span><br><span class="line">x = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">x</span><br><span class="line">torch.t(x)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">tensor(1.7658)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tensor(1.7658)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tensor([ 0.8564,  0.4923, -1.0440])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tensor([ 0.8564,  0.4923, -1.0440])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tensor([[-0.1147, -0.5785,  1.7228],</span><br><span class="line">        [ 0.0507, -1.0331,  0.7053]])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tensor([[-0.1147,  0.0507],</span><br><span class="line">        [-0.5785, -1.0331],</span><br><span class="line">        [ 1.7228,  0.7053]])</span><br></pre></td></tr></table></figure><h2 id="pytorch-中-、-用法"><a href="#pytorch-中-、-用法" class="headerlink" title="pytorch 中@、*用法"></a>pytorch 中@、*用法</h2><p>@:对 tensor 进行矩阵相乘<br>*:对 tensor 进行矩阵进行逐元素相乘</p><h2 id="torch-norm-用法"><a href="#torch-norm-用法" class="headerlink" title="torch.norm()用法"></a>torch.norm()用法</h2><p>torch.norm(input, p=’fro’, dim=None, keepdim=False, out=None, dtype=None)  #dim：要缩减的维度<br>Returns the matrix norm or vector norm of a given tensor.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a = torch.rand((<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line">at = torch.norm(a,p=<span class="number">2</span>,dim=<span class="number">1</span>,keepdim=<span class="literal">True</span>)   <span class="comment">#保持维度</span></span><br><span class="line">af = torch.norm(a,p=<span class="number">2</span>,dim=<span class="number">1</span>,keepdim=<span class="literal">False</span>)  <span class="comment">#不保持维度</span></span><br><span class="line">print(a.shape)</span><br><span class="line">print(at.shape)</span><br><span class="line">print(af.shape)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([2, 3, 4])</span><br><span class="line">torch.Size([2, 1, 4])</span><br><span class="line">torch.Size([2, 4])</span><br></pre></td></tr></table></figure><h2 id="torch-einsum-用法"><a href="#torch-einsum-用法" class="headerlink" title="torch.einsum()用法"></a>torch.einsum()用法</h2><p>爱因斯坦简记法：是一种由爱因斯坦提出的，对向量、矩阵、张量的求和运算的求和简记法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#矩阵求和</span></span><br><span class="line">a = torch.arange(<span class="number">6</span>).view(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">b = torch.arange(<span class="number">6</span>,<span class="number">12</span>).reshape(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">torch.einsum(<span class="string">&#x27;ij,ij-&gt;&#x27;</span>, [a, b])</span><br><span class="line"><span class="comment">#矩阵外积</span></span><br><span class="line">a = torch.arange(<span class="number">3</span>)</span><br><span class="line"><span class="comment"># a</span></span><br><span class="line">b = torch.arange(<span class="number">3</span>,<span class="number">7</span>)</span><br><span class="line"><span class="comment"># b</span></span><br><span class="line">torch.einsum(<span class="string">&#x27;i,j-&gt;ij&#x27;</span>, [a, b])</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">tensor(145)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tensor([[ 0,  0,  0,  0],</span><br><span class="line">        [ 3,  4,  5,  6],</span><br><span class="line">        [ 6,  8, 10, 12]])</span><br></pre></td></tr></table></figure><h2 id="torch-sum-用法"><a href="#torch-sum-用法" class="headerlink" title="torch.sum()用法"></a>torch.sum()用法</h2><p>参数<br>input (Tensor) – the input tensor.<br>dim (int or tuple of python:ints) – the dimension or dimensions to reduce. 要缩减的维度<br>keepdim (bool) – whether the output tensor has dim retained or not.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">b = torch.arange(<span class="number">2</span> * <span class="number">3</span> * <span class="number">4</span>).view(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"><span class="comment"># b</span></span><br><span class="line">torch.<span class="built_in">sum</span>(b, (<span class="number">2</span>, <span class="number">1</span>))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([ 66, 210])</span><br></pre></td></tr></table></figure><h2 id="一维-Tensor-与一维-Array-的形状区别"><a href="#一维-Tensor-与一维-Array-的形状区别" class="headerlink" title="一维 Tensor 与一维 Array 的形状区别"></a>一维 Tensor 与一维 Array 的形状区别</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.tensor([<span class="number">1</span>,<span class="number">2</span>]).shape</span><br><span class="line">np.array([<span class="number">1</span>,<span class="number">2</span>]).shape</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([2])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">(2,)</span><br></pre></td></tr></table></figure><h2 id="torch-max-用法"><a href="#torch-max-用法" class="headerlink" title="torch.max()用法"></a>torch.max()用法</h2><p>torch.max(input, dim, keepdim=False, *, out=None) -&gt; (Tensor, LongTensor)</p><p>Returns a namedtuple (values, indices) where values is the maximum value of each row of the input tensor in the given dimension dim. And indices is the index location of each maximum value found (argmax).<br>If keepdim is True, the output tensors are of the same size as input except in the dimension dim where they are of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in the output tensors having 1 fewer dimension than input.</p><p>Parameters<br>input (Tensor) – the input tensor.<br>dim (int) – the dimension to reduce.<br>keepdim (bool) – whether the output tensor has dim retained or not. Default: False.</p><p>Keyword Arguments<br>out (tuple, optional) – the result tuple of two output tensors (max, max_indices)</p><p>a = torch.randn(4, 4)<br>a<br>torch.max(a, 1) #只取值不取索引<br>torch.max(a, 1).values</p><h2 id="torch-cumsum-用法"><a href="#torch-cumsum-用法" class="headerlink" title="torch.cumsum 用法"></a>torch.cumsum 用法</h2><p>torch.cumsum(input, dim, *, dtype=None, out=None) → Tensor<br>Returns the cumulative sum of elements of input in the dimension dim.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">10</span>)</span><br><span class="line">a</span><br><span class="line">torch.cumsum(a, dim=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">tensor([ 0.3418, -0.9971, -1.3957, -0.2646, -0.8354, -1.3760,  0.3042,  0.4878,</span><br><span class="line">         0.2004, -0.8370])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tensor([ 0.3418, -0.6552, -2.0509, -2.3155, -3.1509, -4.5268, -4.2227, -3.7349,</span><br><span class="line">        -3.5345, -4.3715])</span><br></pre></td></tr></table></figure><h2 id="PyTorch-使用-tensorboardX-SummaryWriter"><a href="#PyTorch-使用-tensorboardX-SummaryWriter" class="headerlink" title="PyTorch 使用 tensorboardX.SummaryWriter"></a>PyTorch 使用 tensorboardX.SummaryWriter</h2><p>tensorboardX 用于训练、验证时自动记录数据绘图。但大多数情况只看 loss,lr,accu 这些曲线，就先总结这些，什么 images,audios 以后需要再总结。</p><ol><li>安装：pip install tensorboardX</li><li>调用<br>from tensorboardX import SummaryWriter<br>writer = SummaryWriter(‘log’)<br>writer 相当于日志，保存你要做图的所有信息。SummaryWriter(‘log’)会在当前项目目录下建立一个文件夹 log，存放画图用的文件。刚开始的时候是空的。<br>训练的循环中，每次写入图像名称，loss 数值， n_iteration：writer.add_scalar(‘Train/Loss’, loss.data[0], niter)<br>验证的循环中，写入预测的准确度即可：writer.add_scalar(‘Test/Accu’, correct/total, niter)<br>为了看得清楚一点，我把整个 train_eval 写一起了<br>def train_eval(epoch):<br>running_loss = 0.0<br>for i, data in enumerate(trainloader, 0):<br>inputs, labels = data<br>inputs, labels = Variable(inputs), Variable(labels)<br>optimizer.zero_grad()<br>outputs = net(inputs)<br>loss = criterion(outputs, labels)<br>loss.backward()<br>optimizer.step()<br>running_loss += loss.data[0] #每 2000 个 batch 显示一次当前的 loss 和 accu<br>if i % 2000 == 1999:<br>print(‘[epoch: %d, batch: %5d] loss: %.3f’ %<br>(epoch + 1, i+1, running_loss / 2000))<br>running_loss = 0.0<br>print(‘[epoch: %d, batch: %5d] Accu: %.3f’ %(epoch + 1, i+1, correct/total))``` #每 10 个 batch 画个点用于 loss 曲线<br>if i % 10 == 0:<br>niter = epoch * len(trainloader) + i<br>writer.add_scalar(‘Train/Loss’, loss.data[0], niter)</li></ol><p>#每 500 个 batch 全验证集检测，画个点用于 Accu<br>if i % 500 == 0:<br>correct = 0<br>total = 0<br>for data in testloader:<br>images, target = data<br>res = net(Variable(images))<br>_, predicted = torch.max(res.data, 1)<br>total += labels.size(0)<br>correct += (predicted == target).sum()<br>writer.add_scalar(‘Test/Accu’, correct/total, niter)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">3.显示</span><br><span class="line"></span><br><span class="line">会发现刚刚的log文件夹里面有文件了。在命令行输入如下，载入刚刚做图的文件（那个.&#x2F;log要写完整的路径）</span><br><span class="line"></span><br><span class="line">tensorboard --logdir&#x3D;.&#x2F;log</span><br><span class="line"></span><br><span class="line">在浏览器输入：</span><br><span class="line"></span><br><span class="line">[http:&#x2F;&#x2F;0.0.0.0:6006&#x2F;](http:&#x2F;&#x2F;0.0.0.0:6006&#x2F;)</span><br><span class="line"></span><br><span class="line">就可以看到我们做的两个图了</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;tags: [笔记, Pytorch]&lt;br&gt;categories: [笔记, Pytorch]&lt;/p&gt;
&lt;h1 id=&quot;Pytorch-用法教程&quot;&gt;&lt;a href=&quot;#Pytorch-用法教程&quot; class=&quot;headerlink&quot; title=&quot;Pytorch 用法教程&quot;&gt;&lt;/a&gt;Pytorch 用法教程&lt;/h1&gt;&lt;h2 id=&quot;相关配置&quot;&gt;&lt;a href=&quot;#相关配置&quot; class=&quot;headerlink&quot; title=&quot;相关配置&quot;&gt;&lt;/a&gt;相关配置&lt;/h2&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;#导入相关包&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; torch&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; numpy &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; np&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;#设置单元格全部行的输出结果&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; IPython.core.interactiveshell &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; InteractiveShell&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;InteractiveShell.ast_node_interactivity = &lt;span class=&quot;string&quot;&gt;&amp;quot;all&amp;quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>Numpy操作笔记</title>
    <link href="https://ytno1.github.io/archives/3fcbc08e.html"/>
    <id>https://ytno1.github.io/archives/3fcbc08e.html</id>
    <published>2021-03-30T06:43:15.000Z</published>
    <updated>2021-04-12T16:58:26.606Z</updated>
    
    <content type="html"><![CDATA[<h1 id="基础配置"><a href="#基础配置" class="headerlink" title="基础配置"></a>基础配置</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="comment">#设置单元格全部行的输出结果</span></span><br><span class="line"><span class="keyword">from</span> IPython.core.interactiveshell <span class="keyword">import</span> InteractiveShell</span><br><span class="line">InteractiveShell.ast_node_interactivity = <span class="string">&quot;all&quot;</span></span><br></pre></td></tr></table></figure><span id="more"></span><h1 id="np-concatenate-用法"><a href="#np-concatenate-用法" class="headerlink" title="np.concatenate()用法"></a>np.concatenate()用法</h1><p>concatenate 功能：数组拼接</p><p>函数定义：numpy.concatenate((a1, a2, …), axis=0, out=None)</p><p>Parameters:</p><p>a1, a2, … :  数组序列，除了 axis 对应的轴，数组其他维度必须一样</p><p>axis :  可选，默认是 0</p><p>out :  多维数组，可选，如果 out 提供，则结果保存在 out 中。数组大小必须正确，与真实结果相匹配。</p><p>Returns: res :  多维数组，最后拼接好的数组</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line">b = np.array([[<span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line">np.concatenate((a, b), axis=<span class="number">0</span>)</span><br><span class="line">np.concatenate((a, b.T), axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">array([[1, 2],</span><br><span class="line">       [3, 4],</span><br><span class="line">       [5, 6]])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">array([[1, 2, 5],</span><br><span class="line">       [3, 4, 6]])</span><br></pre></td></tr></table></figure><h1 id="numpy-empty-用法"><a href="#numpy-empty-用法" class="headerlink" title="numpy.empty()用法"></a>numpy.empty()用法</h1><p>numpy.empty 方法用来创建一个指定形状（shape）、数据类型（dtype）、顺序且未初始化（随机）的数组：</p><p>numpy.empty(shape, dtype = float, order = ‘C’)</p><p>参数说明<br>shape 数组形状</p><p>dtype 数据类型，可选</p><p>order 有”C”和”F”两个选项,分别代表，行优先和列优先，在计算机内存中的存储元素的顺序。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">x = np.empty([<span class="number">3</span>,<span class="number">2</span>], dtype = <span class="built_in">int</span>,order=<span class="string">&#x27;C&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span> (x)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[[ 538976288 1092624416]</span><br><span class="line"> [1701410336 1718558839]</span><br><span class="line"> [1617780768 1953068832]]</span><br></pre></td></tr></table></figure><h1 id="取上、下三角矩阵（常用于-mask）"><a href="#取上、下三角矩阵（常用于-mask）" class="headerlink" title="取上、下三角矩阵（常用于 mask）"></a>取上、下三角矩阵（常用于 mask）</h1><h2 id="np-tiru-用法"><a href="#np-tiru-用法" class="headerlink" title="np.tiru()用法"></a>np.tiru()用法</h2><p>语法：np.triu(m, k=0)   m：表示一个矩阵，K：表示对角线的起始位置（k 取值默认为 0），取矩阵的上三角矩阵</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">data=torch.arange(<span class="number">25</span>).reshape((<span class="number">5</span>,<span class="number">5</span>))</span><br><span class="line">data</span><br><span class="line"><span class="comment">#k=0表示正常的上三角矩阵</span></span><br><span class="line">upper_triangle = np.triu(data, <span class="number">0</span>)</span><br><span class="line">upper_triangle</span><br><span class="line"><span class="comment">#k=-1表示对角线的位置下移1个对角线</span></span><br><span class="line">upper_triangle = np.triu(data, -<span class="number">1</span>)</span><br><span class="line">upper_triangle</span><br><span class="line"><span class="comment">#k=1表示对角线的位置上移1个对角线</span></span><br><span class="line">upper_triangle = np.triu(data, <span class="number">1</span>)</span><br><span class="line">upper_triangle</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ 0,  1,  2,  3,  4],</span><br><span class="line">        [ 5,  6,  7,  8,  9],</span><br><span class="line">        [10, 11, 12, 13, 14],</span><br><span class="line">        [15, 16, 17, 18, 19],</span><br><span class="line">        [20, 21, 22, 23, 24]])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">array([[ 0,  1,  2,  3,  4],</span><br><span class="line">       [ 0,  6,  7,  8,  9],</span><br><span class="line">       [ 0,  0, 12, 13, 14],</span><br><span class="line">       [ 0,  0,  0, 18, 19],</span><br><span class="line">       [ 0,  0,  0,  0, 24]], dtype&#x3D;int64)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">array([[ 0,  1,  2,  3,  4],</span><br><span class="line">       [ 5,  6,  7,  8,  9],</span><br><span class="line">       [ 0, 11, 12, 13, 14],</span><br><span class="line">       [ 0,  0, 17, 18, 19],</span><br><span class="line">       [ 0,  0,  0, 23, 24]], dtype&#x3D;int64)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">array([[ 0,  1,  2,  3,  4],</span><br><span class="line">       [ 0,  0,  7,  8,  9],</span><br><span class="line">       [ 0,  0,  0, 13, 14],</span><br><span class="line">       [ 0,  0,  0,  0, 19],</span><br><span class="line">       [ 0,  0,  0,  0,  0]], dtype&#x3D;int64)</span><br></pre></td></tr></table></figure><h2 id="tril（m，-k）用法"><a href="#tril（m，-k）用法" class="headerlink" title="tril（m， k）用法"></a>tril（m， k）用法</h2><p>tril（m， k）取矩阵的下三角阵</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">lower_triangle = np.tril(data, <span class="number">0</span>)</span><br><span class="line">lower_triangle</span><br><span class="line">lower_triangle = np.tril(data, -<span class="number">1</span>)</span><br><span class="line">lower_triangle</span><br><span class="line">lower_triangle = np.tril(data, <span class="number">1</span>)</span><br><span class="line">lower_triangle</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">array([[ 0,  0,  0,  0,  0],</span><br><span class="line">       [ 5,  6,  0,  0,  0],</span><br><span class="line">       [10, 11, 12,  0,  0],</span><br><span class="line">       [15, 16, 17, 18,  0],</span><br><span class="line">       [20, 21, 22, 23, 24]], dtype&#x3D;int64)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">array([[ 0,  0,  0,  0,  0],</span><br><span class="line">       [ 5,  0,  0,  0,  0],</span><br><span class="line">       [10, 11,  0,  0,  0],</span><br><span class="line">       [15, 16, 17,  0,  0],</span><br><span class="line">       [20, 21, 22, 23,  0]], dtype&#x3D;int64)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">array([[ 0,  1,  0,  0,  0],</span><br><span class="line">       [ 5,  6,  7,  0,  0],</span><br><span class="line">       [10, 11, 12, 13,  0],</span><br><span class="line">       [15, 16, 17, 18, 19],</span><br><span class="line">       [20, 21, 22, 23, 24]], dtype&#x3D;int64)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a=np.triu(np.ones((<span class="number">1</span>, <span class="number">10</span>, <span class="number">10</span>)), k=<span class="number">1</span>)</span><br><span class="line">a</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">array([[[0., 1., 1., 1., 1., 1., 1., 1., 1., 1.],</span><br><span class="line">        [0., 0., 1., 1., 1., 1., 1., 1., 1., 1.],</span><br><span class="line">        [0., 0., 0., 1., 1., 1., 1., 1., 1., 1.],</span><br><span class="line">        [0., 0., 0., 0., 1., 1., 1., 1., 1., 1.],</span><br><span class="line">        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],</span><br><span class="line">        [0., 0., 0., 0., 0., 0., 1., 1., 1., 1.],</span><br><span class="line">        [0., 0., 0., 0., 0., 0., 0., 1., 1., 1.],</span><br><span class="line">        [0., 0., 0., 0., 0., 0., 0., 0., 1., 1.],</span><br><span class="line">        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],</span><br><span class="line">        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;基础配置&quot;&gt;&lt;a href=&quot;#基础配置&quot; class=&quot;headerlink&quot; title=&quot;基础配置&quot;&gt;&lt;/a&gt;基础配置&lt;/h1&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; numpy &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; np&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; torch&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; torch.nn &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; nn&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;#设置单元格全部行的输出结果&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; IPython.core.interactiveshell &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; InteractiveShell&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;InteractiveShell.ast_node_interactivity = &lt;span class=&quot;string&quot;&gt;&amp;quot;all&amp;quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>jupyter notebook使用</title>
    <link href="https://ytno1.github.io/archives/db642960.html"/>
    <id>https://ytno1.github.io/archives/db642960.html</id>
    <published>2021-03-30T03:32:53.000Z</published>
    <updated>2021-04-12T16:58:26.643Z</updated>
    
    <content type="html"><![CDATA[<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>notebook 开启服务器，以网页形式打开，单元格分为 code 类型和 markdown 类型；code 类型单元格用于编写和运行代码，markdown 类型单元格则用于编写说明文档，做及时的说明和解释 。</p><h1 id="好处："><a href="#好处：" class="headerlink" title="好处："></a>好处：</h1><p>可以重复使用先前写好的代码，并保存相应数据至变量中，其他单元格可以调取使用</p><span id="more"></span><h1 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a>基本操作</h1><p>1、新建文件与导入文件</p><p>2、单元格分类：code、markdown</p><p>3、命令模式（仅仅表示为选中，蓝色边框）与编辑模式（绿色边框）</p><p>4、常用快捷键<br>单元格类型转换：Y、M；插入单元格：A、B；进入命令模式：Esc；代码补全：tab；运行单元格：ctrl+enter；删除单元格：DD；选中方框变蓝色后，按下键盘上的小写 L：显示行数；shift/alt+enter：在当前单元格的下面新建一个单元格</p><h1 id="markdown-语法"><a href="#markdown-语法" class="headerlink" title="markdown 语法"></a>markdown 语法</h1><p>标题：使用 1<del>6 个#跟随一个空格来表示 1</del>6 级标题</p><p>无序列表：使用*，-或+后跟随一个空格来表示</p><p>有序列表：使用数字+点表示不换行有序，使用数字+点+空格表示换行有序</p><p>换行：使用两个或以上的空行</p><p>代码：可以使用<code>代码</code>（反引号：英文状态下，1 左边的按键）来标记代码部分，使用<code>语言类型 代码块</code>来标记代码块</p><p>分割线：3 个星号***或 3 个减号—</p><p>链接与图片：[文字]（链接地址） <img src="%E5%9B%BE%E7%89%87%E9%93%BE%E6%8E%A5%E5%9C%B0%E5%9D%80" alt="图像未加载成功的说明" title="图片说明信息"></p><p>文字加粗：<strong>这是加粗的文字</strong></p><p>斜体文字：<em>这是斜体文字</em></p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h1&gt;&lt;p&gt;notebook 开启服务器，以网页形式打开，单元格分为 code 类型和 markdown 类型；code 类型单元格用于编写和运行代码，markdown 类型单元格则用于编写说明文档，做及时的说明和解释 。&lt;/p&gt;
&lt;h1 id=&quot;好处：&quot;&gt;&lt;a href=&quot;#好处：&quot; class=&quot;headerlink&quot; title=&quot;好处：&quot;&gt;&lt;/a&gt;好处：&lt;/h1&gt;&lt;p&gt;可以重复使用先前写好的代码，并保存相应数据至变量中，其他单元格可以调取使用&lt;/p&gt;</summary>
    
    
    
    <category term="快捷键" scheme="https://ytno1.github.io/categories/%E5%BF%AB%E6%8D%B7%E9%94%AE/"/>
    
    <category term="notebook" scheme="https://ytno1.github.io/categories/%E5%BF%AB%E6%8D%B7%E9%94%AE/notebook/"/>
    
    
    <category term="快捷键" scheme="https://ytno1.github.io/tags/%E5%BF%AB%E6%8D%B7%E9%94%AE/"/>
    
    <category term="notebook" scheme="https://ytno1.github.io/tags/notebook/"/>
    
  </entry>
  
  <entry>
    <title>np.linspace()用法</title>
    <link href="https://ytno1.github.io/archives/c539c65b.html"/>
    <id>https://ytno1.github.io/archives/c539c65b.html</id>
    <published>2021-03-29T14:04:31.000Z</published>
    <updated>2021-04-12T16:58:25.542Z</updated>
    
    <content type="html"><![CDATA[<h1 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h1><p>numpy.linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None)</p><h1 id="作用"><a href="#作用" class="headerlink" title="作用"></a>作用</h1><p>在指定的间隔内返回均匀间隔的数字。返回 num 个以均匀间隔分布的样本，在[start, stop]。这个区间的末端点可以通过 endpoint=False 来排除在外。</p><span id="more"></span><h1 id="Parameters-参数"><a href="#Parameters-参数" class="headerlink" title="Parameters(参数):"></a>Parameters(参数):</h1><ol><li><p>start : scalar(标量)，The starting value of the sequence(序列的起始点).</p></li><li><p>stop : scalar，序列的结束点，除非 endpoint 被设置为 False，在这种情况下, the sequence consists of all but the last of num + 1 evenly spaced samples(该序列由除最后一个样本(num + 1)以外的所有均匀间隔的样本组成), 不包括 stop.当 endpoint=False 的时候注意步长的大小(下面有例子).</p></li><li><p>num : int, optional(可选),生成的样本数，默认是 50。必须是非负。</p></li><li><p>endpoint : bool, optional,如果是真，则一定包括 stop，如果为 False，一定不会有 stop</p></li><li><p>retstep : bool, optional,If True, return (samples, step), where step is the spacing between samples.(看例子)</p></li><li><p>dtype : dtype, optional,The type of the output array. If dtype is not given, infer the data type from the other input arguments(推断这个输入用例从其他的输入中).</p></li></ol><h1 id="Returns"><a href="#Returns" class="headerlink" title="Returns:"></a>Returns:</h1><ol><li><p>samples : ndarray,There are num equally spaced samples in the closed interval [start, stop] or the half-open interval [start, stop) (depending on whether endpoint is True or False).</p></li><li><p>step : float(只有当 retstep 设置为真的时候才会存在),Only returned if retstep is True, Size of spacing between samples.</p></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># space: (3.0-2.0)/(5-1)=0.25</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.linspace(<span class="number">2.0</span>, <span class="number">3.0</span>, num=<span class="number">5</span>)</span><br><span class="line">    array([ <span class="number">2.</span>  ,  <span class="number">2.25</span>,  <span class="number">2.5</span> ,  <span class="number">2.75</span>,  <span class="number">3.</span>  ])</span><br><span class="line"><span class="comment"># step: (3.0-2.0)/5=0.2,因为要生成num+1个样本，所以step为6-1=5</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.linspace(<span class="number">2.0</span>, <span class="number">3.0</span>, num=<span class="number">5</span>, endpoint=<span class="literal">False</span>)</span><br><span class="line">    array([ <span class="number">2.</span> ,  <span class="number">2.2</span>,  <span class="number">2.4</span>,  <span class="number">2.6</span>,  <span class="number">2.8</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.linspace(<span class="number">2.0</span>, <span class="number">3.0</span>, num=<span class="number">5</span>, retstep=<span class="literal">True</span>)</span><br><span class="line">    (array([ <span class="number">2.</span>  ,  <span class="number">2.25</span>,  <span class="number">2.5</span> ,  <span class="number">2.75</span>,  <span class="number">3.</span>  ]), <span class="number">0.25</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;语法&quot;&gt;&lt;a href=&quot;#语法&quot; class=&quot;headerlink&quot; title=&quot;语法&quot;&gt;&lt;/a&gt;语法&lt;/h1&gt;&lt;p&gt;numpy.linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None)&lt;/p&gt;
&lt;h1 id=&quot;作用&quot;&gt;&lt;a href=&quot;#作用&quot; class=&quot;headerlink&quot; title=&quot;作用&quot;&gt;&lt;/a&gt;作用&lt;/h1&gt;&lt;p&gt;在指定的间隔内返回均匀间隔的数字。返回 num 个以均匀间隔分布的样本，在[start, stop]。这个区间的末端点可以通过 endpoint=False 来排除在外。&lt;/p&gt;</summary>
    
    
    
    <category term="笔记" scheme="https://ytno1.github.io/categories/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="Numpy" scheme="https://ytno1.github.io/categories/%E7%AC%94%E8%AE%B0/Numpy/"/>
    
    <category term="np.linspace" scheme="https://ytno1.github.io/categories/%E7%AC%94%E8%AE%B0/Numpy/np-linspace/"/>
    
    
    <category term="笔记" scheme="https://ytno1.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="Numpy" scheme="https://ytno1.github.io/tags/Numpy/"/>
    
    <category term="np.linspace" scheme="https://ytno1.github.io/tags/np-linspace/"/>
    
  </entry>
  
  <entry>
    <title>在 Matplotlib 中设置轴的范围</title>
    <link href="https://ytno1.github.io/archives/338dafb9.html"/>
    <id>https://ytno1.github.io/archives/338dafb9.html</id>
    <published>2021-03-29T13:45:03.000Z</published>
    <updated>2021-04-12T16:58:25.572Z</updated>
    
    <content type="html"><![CDATA[<h1 id="开始之前"><a href="#开始之前" class="headerlink" title="开始之前"></a>开始之前</h1><ol><li>xlim() 和 ylim() 在 Matplotlib 中设置轴的限制</li><li>set_xlim() 和 set_ylim() 方法来设置轴限制</li><li>使用 axis() 方法在 Matplotlib 中设置轴的限制</li></ol><p>为了设置 X 轴的范围限制，我们可以使用  <code>xlim()</code>  和  <code>set_xlim()</code>  方法。类似地，为 Y 轴设置范围限制，我们可以使用  <code>ylim()</code>  和  <code>set_ylim()</code>  方法。我们也可以使用  <code>axis()</code>  方法来控制两个轴的范围。</p><span id="more"></span><h1 id="未限制坐标范围的原始图形"><a href="#未限制坐标范围的原始图形" class="headerlink" title="未限制坐标范围的原始图形"></a>未限制坐标范围的原始图形</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">x=np.linspace(<span class="number">1</span>,<span class="number">10</span>,<span class="number">500</span>)</span><br><span class="line">y=np.sin(<span class="number">2</span> * np.pi * x)+<span class="number">1</span></span><br><span class="line"></span><br><span class="line">fig = plt.figure(figsize=(<span class="number">8</span>, <span class="number">6</span>))</span><br><span class="line">plt.plot(x,y)</span><br><span class="line">plt.title(<span class="string">&quot;Plot without limiting axes&quot;</span>,fontsize=<span class="number">25</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;x&quot;</span>,fontsize=<span class="number">18</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;1+sinx&quot;</span>,fontsize=<span class="number">18</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1617026257146-f23d5b33-5b13-449d-903f-d37ea123a262.png#align=left&display=inline&height=248&margin=%5Bobject%20Object%5D&originHeight=600&originWidth=800&size=0&status=done&style=none&width=330"><br>如果不使用 xlim()和 ylim()函数来限制轴的范围，则该图的整个轴范围为 ：X 轴范围从 0 到 10，而 Y 轴范围从 0 到 2。</p><h1 id="xlim-和-ylim-在-Matplotlib-中设置轴的限制"><a href="#xlim-和-ylim-在-Matplotlib-中设置轴的限制" class="headerlink" title="xlim() 和 ylim() 在 Matplotlib 中设置轴的限制"></a>xlim() 和 ylim() 在 Matplotlib 中设置轴的限制</h1><p>matplotlib.pyplot.xlim() 和 matplotlib.pyplot.ylim() 可用于分别设置或获取 X 轴和 Y 轴的范围限制。如果在这些方法中传递参数，则它们将设置各个轴的极限，如果不传递任何参数，则将获得各个轴的范围。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">x=np.linspace(<span class="number">0</span>,<span class="number">10</span>,<span class="number">500</span>)</span><br><span class="line">y=np.sin(<span class="number">2</span> * np.pi * x)+<span class="number">1</span></span><br><span class="line"></span><br><span class="line">fig = plt.figure(figsize=(<span class="number">8</span>, <span class="number">6</span>))</span><br><span class="line">plt.plot(x,y)</span><br><span class="line">plt.title(<span class="string">&quot;Setting range of Axes&quot;</span>,fontsize=<span class="number">25</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;x&quot;</span>,fontsize=<span class="number">18</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;1+sinx&quot;</span>,fontsize=<span class="number">18</span>)</span><br><span class="line">plt.xlim(<span class="number">4</span>,<span class="number">8</span>)</span><br><span class="line">plt.ylim(-<span class="number">0.5</span>,<span class="number">2.5</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1617026106452-3ab8433e-80b5-4cae-a412-dee5bc6eb8a7.png#align=left&display=inline&height=262&margin=%5Bobject%20Object%5D&originHeight=600&originWidth=800&size=0&status=done&style=none&width=349"><br>这会将 X 轴的范围限制为 4-8，而将 Y 轴的范围限制为-0.5-2.5。</p><h1 id="set-xlim-和-set-ylim-方法来设置轴限制"><a href="#set-xlim-和-set-ylim-方法来设置轴限制" class="headerlink" title="set_xlim() 和 set_ylim() 方法来设置轴限制"></a>set_xlim() 和 set_ylim() 方法来设置轴限制</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">x=np.linspace(<span class="number">0</span>,<span class="number">10</span>,<span class="number">500</span>)</span><br><span class="line">y=np.sin(<span class="number">2</span> * np.pi * x)+<span class="number">1</span></span><br><span class="line"></span><br><span class="line">fig = plt.figure(figsize=(<span class="number">8</span>, <span class="number">6</span>))</span><br><span class="line">axes = plt.axes()</span><br><span class="line">axes.set_xlim([<span class="number">4</span>, <span class="number">8</span>])</span><br><span class="line">axes.set_ylim([-<span class="number">0.5</span>, <span class="number">2.5</span>])</span><br><span class="line">plt.plot(x,y)</span><br><span class="line">plt.title(<span class="string">&quot;Setting range of Axes&quot;</span>,fontsize=<span class="number">25</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;x&quot;</span>,fontsize=<span class="number">18</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;1+sinx&quot;</span>,fontsize=<span class="number">18</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1617026474998-d40be601-a908-45ac-b403-12e53ced96be.png#align=left&display=inline&height=234&margin=%5Bobject%20Object%5D&originHeight=600&originWidth=800&size=0&status=done&style=none&width=312"></p><h1 id="使用-axis-方法在-Matplotlib-中设置轴的限制"><a href="#使用-axis-方法在-Matplotlib-中设置轴的限制" class="headerlink" title="使用 axis() 方法在 Matplotlib 中设置轴的限制"></a>使用 axis() 方法在 Matplotlib 中设置轴的限制</h1><p>我们还可以使用 matplotlib.pyplot.axis() 来设置轴的范围限制。语法如下：<br>plt.axis([xmin, xmax, ymin, ymax])<br>该方法避免了需要单独控制 X 轴和 Y 轴范围的麻烦。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">x=np.linspace(<span class="number">0</span>,<span class="number">10</span>,<span class="number">50</span>)</span><br><span class="line">y=np.sin(<span class="number">2</span> * np.pi * x)+<span class="number">1</span></span><br><span class="line"></span><br><span class="line">fig = plt.figure(figsize=(<span class="number">8</span>, <span class="number">6</span>))</span><br><span class="line">plt.axis([<span class="number">4</span>, <span class="number">9</span>, -<span class="number">0.5</span>, <span class="number">2.5</span>])</span><br><span class="line">plt.plot(x,y)</span><br><span class="line">plt.title(<span class="string">&quot;Setting range of Axes&quot;</span>,fontsize=<span class="number">25</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;x&quot;</span>,fontsize=<span class="number">18</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;1+sinx&quot;</span>,fontsize=<span class="number">18</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1617026647822-e4709c9e-ee4e-47fc-b938-aee1f2d1a559.png#align=left&display=inline&height=230&margin=%5Bobject%20Object%5D&originHeight=600&originWidth=800&size=0&status=done&style=none&width=307"></p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;开始之前&quot;&gt;&lt;a href=&quot;#开始之前&quot; class=&quot;headerlink&quot; title=&quot;开始之前&quot;&gt;&lt;/a&gt;开始之前&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;xlim() 和 ylim() 在 Matplotlib 中设置轴的限制&lt;/li&gt;
&lt;li&gt;set_xlim() 和 set_ylim() 方法来设置轴限制&lt;/li&gt;
&lt;li&gt;使用 axis() 方法在 Matplotlib 中设置轴的限制&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;为了设置 X 轴的范围限制，我们可以使用  &lt;code&gt;xlim()&lt;/code&gt;  和  &lt;code&gt;set_xlim()&lt;/code&gt;  方法。类似地，为 Y 轴设置范围限制，我们可以使用  &lt;code&gt;ylim()&lt;/code&gt;  和  &lt;code&gt;set_ylim()&lt;/code&gt;  方法。我们也可以使用  &lt;code&gt;axis()&lt;/code&gt;  方法来控制两个轴的范围。&lt;/p&gt;</summary>
    
    
    
    <category term="笔记" scheme="https://ytno1.github.io/categories/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="Matplotlib" scheme="https://ytno1.github.io/categories/%E7%AC%94%E8%AE%B0/Matplotlib/"/>
    
    <category term="set_xlim()" scheme="https://ytno1.github.io/categories/%E7%AC%94%E8%AE%B0/Matplotlib/set-xlim/"/>
    
    
    <category term="笔记" scheme="https://ytno1.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="Matplotlib" scheme="https://ytno1.github.io/tags/Matplotlib/"/>
    
    <category term="set_xlim()" scheme="https://ytno1.github.io/tags/set-xlim/"/>
    
  </entry>
  
  <entry>
    <title>pyplot.gca() 用法</title>
    <link href="https://ytno1.github.io/archives/3b195ead.html"/>
    <id>https://ytno1.github.io/archives/3b195ead.html</id>
    <published>2021-03-29T13:14:13.000Z</published>
    <updated>2021-04-12T16:58:25.582Z</updated>
    
    <content type="html"><![CDATA[<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>可以使用 plt.gcf()和 plt.gca()获得当前的图表和坐标轴，分别表示 Get Current Figure 和 Get Current Axes。在 pyplot 模块中，许多函数都是对当前的 Figure 或 Axes 对象进行处理，比如说：plt.plot()实际上会通过 plt.gca()获得当前的 Axes 对象 ax，然后再调用 ax.plot()方法实现真正的绘图。</p><span id="more"></span><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Signature: plt.gca(**kwargs)</span><br><span class="line">Docstring:</span><br><span class="line">Get the current :<span class="class"><span class="keyword">class</span>:</span>`~matplotlib.axes.Axes` instance on the</span><br><span class="line">current figure matching the given keyword args, <span class="keyword">or</span> create one.</span><br><span class="line"></span><br><span class="line">Examples</span><br><span class="line">--------</span><br><span class="line">To get the current polar axes on the current figure::</span><br><span class="line"></span><br><span class="line">    plt.gca(projection=<span class="string">&#x27;polar&#x27;</span>)</span><br><span class="line"></span><br><span class="line">If the current axes doesn<span class="string">&#x27;t exist, or isn&#x27;</span>t a polar one, the appropriate</span><br><span class="line">axes will be created <span class="keyword">and</span> then returned.</span><br></pre></td></tr></table></figure><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h1&gt;&lt;p&gt;可以使用 plt.gcf()和 plt.gca()获得当前的图表和坐标轴，分别表示 Get Current Figure 和 Get Current Axes。在 pyplot 模块中，许多函数都是对当前的 Figure 或 Axes 对象进行处理，比如说：plt.plot()实际上会通过 plt.gca()获得当前的 Axes 对象 ax，然后再调用 ax.plot()方法实现真正的绘图。&lt;/p&gt;</summary>
    
    
    
    <category term="笔记" scheme="https://ytno1.github.io/categories/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="Matplotlib" scheme="https://ytno1.github.io/categories/%E7%AC%94%E8%AE%B0/Matplotlib/"/>
    
    <category term="pyplot.gca()" scheme="https://ytno1.github.io/categories/%E7%AC%94%E8%AE%B0/Matplotlib/pyplot-gca/"/>
    
    
    <category term="笔记" scheme="https://ytno1.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="Matplotlib" scheme="https://ytno1.github.io/tags/Matplotlib/"/>
    
    <category term="pyplot.gca()" scheme="https://ytno1.github.io/tags/pyplot-gca/"/>
    
  </entry>
  
  <entry>
    <title>np.nansum()、np.nanmean()用法</title>
    <link href="https://ytno1.github.io/archives/d479017a.html"/>
    <id>https://ytno1.github.io/archives/d479017a.html</id>
    <published>2021-03-29T12:16:49.000Z</published>
    <updated>2021-04-12T16:58:25.598Z</updated>
    
    <content type="html"><![CDATA[<h1 id="相关概念（NaN、Inf、None）"><a href="#相关概念（NaN、Inf、None）" class="headerlink" title="相关概念（NaN、Inf、None）"></a>相关概念（NaN、Inf、None）</h1><p><strong>NaN: Not a Number</strong>：NaN 在 numpy 和 pandas 中用于标识空缺数据，但是 NaN 却是一个 Number 类型（特殊的 float），代表一个“不是数字”的值，这个值不能直接进行运算。<br><strong>Inf：Infinity(无穷大）</strong><br><strong>None：</strong>None 在 python 中用于标识空缺数据，None 是一个 python 特殊的数据类型。</p><span id="more"></span><h1 id="np-nansum-和-np-nanmean-用法"><a href="#np-nansum-和-np-nanmean-用法" class="headerlink" title="np.nansum()和 np.nanmean()用法"></a>np.nansum()和 np.nanmean()用法</h1><p>在进行 numpy 数组求和、均值时，如果这个数组里包含了 nan，则程序会报错或者求出来的值是 nan，如下代码所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>arr = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, np.nan])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>arr.<span class="built_in">sum</span>()</span><br><span class="line">nan</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>arr.mean()</span><br><span class="line">nan</span><br></pre></td></tr></table></figure><p>使用 np.nansum()、np.nanmean()在求和、均值时忽略 nan</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>arr = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, np.nan])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.nansum(arr) <span class="comment"># np.nansum()中：nan取值为 0</span></span><br><span class="line"><span class="number">10.0</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.nanmean(arr) <span class="comment"># np.nanmean()中：nan取值为0且取均值时忽略它</span></span><br><span class="line"><span class="number">2.5</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;相关概念（NaN、Inf、None）&quot;&gt;&lt;a href=&quot;#相关概念（NaN、Inf、None）&quot; class=&quot;headerlink&quot; title=&quot;相关概念（NaN、Inf、None）&quot;&gt;&lt;/a&gt;相关概念（NaN、Inf、None）&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;NaN: Not a Number&lt;/strong&gt;：NaN 在 numpy 和 pandas 中用于标识空缺数据，但是 NaN 却是一个 Number 类型（特殊的 float），代表一个“不是数字”的值，这个值不能直接进行运算。&lt;br&gt;&lt;strong&gt;Inf：Infinity(无穷大）&lt;/strong&gt;&lt;br&gt;&lt;strong&gt;None：&lt;/strong&gt;None 在 python 中用于标识空缺数据，None 是一个 python 特殊的数据类型。&lt;/p&gt;</summary>
    
    
    
    <category term="笔记" scheme="https://ytno1.github.io/categories/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="Numpy" scheme="https://ytno1.github.io/categories/%E7%AC%94%E8%AE%B0/Numpy/"/>
    
    <category term="np.nansum()" scheme="https://ytno1.github.io/categories/%E7%AC%94%E8%AE%B0/Numpy/np-nansum/"/>
    
    
    <category term="笔记" scheme="https://ytno1.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="Numpy" scheme="https://ytno1.github.io/tags/Numpy/"/>
    
    <category term="np.nansum()" scheme="https://ytno1.github.io/tags/np-nansum/"/>
    
  </entry>
  
  <entry>
    <title>Numpy常见矩阵运算</title>
    <link href="https://ytno1.github.io/archives/745ee1a.html"/>
    <id>https://ytno1.github.io/archives/745ee1a.html</id>
    <published>2021-03-29T10:50:45.000Z</published>
    <updated>2021-04-12T16:58:25.630Z</updated>
    
    <content type="html"><![CDATA[<h1 id="开始之前"><a href="#开始之前" class="headerlink" title="开始之前"></a>开始之前</h1><p>机器学习中的损失函数：<img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1617015272315-88ef6eaa-61a0-4499-8314-b8d2fb2581b7.png#align=left&display=inline&height=27&margin=%5Bobject%20Object%5D&name=image.png&originHeight=27&originWidth=252&size=3160&status=done&style=none&width=252" alt="image.png"><br>代码实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">logprobs = np.multiply(-np.log(a3),Y) + np.multiply(-np.log(<span class="number">1</span> - a3), <span class="number">1</span> - Y)</span><br><span class="line">loss = <span class="number">1.</span>/m * np.nansum(logprobs)</span><br></pre></td></tr></table></figure><span id="more"></span><h1 id="numpy-中-array-和-matrix-的区别："><a href="#numpy-中-array-和-matrix-的区别：" class="headerlink" title="numpy 中 array 和 matrix 的区别："></a>numpy 中 array 和 matrix 的区别：</h1><p>matrix 是 array 的分支，matrix 和 array 操作在多数情况下都是一样的，但 array 操作更灵活，速度更快。</p><h1 id="创建矩阵或数组"><a href="#创建矩阵或数组" class="headerlink" title="创建矩阵或数组"></a>创建矩阵或数组</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a = np.array([<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line">print(a)</span><br><span class="line">print(a.shape,a.ndim,a.dtype,a.dtype.name,a.itemsize,a.size,a.data)</span><br><span class="line"></span><br><span class="line">b = np.arange(<span class="number">12</span>).reshape(<span class="number">4</span>,<span class="number">3</span>)</span><br><span class="line">b</span><br><span class="line"></span><br><span class="line">m = np.asmatrix(b)</span><br><span class="line"><span class="built_in">type</span>(m)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">2</span> <span class="number">3</span> <span class="number">4</span>]</span><br><span class="line"><span class="string">&#x27;int32&#x27;</span></span><br><span class="line">(<span class="number">3</span>,) <span class="number">1</span> int32 int32 <span class="number">4</span> <span class="number">3</span> &lt;memory at <span class="number">0x000001AF30C23348</span>&gt;</span><br><span class="line">array([[ <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">2</span>],</span><br><span class="line">       [ <span class="number">3</span>,  <span class="number">4</span>,  <span class="number">5</span>],</span><br><span class="line">       [ <span class="number">6</span>,  <span class="number">7</span>,  <span class="number">8</span>],</span><br><span class="line">       [ <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>]])</span><br><span class="line">numpy.matrix</span><br></pre></td></tr></table></figure><h1 id="广播运算"><a href="#广播运算" class="headerlink" title="广播运算"></a>广播运算</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 广播运算</span></span><br><span class="line">a = np.array( [<span class="number">20</span>,<span class="number">30</span>,<span class="number">40</span>,<span class="number">50</span>] )</span><br><span class="line">b = np.arange(<span class="number">4</span>)</span><br><span class="line">b</span><br><span class="line">c = a-b</span><br><span class="line">c</span><br><span class="line">b**<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="number">10</span>*np.sin(a)</span><br><span class="line"></span><br><span class="line">a&lt;<span class="number">35</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">array([<span class="number">20</span>, <span class="number">29</span>, <span class="number">38</span>, <span class="number">47</span>])</span><br><span class="line">array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">9</span>], dtype=int32)</span><br><span class="line">array([ <span class="number">9.12945251</span>, -<span class="number">9.88031624</span>,  <span class="number">7.4511316</span> , -<span class="number">2.62374854</span>])</span><br><span class="line">array([ <span class="literal">True</span>,  <span class="literal">True</span>, <span class="literal">False</span>, <span class="literal">False</span>])</span><br></pre></td></tr></table></figure><h1 id="矩阵乘法（元素乘、矩阵乘）"><a href="#矩阵乘法（元素乘、矩阵乘）" class="headerlink" title="矩阵乘法（元素乘、矩阵乘）"></a>矩阵乘法（元素乘、矩阵乘）</h1><p>元素乘法：np.multiply(a,b)<br>矩阵乘法：np.dot(a,b) 或 np.matmul(a,b) 或 a.dot(b) 或直接用 a @ b !<br>唯独注意：*，在 np.array 中重载为元素乘法，在 np.matrix 中重载为矩阵乘法!</p><h2 id="对于-np-array-对象"><a href="#对于-np-array-对象" class="headerlink" title="对于 np.array 对象"></a>对于 np.array 对象</h2><h3 id="元素乘法-用-a-b-或-np-multiply-a-b"><a href="#元素乘法-用-a-b-或-np-multiply-a-b" class="headerlink" title="元素乘法 用 a*b 或 np.multiply(a,b)"></a>元素乘法 用 a*b 或 np.multiply(a,b)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">array([[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">       [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a*a</span><br><span class="line">array([[ <span class="number">1</span>,  <span class="number">4</span>],</span><br><span class="line">       [ <span class="number">9</span>, <span class="number">16</span>]])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.multiply(a,a)</span><br><span class="line">array([[ <span class="number">1</span>,  <span class="number">4</span>],</span><br><span class="line">       [ <span class="number">9</span>, <span class="number">16</span>]])</span><br></pre></td></tr></table></figure><h3 id="矩阵乘法-用-np-dot-a-b-或-np-matmul-a-b-或-a-dot-b-或“-”"><a href="#矩阵乘法-用-np-dot-a-b-或-np-matmul-a-b-或-a-dot-b-或“-”" class="headerlink" title="矩阵乘法  用 np.dot(a,b) 或 np.matmul(a,b) 或 a.dot(b)或“@”"></a><em>矩阵乘法</em>  用 np.dot(a,b) 或 np.matmul(a,b) 或 a.dot(b)或“@”</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.dot(a,a)</span><br><span class="line">array([[ <span class="number">7</span>, <span class="number">10</span>],</span><br><span class="line">       [<span class="number">15</span>, <span class="number">22</span>]])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.matmul(a,a)</span><br><span class="line">array([[ <span class="number">7</span>, <span class="number">10</span>],</span><br><span class="line">       [<span class="number">15</span>, <span class="number">22</span>]])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.dot(a)</span><br><span class="line">array([[ <span class="number">7</span>, <span class="number">10</span>],</span><br><span class="line">       [<span class="number">15</span>, <span class="number">22</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a@a</span><br><span class="line">array([[ <span class="number">7</span>, <span class="number">10</span>],</span><br><span class="line">       [<span class="number">15</span>, <span class="number">22</span>]])</span><br></pre></td></tr></table></figure><h2 id="对于-np-matrix-对象"><a href="#对于-np-matrix-对象" class="headerlink" title="对于 np.matrix 对象"></a>对于 np.matrix 对象</h2><h3 id="元素乘法-用-np-multiply-a-b"><a href="#元素乘法-用-np-multiply-a-b" class="headerlink" title="元素乘法 用 np.multiply(a,b)"></a><em>元素乘法</em> 用 np.multiply(a,b)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>A</span><br><span class="line">matrix([[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.multiply(A,A)</span><br><span class="line">matrix([[ <span class="number">1</span>,  <span class="number">4</span>],</span><br><span class="line">        [ <span class="number">9</span>, <span class="number">16</span>]])</span><br></pre></td></tr></table></figure><h3 id="矩阵乘法-用-“-”或“-”-或-np-dot-a-b-或-np-matmul-a-b-或-a-dot-b"><a href="#矩阵乘法-用-“-”或“-”-或-np-dot-a-b-或-np-matmul-a-b-或-a-dot-b" class="headerlink" title="矩阵乘法  用 “*”或“@” 或 np.dot(a,b) 或 np.matmul(a,b) 或 a.dot(b)"></a><em>矩阵乘法</em>  用 “*”或“@” 或 np.dot(a,b) 或 np.matmul(a,b) 或 a.dot(b)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>A*A</span><br><span class="line">matrix([[ <span class="number">7</span>, <span class="number">10</span>],</span><br><span class="line">        [<span class="number">15</span>, <span class="number">22</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>A@A</span><br><span class="line">matrix([[ <span class="number">7</span>, <span class="number">10</span>],</span><br><span class="line">        [<span class="number">15</span>, <span class="number">22</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.dot(A,A)</span><br><span class="line">matrix([[ <span class="number">7</span>, <span class="number">10</span>],</span><br><span class="line">        [<span class="number">15</span>, <span class="number">22</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.matmul(A,A)</span><br><span class="line">matrix([[ <span class="number">7</span>, <span class="number">10</span>],</span><br><span class="line">        [<span class="number">15</span>, <span class="number">22</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>A.dot(A)</span><br><span class="line">matrix([[ <span class="number">7</span>, <span class="number">10</span>],</span><br><span class="line">        [<span class="number">15</span>, <span class="number">22</span>]])</span><br></pre></td></tr></table></figure><h2 id="矩阵的其他操作（转置、求逆…）"><a href="#矩阵的其他操作（转置、求逆…）" class="headerlink" title="矩阵的其他操作（转置、求逆…）"></a>矩阵的其他操作（转置、求逆…）</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a = np.array([[<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>]])</span><br><span class="line">print(a)</span><br><span class="line"></span><br><span class="line">a.transpose() <span class="comment"># 转置</span></span><br><span class="line">np.linalg.inv(a) <span class="comment"># 求逆</span></span><br><span class="line"></span><br><span class="line">u = np.eye(<span class="number">2</span>) <span class="comment"># 2x2 unit matrix; &quot;eye&quot; represents &quot;I&quot;</span></span><br><span class="line">u</span><br><span class="line">j = np.array([[<span class="number">0.0</span>, -<span class="number">1.0</span>], [<span class="number">1.0</span>, <span class="number">0.0</span>]])</span><br><span class="line">j</span><br><span class="line">j @ j        <span class="comment"># matrix product</span></span><br><span class="line"></span><br><span class="line">np.trace(u)  <span class="comment"># trace</span></span><br><span class="line">y = np.array([[<span class="number">5.</span>], [<span class="number">7.</span>]])</span><br><span class="line">np.linalg.solve(a, y)</span><br><span class="line"></span><br><span class="line">np.linalg.eig(j)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">1.</span> <span class="number">2.</span>]</span><br><span class="line"> [<span class="number">3.</span> <span class="number">4.</span>]]</span><br><span class="line">array([[<span class="number">1.</span>, <span class="number">3.</span>],</span><br><span class="line">       [<span class="number">2.</span>, <span class="number">4.</span>]])</span><br><span class="line">array([[-<span class="number">2.</span> ,  <span class="number">1.</span> ],</span><br><span class="line">       [ <span class="number">1.5</span>, -<span class="number">0.5</span>]])</span><br><span class="line">array([[<span class="number">1.</span>, <span class="number">0.</span>],</span><br><span class="line">       [<span class="number">0.</span>, <span class="number">1.</span>]])</span><br><span class="line">array([[ <span class="number">0.</span>, -<span class="number">1.</span>],</span><br><span class="line">       [ <span class="number">1.</span>,  <span class="number">0.</span>]])</span><br><span class="line">array([[-<span class="number">1.</span>,  <span class="number">0.</span>],</span><br><span class="line">       [ <span class="number">0.</span>, -<span class="number">1.</span>]])</span><br><span class="line"><span class="number">2.0</span></span><br><span class="line">array([[-<span class="number">3.</span>],</span><br><span class="line">       [ <span class="number">4.</span>]])</span><br><span class="line">(array([<span class="number">0.</span>+<span class="number">1.j</span>, <span class="number">0.</span>-<span class="number">1.j</span>]),</span><br><span class="line"> array([[<span class="number">0.70710678</span>+<span class="number">0.j</span>        , <span class="number">0.70710678</span>-<span class="number">0.j</span>        ],</span><br><span class="line">        [<span class="number">0.</span>        -<span class="number">0.70710678j</span>, <span class="number">0.</span>        +<span class="number">0.70710678j</span>]]))</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;开始之前&quot;&gt;&lt;a href=&quot;#开始之前&quot; class=&quot;headerlink&quot; title=&quot;开始之前&quot;&gt;&lt;/a&gt;开始之前&lt;/h1&gt;&lt;p&gt;机器学习中的损失函数：&lt;img src=&quot;https://cdn.nlark.com/yuque/0/2021/png/12870328/1617015272315-88ef6eaa-61a0-4499-8314-b8d2fb2581b7.png#align=left&amp;display=inline&amp;height=27&amp;margin=%5Bobject%20Object%5D&amp;name=image.png&amp;originHeight=27&amp;originWidth=252&amp;size=3160&amp;status=done&amp;style=none&amp;width=252&quot; alt=&quot;image.png&quot;&gt;&lt;br&gt;代码实现：&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;logprobs = np.multiply(-np.log(a3),Y) + np.multiply(-np.log(&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt; - a3), &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt; - Y)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;loss = &lt;span class=&quot;number&quot;&gt;1.&lt;/span&gt;/m * np.nansum(logprobs)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;</summary>
    
    
    
    <category term="笔记" scheme="https://ytno1.github.io/categories/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="Numpy" scheme="https://ytno1.github.io/categories/%E7%AC%94%E8%AE%B0/Numpy/"/>
    
    <category term="矩阵运算" scheme="https://ytno1.github.io/categories/%E7%AC%94%E8%AE%B0/Numpy/%E7%9F%A9%E9%98%B5%E8%BF%90%E7%AE%97/"/>
    
    
    <category term="笔记" scheme="https://ytno1.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="Numpy" scheme="https://ytno1.github.io/tags/Numpy/"/>
    
    <category term="矩阵运算" scheme="https://ytno1.github.io/tags/%E7%9F%A9%E9%98%B5%E8%BF%90%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>语雀快捷键操作</title>
    <link href="https://ytno1.github.io/archives/e116e878.html"/>
    <id>https://ytno1.github.io/archives/e116e878.html</id>
    <published>2021-03-29T10:27:30.000Z</published>
    <updated>2021-04-12T16:58:25.634Z</updated>
    
    
    
    
    
  </entry>
  
  <entry>
    <title>Sklearn.dataset</title>
    <link href="https://ytno1.github.io/archives/9e1de974.html"/>
    <id>https://ytno1.github.io/archives/9e1de974.html</id>
    <published>2021-03-27T09:15:12.000Z</published>
    <updated>2021-04-12T16:58:25.508Z</updated>
    
    <content type="html"><![CDATA[<h1 id="相关配置"><a href="#相关配置" class="headerlink" title="相关配置"></a>相关配置</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sklearn</span><br></pre></td></tr></table></figure><h1 id="sklearn-datasets-使用"><a href="#sklearn-datasets-使用" class="headerlink" title="sklearn.datasets 使用"></a>sklearn.datasets 使用</h1><p>sklearn 的数据集库 datasets 提供很多不同的数据集，主要包含以下几大类：</p><ol><li>玩具数据集</li><li>真实世界中的数据集</li><li>样本生成器</li><li>样本图片</li><li>svmlight 或 libsvm 格式的数据</li><li>从<a href="http://openml.org下载的数据/">http://openml.org下载的数据</a></li><li>从外部加载的数据<br>用的比较多的就是 1 和 3，这里进行主要介绍，其他的会进行简单介绍，但是不建议使用。</li></ol><span id="more"></span><h2 id="样本生成器"><a href="#样本生成器" class="headerlink" title="样本生成器"></a>样本生成器</h2><p>scikit-learn 包括各种随机样本的生成器，可以用来建立可控制的大小和复杂性人工数据集。</p><h3 id="用于分类和聚类的生成器"><a href="#用于分类和聚类的生成器" class="headerlink" title="用于分类和聚类的生成器"></a>用于分类和聚类的生成器</h3><p>(一) 簇 datasets.make_blobs()</p><p>1.n_samples:样本数 2.n_features:特征数（维度）3.centers:中心数，也可以是中心的坐标 4.cluster_std:簇的方差</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line">centers = [[<span class="number">2</span>,<span class="number">2</span>],[<span class="number">8</span>,<span class="number">2</span>],[<span class="number">2</span>,<span class="number">8</span>],[<span class="number">8</span>,<span class="number">8</span>]]</span><br><span class="line">x, y = datasets.make_blobs(n_samples=<span class="number">1000</span>, n_features=<span class="number">2</span>, centers=<span class="number">4</span>,cluster_std=<span class="number">1</span>)</span><br><span class="line">print(x.shape)</span><br><span class="line">y.shape</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">(1000, 2)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">(1000,)</span><br></pre></td></tr></table></figure><p>(二) 同心圆 sklearn.datasets.make_circles</p><ol><li>noise:在数据中加入高斯噪声的标准差。</li><li>factor:内圆与外圆之间的比例因子,在(0,1)范围内。</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_circles</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">fig=plt.figure(<span class="number">1</span>)</span><br><span class="line">x1,y1=make_circles(n_samples=<span class="number">1000</span>,factor=<span class="number">0.5</span>,noise=<span class="number">0.1</span>)</span><br><span class="line"><span class="comment"># datasets.make_circles()专门用来生成圆圈形状的二维样本.factor表示里圈和外圈的距离之比.每圈共有n_samples/2个点，</span></span><br><span class="line"><span class="comment"># 里圈代表一个类，外圈也代表一个类.noise表示有0.1的点是异常点</span></span><br><span class="line">plt.title(<span class="string">&#x27;make_circles function example&#x27;</span>)</span><br><span class="line">plt.scatter(x1[:,<span class="number">0</span>],x1[:,<span class="number">1</span>],marker=<span class="string">&#x27;o&#x27;</span>,c=y1)</span><br></pre></td></tr></table></figure><p><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1616837055391-5fd31f9b-d90e-49c6-ba0f-622c2b214371.png#align=left&display=inline&height=267&margin=%5Bobject%20Object%5D&name=image.png&originHeight=267&originWidth=384&size=45360&status=done&style=none&width=384" alt="image.png"></p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;相关配置&quot;&gt;&lt;a href=&quot;#相关配置&quot; class=&quot;headerlink&quot; title=&quot;相关配置&quot;&gt;&lt;/a&gt;相关配置&lt;/h1&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; sklearn&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;

&lt;h1 id=&quot;sklearn-datasets-使用&quot;&gt;&lt;a href=&quot;#sklearn-datasets-使用&quot; class=&quot;headerlink&quot; title=&quot;sklearn.datasets 使用&quot;&gt;&lt;/a&gt;sklearn.datasets 使用&lt;/h1&gt;&lt;p&gt;sklearn 的数据集库 datasets 提供很多不同的数据集，主要包含以下几大类：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;玩具数据集&lt;/li&gt;
&lt;li&gt;真实世界中的数据集&lt;/li&gt;
&lt;li&gt;样本生成器&lt;/li&gt;
&lt;li&gt;样本图片&lt;/li&gt;
&lt;li&gt;svmlight 或 libsvm 格式的数据&lt;/li&gt;
&lt;li&gt;从&lt;a href=&quot;http://openml.org下载的数据/&quot;&gt;http://openml.org下载的数据&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;从外部加载的数据&lt;br&gt;用的比较多的就是 1 和 3，这里进行主要介绍，其他的会进行简单介绍，但是不建议使用。&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="笔记" scheme="https://ytno1.github.io/categories/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="Sklearn" scheme="https://ytno1.github.io/categories/%E7%AC%94%E8%AE%B0/Sklearn/"/>
    
    
    <category term="笔记" scheme="https://ytno1.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="Sklearn" scheme="https://ytno1.github.io/tags/Sklearn/"/>
    
  </entry>
  
</feed>
