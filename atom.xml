<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>YT&#39;s blog</title>
  <icon>https://www.gravatar.com/avatar/736ec2aba8de6a35a6db7c1b6e6c0ca4</icon>
  <subtitle>Reading and Coding!</subtitle>
  <link href="https://ytno1.github.io/atom.xml" rel="self"/>
  
  <link href="https://ytno1.github.io/"/>
  <updated>2021-08-31T07:14:52.055Z</updated>
  <id>https://ytno1.github.io/</id>
  
  <author>
    <name>YuT</name>
    <email>2542106000@qq.com</email>
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Pandas介绍</title>
    <link href="https://ytno1.github.io/archives/319b4403.html"/>
    <id>https://ytno1.github.io/archives/319b4403.html</id>
    <published>2021-08-18T07:29:23.000Z</published>
    <updated>2021-08-31T07:14:52.055Z</updated>
    
    <content type="html"><![CDATA[<h1 id="简要介绍"><a href="#简要介绍" class="headerlink" title="简要介绍"></a>简要介绍</h1><p>1、2008 年 WesMcKinney 开发出的库 2、专门用于数据挖掘的开源 python 库 3、以 Numpy 为基础，借力 Numpy 模块在计算方面性能高的优势 4、基于 matplotlib，能够简便的画图 5、独特的数据结构<br>内容整理自 <a href="https://mp.weixin.qq.com/s/5YIz-aXy18289JQH9agNiQ">https://mp.weixin.qq.com/s/5YIz-aXy18289JQH9agNiQ</a></p><span id="more"></span><h1 id="1、Pandas-优势"><a href="#1、Pandas-优势" class="headerlink" title="1、Pandas 优势"></a>1、Pandas 优势</h1><p>（1）增强图表可读性<br>（2）便捷的数据处理能力<br>（3）读取文件方便<br>（4）封装了 Matplotlib、Numpy 的画图和计算</p><h1 id="2、Pandas-数据结构"><a href="#2、Pandas-数据结构" class="headerlink" title="2、Pandas 数据结构"></a>2、Pandas 数据结构</h1><p>Pandas 中一共有三种数据结构，分别为：Series、DataFrame 和 MultiIndex（老版本中叫 Panel ）。<br>​</p><p>其中 Series 是一维数据结构，DataFrame 是二维的表格型数据结构，MultiIndex 是三维的数据结构。<br>​</p><p>三种数据结构对应有自己的创建方式、属性以及相应操作<br>1、通过已有数据创建：（1）指定内容(list、ndarray)，默认索引（2）指定内容和索引（3）通过字典数据创建<br>​</p><p>2、属性一般包括：<br>Series：（1）index（2）values；<br>DataFrame：（1）shape（2）行索引，表明不同行，横向索引，叫 index，0 轴，axis=0（3）列索引，表名不同列，纵向索引，叫 columns，1 轴，axis=1（4）values（5）T 转置（6）head(5) 显示前 5 行内容（7）tail(5) 显示后 5 行内容<br>​</p><p>3、相应操作<br>DataFrame 索引的设置：(1)修改行列索引值（2）重设索引（3）以某列值设置为新的索引<br>​</p><p>MultiIndex：MultiIndex 是三维的数据结构;多级索引（也称层次化索引）是 pandas 的重要功能，可以在 Series、DataFrame 对象上拥有 2 个以及 2 个以上的索引。<br>​</p><h1 id="3、基本数据操作"><a href="#3、基本数据操作" class="headerlink" title="3、基本数据操作"></a>3、基本数据操作</h1><p>1.索引[掌握]</p><ul><li><p>直接索引–先列后行，是需要通过索引的字符串进行获取</p></li><li><p>loc ——先行后列，是需要通过索引的字符串进行获取</p></li><li><p>iloc——先行后列，是通过下标进行索引</p></li><li><p>ix——先行后列，可以用上面两种方法混合进行索引</p><p>2.赋值[知道]</p></li><li><p>data[“”]= **</p></li><li><p>data. =</p><p>3.排序[知道]</p></li><li><p>dataframe</p><ul><li>对象.sort_ _values()</li><li>对象.sort _index()</li></ul></li><li><p>series</p><ul><li>对象 sort_ _values()</li><li>对象.sort_ index()</li></ul></li></ul><h1 id="4、DataFrame-运算"><a href="#4、DataFrame-运算" class="headerlink" title="4、DataFrame 运算"></a>4、DataFrame 运算</h1><p>​</p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;简要介绍&quot;&gt;&lt;a href=&quot;#简要介绍&quot; class=&quot;headerlink&quot; title=&quot;简要介绍&quot;&gt;&lt;/a&gt;简要介绍&lt;/h1&gt;&lt;p&gt;1、2008 年 WesMcKinney 开发出的库 2、专门用于数据挖掘的开源 python 库 3、以 Numpy 为基础，借力 Numpy 模块在计算方面性能高的优势 4、基于 matplotlib，能够简便的画图 5、独特的数据结构&lt;br&gt;内容整理自 &lt;a href=&quot;https://mp.weixin.qq.com/s/5YIz-aXy18289JQH9agNiQ&quot;&gt;https://mp.weixin.qq.com/s/5YIz-aXy18289JQH9agNiQ&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="笔记" scheme="https://ytno1.github.io/categories/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="Pandas" scheme="https://ytno1.github.io/categories/%E7%AC%94%E8%AE%B0/Pandas/"/>
    
    
    <category term="笔记" scheme="https://ytno1.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="Pandas" scheme="https://ytno1.github.io/tags/Pandas/"/>
    
  </entry>
  
  <entry>
    <title>KT常用数据集</title>
    <link href="https://ytno1.github.io/archives/116ef2f7.html"/>
    <id>https://ytno1.github.io/archives/116ef2f7.html</id>
    <published>2021-07-16T05:26:27.000Z</published>
    <updated>2021-08-31T07:14:52.227Z</updated>
    
    <content type="html"><![CDATA[<p>整理 KT 领域常用的公开数据集。</p><span id="more"></span><p>KDD Cup 2010 <a href="https://pslcdatashop.web.cmu.edu/KDDCup/downloads.jsp">https://pslcdatashop.web.cmu.edu/KDDCup/downloads.jsp</a></p><p>ASSISTments <a href="https://sites.google.com/site/assistmentsdata/">https://sites.google.com/site/assistmentsdata/</a></p><p>OLI Engineering Statics 2011 <a href="https://pslcdatashop.web.cmu.edu/DatasetInfo?datasetId=507">https://pslcdatashop.web.cmu.edu/DatasetInfo?datasetId=507</a></p><p>JunyiAcademy Math Practicing Log <a href="https://pslcdatashop.web.cmu.edu/DatasetInfo?datasetId=1198">https://pslcdatashop.web.cmu.edu/DatasetInfo?datasetId=1198</a></p><p>slepemapy.cz <a href="https://www.fi.muni.cz/adaptivelearning/?a=data">https://www.fi.muni.cz/adaptivelearning/?a=data</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;整理 KT 领域常用的公开数据集。&lt;/p&gt;</summary>
    
    
    
    <category term="素材" scheme="https://ytno1.github.io/categories/%E7%B4%A0%E6%9D%90/"/>
    
    <category term="KT数据集" scheme="https://ytno1.github.io/categories/%E7%B4%A0%E6%9D%90/KT%E6%95%B0%E6%8D%AE%E9%9B%86/"/>
    
    
    <category term="素材" scheme="https://ytno1.github.io/tags/%E7%B4%A0%E6%9D%90/"/>
    
    <category term="KT数据集" scheme="https://ytno1.github.io/tags/KT%E6%95%B0%E6%8D%AE%E9%9B%86/"/>
    
  </entry>
  
  <entry>
    <title>科研推荐网站</title>
    <link href="https://ytno1.github.io/archives/722e36b0.html"/>
    <id>https://ytno1.github.io/archives/722e36b0.html</id>
    <published>2021-07-16T05:19:55.000Z</published>
    <updated>2021-08-31T07:14:52.217Z</updated>
    
    <content type="html"><![CDATA[<p>梳理日常学习中常用的一些科研网站。</p><span id="more"></span><p>谷歌学术镜像 <a href="http://ac.scmor.com/">http://ac.scmor.com/</a><br>文献调研神器 <a href="https://www.connectedpapers.com/">https://www.connectedpapers.com</a><br>前沿论文搜索 <a href="http://www.arxiv-sanity.com/?ref=bestofml.com">http://www.arxiv-sanity.com/?ref=bestofml.com</a><br>学术综合平台 <a href="https://www.aminer.org/">https://www.aminer.org/</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;梳理日常学习中常用的一些科研网站。&lt;/p&gt;</summary>
    
    
    
    <category term="素材" scheme="https://ytno1.github.io/categories/%E7%B4%A0%E6%9D%90/"/>
    
    <category term="科研推荐网站" scheme="https://ytno1.github.io/categories/%E7%B4%A0%E6%9D%90/%E7%A7%91%E7%A0%94%E6%8E%A8%E8%8D%90%E7%BD%91%E7%AB%99/"/>
    
    
    <category term="素材" scheme="https://ytno1.github.io/tags/%E7%B4%A0%E6%9D%90/"/>
    
    <category term="科研推荐网站" scheme="https://ytno1.github.io/tags/%E7%A7%91%E7%A0%94%E6%8E%A8%E8%8D%90%E7%BD%91%E7%AB%99/"/>
    
  </entry>
  
  <entry>
    <title>21年小论文开题方向确定</title>
    <link href="https://ytno1.github.io/archives/1a8b97ec.html"/>
    <id>https://ytno1.github.io/archives/1a8b97ec.html</id>
    <published>2021-07-15T12:05:42.000Z</published>
    <updated>2021-08-31T07:14:52.328Z</updated>
    
    <content type="html"><![CDATA[<p>整理最新的 KT 论文，从中找到自己的小论文开题方向，确定模型的改进之处和论文的创新点。</p><span id="more"></span><h1 id="一、试题嵌入"><a href="#一、试题嵌入" class="headerlink" title="一、试题嵌入"></a>一、试题嵌入</h1><p>Bert or HetGNN</p><h1 id="二、学生能力-or-学习速率"><a href="#二、学生能力-or-学习速率" class="headerlink" title="二、学生能力 or 学习速率"></a>二、学生能力 or 学习速率</h1><h2 id="1-1-Convolutional-Knowledge-Tracing：Modeling-Individualization-in-Student-Learning-Process"><a href="#1-1-Convolutional-Knowledge-Tracing：Modeling-Individualization-in-Student-Learning-Process" class="headerlink" title="1.1 Convolutional Knowledge Tracing：Modeling Individualization in Student Learning Process"></a>1.1 Convolutional Knowledge Tracing：Modeling Individualization in Student Learning Process</h2><h3 id="研究背景-or-问题"><a href="#研究背景-or-问题" class="headerlink" title="研究背景 or 问题"></a>研究背景 or 问题</h3><p>随着在线教育系统的发展，越来越多的研究工作聚焦于知识追踪(KT)，它旨在评估学生不断变化的知识状态，帮助他们更有效地学习知识概念。然而，现有的 KT 方法大多只给出学生的学习互动，而忽略了学生的个性化，即学生之间的先验知识和学习率是不同的。</p><p>为了更好地说明学生的个性化，我们在图 1 中给出了一个例子，其中 3 个学生回答了与 3 个不同知识概念相关的 7 个练习。如图 1 所示，学生 s2 在较少的错误后就能快速掌握概念 k1 和 k2，表明 s2 的学习速度比学生 s1 和 s3 快。同时，学生 S3 第一次能够正确回答练习 E7，表明 S3 可能已经掌握了概念 K3。不幸的是，不同学生的个性化先验知识和学习率没有事先给出，这给测量他们带来了很大的挑战。<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1626698730239-907e9248-bb9e-43bf-a256-d3f92053b22c.png#height=238&id=iOk5o&margin=%5Bobject%20Object%5D&name=image.png&originHeight=238&originWidth=508&originalType=binary%E2%88%B6=1&size=42659&status=done&style=none&width=508" alt="image.png"></p><h3 id="研究目的-or-解决方案"><a href="#研究目的-or-解决方案" class="headerlink" title="研究目的 or 解决方案"></a>研究目的 or 解决方案</h3><p>针对学生个性化建模面临的挑战，本文提出了一种新的卷积知识追踪(CKT)方法来进行个性化建模（从学生的学习交互序列中测量学生个性化的先验知识和学习率）。具体地说，对于个性化的先验知识，我们从学生的历史学习互动中进行测量。对于个性化的学习率，我们设计了分层卷积层，通过在一个滑动窗口内同时处理多个连续的学习交互来提取学习速率特征。大量实验表明，CKT 通过对学习过程中的个性化建模，可以获得更好的知识追踪结果。此外，CKT 可以自动学习有意义的练习嵌入。</p><h3 id="本文贡献-or-创新点"><a href="#本文贡献-or-创新点" class="headerlink" title="本文贡献 or 创新点"></a>本文贡献 or 创新点</h3><ul><li>针对学生个性化建模面临的挑战，本文提出了一种新的卷积知识追踪(CKT)方法来进行个性化建模（从学生的学习交互序列中测量学生个性化的先验知识和学习率）。</li><li>在五个公开的数据集上进行了大量的实验来评估 CKT 的性能，结果表明 CKT 可以通过对学生学习过程中的个性化建模来获得更好的知识追踪结果。</li></ul><h3 id="提出模型"><a href="#提出模型" class="headerlink" title="提出模型"></a>提出模型</h3><p><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1626854027424-5d0e66cc-ff5d-4c61-b3ab-9d1aa9110c45.png#height=543&id=u1ed0de1b&margin=%5Bobject%20Object%5D&name=image.png&originHeight=543&originWidth=646&originalType=binary%E2%88%B6=1&size=105449&status=done&style=none&width=646" alt="image.png"><br><strong>Embedding</strong><br>随机初始化<img src="https://cdn.nlark.com/yuque/__latex/cdde75cda05c8ab155ec239826ae8eb6.svg#card=math&code=e%5Et%E2%88%88R%5EK&height=19&id=QBrkI">作为练习<img src="https://cdn.nlark.com/yuque/__latex/58eb15a89dc5cd64584efaaef9d98552.svg#card=math&code=e%5Et&height=18&id=mRbam">的嵌入，因此所有练习可转化为嵌入矩阵<img src="https://cdn.nlark.com/yuque/__latex/edeebf096f88b0e79bfa1f4bf9fb5ed0.svg#card=math&code=E%E2%88%88R%5E%7BN%C3%97K%7D&height=19&id=bCyWN">，该练习嵌入矩阵将在在训练过程中自动学习。因其中 N 为习题数，K 是嵌入维度。考虑到答题正确与否对模型预测的准确性影响，将<img src="https://cdn.nlark.com/yuque/__latex/6189f834e7548fd0ad1bd1ea735fd04a.svg#card=math&code=a%5Et&height=18&id=hur7N">表示为为与 <img src="https://cdn.nlark.com/yuque/__latex/58eb15a89dc5cd64584efaaef9d98552.svg#card=math&code=e%5Et&height=18&id=tee8E">维度相同的零向量，并用公式 1 表示学习交互<img src="https://cdn.nlark.com/yuque/__latex/ea3cf651a16e1695000b060b90ab4917.svg#card=math&code=X_t%E2%88%88R%5E%7B2K%7D%0A&height=21&id=SMtkB">的嵌入。</p><p><img src="https://cdn.nlark.com/yuque/__latex/e402f6be4ecd1de625f7739628f04165.svg#card=math&code=%5Cmathbf%7Bx%7D_%7Bt%7D%3D%20%5Cbegin%7Bcases%7D%7B%5Cleft%5B%5Cmathbf%7Be%7D_%7Bt%7D%20%5Coplus%20%5Cmathbf%7Ba%7D_%7Bt%7D%5Cright%5D%2C%7D%20%26%20%5Ctext%20%7B%20if%20%7D%20%5Cquad%20a_%7Bt%7D%3D1%20%5C%5C%20%7B%5Cleft%5B%5Cmathbf%7Ba%7D_%7Bt%7D%20%5Coplus%20%5Cmathbf%7Be%7D_%7Bt%7D%5Cright%5D%2C%7D%20%26%20%5Ctext%20%7B%20if%20%7D%20%5Cquad%20a_%7Bt%7D%3D0%5Cend%7Bcases%7D%20%5Ctag%7B1%7D&height=45&id=FmWIf"><br><strong>Individualized Prior Knowledge</strong><br>（1）历史相关成绩(HRP)：HRP 更详细地反映了学生对特定概念的知识掌握情况。它侧重于测量与要回答的练习相关的学生历史成绩。为了评估当前回答练习<img src="https://cdn.nlark.com/yuque/__latex/a131aa09a034a4d9d05f7b9161d53a89.svg#card=math&code=e_t&height=14&id=CM4GO">和先前回答的练习<img src="https://cdn.nlark.com/yuque/__latex/fead0ce02381e19bcaf17a2c46ead25b.svg#card=math&code=e_i%28i%E2%88%88%281%EF%BC%8Ct%E2%88%921%29%29&height=24&id=rME2t">之间的相关性，通过使用<img src="https://cdn.nlark.com/yuque/__latex/a131aa09a034a4d9d05f7b9161d53a89.svg#card=math&code=e_t&height=14&id=g9QdD">和<img src="https://cdn.nlark.com/yuque/__latex/8dec559e201a7b6a0f99baeaa1731051.svg#card=math&code=e_i&height=14&id=WHJ03">之间的掩码点积的 Softmax 激活来计算相关系数<img src="https://cdn.nlark.com/yuque/__latex/5fb9b7ef82db1bc94334306b161fbebf.svg#card=math&code=w_t%28i%29&height=20&id=ik35M">：<br><img src="https://cdn.nlark.com/yuque/__latex/258d1e5cf177dcd0047d3441d4518c5b.svg#card=math&code=%5Cbegin%7Bcases%7Dr_%7Bt%7D%28i%29%3D%5Coperatorname%7BMasking%7D%5Cleft%28%5Cmathbf%7Be%7D_%7Bi%7D%20%5Ccdot%20%5Cmathbf%7Be%7D_%7Bt%7D%5Cright%29%2C%20%26%20i%20%5Cin%28t%2C%20N%29%2C%20%5C%5C%20w_%7Bt%7D%28i%29%3D%5Coperatorname%7BSoftmax%7D%5Cleft%28r_%7Bt%7D%28i%29%5Cright%29%2C%20%26%20i%20%5Cin%281%2C%20N%29%2C%5Cend%7Bcases%7D%20%5Ctag%7B2%7D&height=45&id=cnKkO"><br>然后，利用<img src="https://cdn.nlark.com/yuque/__latex/aa38f107289d4d73d516190581397349.svg#card=math&code=w_i&height=14&id=iDMyA">对所有历史学习交互进行加权和从而测量<img src="https://cdn.nlark.com/yuque/__latex/c103cedd1639eca2e6677c80ab933cb9.svg#card=math&code=HRP%E2%88%88R%5E%7BN%C3%972K%7D&height=19&id=IDfXQ">，即：<br><img src="https://cdn.nlark.com/yuque/__latex/aa64d131f9722b515bd77efea512c4ff.svg#card=math&code=%5Coperatorname%7BHRP%7D_%7B%5Cmathbf%7Bt%7D%7D%28%5Cmathbf%7Bt%7D%29%3D%5Csum_%7Bi%3D1%7D%5E%7Bt-1%7D%20w_%7Bt%7D%28i%29%20%5Cmathbf%7Bx%7D_%7Bi%7D%20%5Ctag%7B3%7D&height=52&id=hklkt"><br>概念正确率(CPC)：而 CPC 则粗略地反映了学生对所有知识概念的总体知识掌握情况。CPC 由学生对每个知识概念的正确百分比组成。我们通过计算学生得分率来计算<img src="https://cdn.nlark.com/yuque/__latex/016410abb1da03c80f998a8b7b6e8826.svg#card=math&code=CPC%E2%88%88R%5E%7BN%C3%97M%7D&height=19&id=drQPp">：<br><img src="https://cdn.nlark.com/yuque/__latex/b1788cfefe044bda79ee243ef5071d1a.svg#card=math&code=%5Cmathrm%7BCPC%7D_%7B%5Cmathrm%7Bt%7D%7D%28%5Cmathrm%7Bm%7D%29%3D%5Cfrac%7B%5Csum_%7Bi%3D0%7D%5E%7Bt-1%7D%20a_%7Bi%7D%5E%7Bm%7D%3D%3D1%7D%7B%5Coperatorname%7Bcount%7D%5Cleft%28e%5E%7Bm%7D%5Cright%29%7D%20%5Ctag%7B4%7D&height=52&id=sQ2vD"><br>其中<img src="https://cdn.nlark.com/yuque/__latex/2181b1e2af9805ad7521011707f230fb.svg#card=math&code=m%E2%88%88%281%EF%BC%8CM%29&height=24&id=U7Cm6">表示相关练习的知识概念 m，概念数为 M，<img src="https://cdn.nlark.com/yuque/__latex/6cd7caa441d38ce6169ba50cdcfa79a4.svg#card=math&code=%5Coperatorname%7Bcount%7D%5Cleft%28e%5E%7Bm%7D%5Cright%29&height=20&id=kSMzl">是回答练习的次数，<img src="https://cdn.nlark.com/yuque/__latex/94f12be905d7395e247f386a3a416203.svg#card=math&code=%5Csum_%7Bi%3D0%7D%5E%7Bt-1%7D%20a_%7Bi%7D%5E%7Bm%7D%3D%3D1&height=53&id=SYnBF">是正确回答该练习的次数。<br>然后，将学习交互序列（learning interaction sequence，LIS）与相关习题历史答题情况 HRP,学生对于所有知识点掌握程度 CPC 三个向量拼接到一起组成向量 H,再经过门控线性单元(GLU)得到 Q。如公式（5）所示：<br><img src="https://cdn.nlark.com/yuque/__latex/041c614cdb940f41c70c247e88bc9429.svg#card=math&code=%5Cleft%5C%7B%5Cbegin%7Barray%7D%7Bl%7D%0A%5Cmathrm%7BH%7D%3D%5Cmathrm%7BLIS%7D%20%5Coplus%20%5Cmathrm%7BHRP%7D%20%5Coplus%20%5Cmathrm%7BCPC%7D%20%5C%5C%0A%5Cmathrm%7BQ%7D%3D%5Cleft%28%5Cmathrm%7BH%7D%20%2A%20%5Cmathrm%7B~W%7D_%7B1%7D%2B%5Cmathrm%7Bb%7D_%7B1%7D%5Cright%29%20%5Cotimes%20%5Csigma%5Cleft%28%5Cmathrm%7BH%7D%20%2A%20%5Cmathrm%7B~W%7D_%7B2%7D%2B%5Cmathrm%7Bb%7D_%7B2%7D%5Cright%29%0A%5Cend%7Barray%7D%5Cright.%20%5Ctag%7B5%7D&height=45&id=VIkXe"><br>其中，<img src="https://cdn.nlark.com/yuque/__latex/df380800fb52ddba0c9986bc8c8949c0.svg#card=math&code=%5Cmathbf%7BW%7D_%7B1%7D%20%5Cin%20%5Cmathbb%7BR%7D%5E%7B%284%20K%2BM%29%20%5Ctimes%20K%7D%2C%20%5Cmathbf%7Bb%7D_%7B1%7D%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BK%7D%2C%20%5Cmathbf%7BW%7D_%7B2%7D%20%5Cin%20%5Cmathbb%7BR%7D%5E%7B%284%20K%2BM%29%20%5Ctimes%20K%7D%2C%20%5Cmathbf%7Bb%7D_%7B2%7D%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BK%7D&height=23&id=gKi8B">为模型要学习的参数。<br><strong>Individualized Learning rate</strong><br>CKT 利用分层卷积层在滑动窗口内同时处理多个连续的学习交互（矩阵 Q）来提取学习率特征。<br>分层卷积层的输出矩阵<img src="https://cdn.nlark.com/yuque/__latex/5dbb9b15ac53e333090291e02928704c.svg#card=math&code=Z%E2%88%88R%5E%7BN%C3%97K%7D&height=19&id=FR3Eg">代表学生的知识状态。CKT 利用当前学生知识状态和下一练习嵌入的点积来预测学生的表现：<br><img src="https://cdn.nlark.com/yuque/__latex/50ccb78e14508074da1575f03fcf226f.svg#card=math&code=%5Cleft%5C%7B%5Cbegin%7Barray%7D%7Bl%7D%0Ay_%7Bt%2B1%7D%3Dz_%7Bt%7D%20%5Ccdot%20%5Cmathbf%7Be%7D_%7Bt%2B1%7D%20%5C%5C%0Ap_%7Bt%2B1%7D%3D%5Csigma%5Cleft%28y_%7Bt%2B1%7D%5Cright%29%0A%5Cend%7Barray%7D%5Cright.%20%5Ctag%7B6%7D&height=45&id=H7k1p"><br><strong>Objective Function</strong><br>CKT 选择预测答案与实际答案之间的交叉熵对数损失作为模型的目标函数，该目标函数在小批量上使用 ADAM 优化器最小化 loss 值：<br><img src="https://cdn.nlark.com/yuque/__latex/cad585b6b6a03801fa80ec03ded64bd1.svg#card=math&code=L%3D-%5Csum_%7Bt%3D1%7D%5E%7BN%7D%5Cleft%28a_%7Bt%7D%20%5Clog%20p_%7Bt%7D%2B%5Cleft%281-a_%7Bt%7D%5Cright%29%20%5Clog%20%5Cleft%281-p_%7Bt%7D%5Cright%29%5Cright%29%20%5Ctag%7B7%7D&height=52&id=IUSYR"></p><h3 id="实验结果（datasets、baselines）"><a href="#实验结果（datasets、baselines）" class="headerlink" title="实验结果（datasets、baselines）"></a>实验结果（datasets、baselines）</h3><p><strong>Datasets Description</strong><br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1626852137184-cdd7e3b0-ee6a-42e1-8a0c-309b3ea5d880.png#height=156&id=u12ca9c2a&margin=%5Bobject%20Object%5D&name=image.png&originHeight=156&originWidth=462&originalType=binary%E2%88%B6=1&size=23234&status=done&style=none&width=462" alt="image.png"><br><strong>Comparison methods</strong><br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1626852344951-7b620bd0-321a-46b7-85e2-d2b6724bdec4.png#height=240&id=ua4fd7532&margin=%5Bobject%20Object%5D&name=image.png&originHeight=240&originWidth=516&originalType=binary%E2%88%B6=1&size=47064&status=done&style=none&width=516" alt="image.png"><br><strong>Experimental Results</strong><br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1626852367198-521ee081-c056-48a7-b7b6-036d0cbcc9c3.png#height=219&id=u3918fe10&margin=%5Bobject%20Object%5D&name=image.png&originHeight=219&originWidth=1140&originalType=binary%E2%88%B6=1&size=59113&status=done&style=none&width=1140" alt="image.png"></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本文提出了一种称为卷积知识追踪(CKT)的新模型来模拟学生在 KT 任务中的个性化。具体地说，CKT 从学生的历史学习互动(即 HRP 和 CPC)中测量了个性化的先验知识。然后，设计了分层卷积层来提取连续学习交互中的个性化学习率。大量实验结果表明，CKT 通过对学生学习过程进行个性化建模，可以获得更好的知识追踪效果。</p><h2 id="1-2-LANA：Towards-Personalized-Deep-Knowledge-Tracing-Through-Distinguishable-Interactive-Sequences"><a href="#1-2-LANA：Towards-Personalized-Deep-Knowledge-Tracing-Through-Distinguishable-Interactive-Sequences" class="headerlink" title="1.2 LANA：Towards Personalized Deep Knowledge Tracing Through Distinguishable Interactive Sequences"></a>1.2 LANA：Towards Personalized Deep Knowledge Tracing Through Distinguishable Interactive Sequences</h2><h3 id="研究背景-or-问题-1"><a href="#研究背景-or-问题-1" class="headerlink" title="研究背景 or 问题"></a>研究背景 or 问题</h3><p>在教育应用中，知识追踪(KT)是通过总结学生的知识状态来准确预测学生对未来问题的反应的问题，由于它被认为是适应性在线学习的一项基本任务，因此已被广泛研究了数十年。在所有已提出的 KT 方法中，深度知识追踪(DKT)及其变种由于神经网络的高度灵活性是迄今为止最有效的方法。然而，DKT 往往忽略了学生之间的内在差异(例如记忆技能、推理技能等)，平均了所有学生的表现，导致缺乏个性化，因此被认为不足以进行适应性学习。</p><h3 id="研究目的-or-解决方案-1"><a href="#研究目的-or-解决方案-1" class="headerlink" title="研究目的 or 解决方案"></a>研究目的 or 解决方案</h3><p>为了缓解这一问题，本文提出了分层注意力知识跟踪( Leveled Attentive Knowledge Tracing, LANA)，它首先使用了一种新颖的学生相关特征提取器(Student-Related Features Extractor, SRFE)来从学生各自的交互序列中提取学生独特的固有属性。其次，利用枢轴模块（pivot module），根据提取的特征动态重构神经网络的解码器，成功地区分了不同学生随时间的表现。此外，受项目反应理论(IRT)的启发，可解释的 Rasch 模型被用来根据学生的能力水平对学生进行分类，从而利用分层学习将不同的编码器分配给不同的学生群体。通过 Pivot 模块重构了针对单个学生的解码器和分级学习定制化了针对群体的编码器，实现了个性化的 DKT。在两个真实的大规模数据集上进行的大量实验表明，我们提出的 LANA 将 AUC 值提高了至少 1.00 个百分点(即 EDNET ↑ 1.46%和 RAIEd2020 ↑ 1.00%)，大大超过了其他最先进的 KT 方法。</p><p>更具体地说，可转换性是使用枢轴 Pivot 模块和分层学习来完成的，其中，前者是一个极其依赖 SRFE 的模型组件（Pivot 模块重构了针对单个学生的解码器），而后者是专门针对具有可解释的 Rasch 模型定义能力水平的群体的编码器的训练机制（分级学习定制了针对群体的编码器）。从形式上讲，LANA 可以由以下几个方面代表：<br><img src="https://cdn.nlark.com/yuque/__latex/abe0f2bfc5c73eb71eb2a72b26128d5f.svg#card=math&code=%5Coverbrace%7Br_%7Bt%7D%5E%7Bs_%7Bi%7D%7D%20%5Csim%5Cleft%28f%5Cleft%28p_%7Bt%7D%5E%7Bs_%7Bi%7D%7D%5Cright%29%5Cright%29%5Cleft%28h_%7Bt%7D%5E%7Bs_%7Bi%7D%7D%5Cright%29%7D%5E%7B%5Ctext%20%7BAdaptive%20by%20Pivot%20Module%20%7D%7D%2C%20%5Cunderbrace%7Bp_%7Bt%7D%5E%7Bs_%7Bi%7D%7D%20%5Csim%20k%5Cleft%28h_%7Bt%7D%5E%7Bs_%7Bi%7D%7D%5Cright%29%2C%20%5Cquad%20h_%7Bt%7D%5E%7Bs_%7Bi%7D%7D%20%5Csim%20g%5Cleft%28h_%7B%3Ct%7D%5E%7Bs_%7Bi%7D%7D%2C%20I_%7B%3Ct%7D%5E%7Bs_%7Bi%7D%7D%5Cright%29%7D_%7B%5Ctext%20%7BAdaptive%20by%20Leveled%20Learning%20%7D%7D%2C%20%5Ctag%7B1%7D&height=76&id=K3vlb"><br>其中<img src="https://cdn.nlark.com/yuque/__latex/20f355ff95fa95dec8095d727d38f289.svg#card=math&code=h_%7Bt%7D%5E%7Bs_%7Bi%7D%7D&height=23&id=GNLQr">指的是学生<img src="https://cdn.nlark.com/yuque/__latex/e406ac4d7c470823a8619c13dd7101be.svg#card=math&code=s_i&height=14&id=Ee0af">在时间 t 的知识状态，<img src="https://cdn.nlark.com/yuque/__latex/b30396cac6f9efc19010ee1ee821f40b.svg#card=math&code=I_%7B%3Ct%7D%5E%7Bs_%7Bi%7D%7D%20%5Csim%20%5C%7Be_%7B%3Ct%7D%5E%7Bs_%7Bi%7D%7D%2C%20r_%7B%3Ct%7D%5E%7Bs_%7Bi%7D%7D%5C%7D&height=23&id=B0Tpx">指的是学生<img src="https://cdn.nlark.com/yuque/__latex/e406ac4d7c470823a8619c13dd7101be.svg#card=math&code=s_i&height=14&id=OxMrS">在时间 t 之前的练习互动序列，最后<img src="https://cdn.nlark.com/yuque/__latex/0525993c52d8fe068b2df82db78733f5.svg#card=math&code=f%28%C2%B7%29&height=20&id=GBCkf">(解码器)、<img src="https://cdn.nlark.com/yuque/__latex/933d53a95a8815fe351fef7c663413aa.svg#card=math&code=g%28%C2%B7%29&height=20&id=a4hbx">(编码器)和<img src="https://cdn.nlark.com/yuque/__latex/31cff88f3bcc9f1252b2bdaf1c7aae83.svg#card=math&code=k%28%C2%B7%29&height=20&id=PKxBw">(SRFE)是 LANA 模型要学习的三个主要模块。</p><h3 id="本文贡献-or-创新点-1"><a href="#本文贡献-or-创新点-1" class="headerlink" title="本文贡献 or 创新点"></a>本文贡献 or 创新点</h3><ul><li>LANA 是第一个提出了通过一种新颖的 Student-Related Features Extractor (SRFE)从学生各自的交互序列中来提取学生相关特征，极大地降低了实现个性化 KT 的难度。</li><li>LANA 通过提取出独特的学生特征，利用新颖的 Pivot 模块和分层学习，使整个模型可以针对不同阶段的不同学生进行变换，对 DKT 领域具有较强的适应性。</li><li>在两个真实世界的大型 KT 数据集上进行了广泛的实验，并与其他最先进的 KT 方法进行了比较。结果表明，LANA 的性能明显优于其他 KT 方法。消融研究也被用于研究 LANA 中不同关键成分的影响。源代码和实验的超参数是开源的，以保证可重复性。</li><li>中间特征的可视化显示了 LANA 的额外影响，如学习阶段迁移和学习路径推荐。</li></ul><h3 id="提出模型-1"><a href="#提出模型-1" class="headerlink" title="提出模型"></a>提出模型</h3><p><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1626869251196-77db5427-be5c-4997-842f-b7d59454f39f.png#height=786&id=uc0b49052&margin=%5Bobject%20Object%5D&name=image.png&originHeight=786&originWidth=959&originalType=binary%E2%88%B6=1&size=182444&status=done&style=none&width=959" alt="image.png"><br><strong>Overview</strong><br>LANA 方法由一个 LANA 模型和一个训练机制组成。编码器的目的是从模型的输入嵌入中检索任何有用的信息，然后 SRFE 进一步提取这些信息以获得与学生相关的特征(假设 1)。最后，解码器利用从 SRFE 和编码器收集到的信息进行预测。LANA 模型和 SAINT+一样，是基于 Transformer 的 KT 模型。但与 SAINT+不同的是，LANA 模型主要有 3 点改进:一、LANA 模型考虑了 KT 的特性，因此对基本的变压器模型进行了修改，如直接将位置嵌入馈入到注意模块。二、LANA 模型利用一种新颖的 SRFE 从输入序列中提取必要的与学生相关的特征。三、LANA 模型利用 Pivot 模块，提取与学生相关的特征，针对不同的学生动态构造不同的解码器。利用重构的解码器、检索到的知识状态和其他上下文信息，对未来的练习进行相应的个性化反应预测。</p><p>尽管 Pivot 模块可以帮助 LANA 模型根据学生的固有属性对解码器进行变换、重构，但另一方面，编码器在训练后对所有学生而言是固定的。考虑到在 DKT 领域中，为了降低计算要求，输入序列只是学生整个交互序列的一部分，从输入序列中提取特征的难度增加，因此 LANA 模型区分不同阶段学生的能力需要大大增强。为此，本文提出了一种分层学习机制来解决这一问题，即针对不同的学生群体专门设置不同的编码器和 SRFEs，并采用可解释的 Rasch 模型定义学生的能力水平。分层学习的工作流程如图 3 所示。<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1626870908461-289793a5-e796-4f7c-888c-89035e712b51.png#height=437&id=u143f5fc2&margin=%5Bobject%20Object%5D&name=image.png&originHeight=437&originWidth=872&originalType=binary%E2%88%B6=1&size=79638&status=done&style=none&width=872" alt="image.png"><br><strong>Base Modifications</strong><br>在 LANA 模型中主要对 Transformer 进行了两处基本修改。首先，在 LANA 模型中，位置信息(如位置编码、位置嵌入等)直接输入到带有私有线性投影的注意力模块中，而不是添加到输入嵌入中与输入层的其他特征共享同一个线性投影矩阵；虽然[31]中的实验表明，将输入嵌入与位置信息混合是有效的，但最近[27]中的一些研究争论说，当模型变得更深入时，它倾向于“忘记”输入到第一层的位置信息。另外[13]的研究认为，在输入嵌入中加入位置信息并将其提供给注意力模块，本质上是让它们共享同一个线性投影矩阵，这是不合理的，因为输入嵌入和位置信息的影响是明显不同的。由于完全相同的原因，在 LANA 模型中，多个输入嵌入(即问题 ID 嵌入、学生 ID 嵌入等)采用拼接而不是 Add 相加，导致了第二处基本修改。具体来说，假设总共有 m 个输入嵌入，每个都有一个维度<img src="https://cdn.nlark.com/yuque/__latex/eb3ef62e83afbbf23ba5ab90e6f4c22e.svg#card=math&code=D%5Ef&height=19&id=NIEu9">。然后拼接后，输入嵌入的总维数为<img src="https://cdn.nlark.com/yuque/__latex/ba29a5be3f4e486bb88e09e18ef7acdc.svg#card=math&code=D%5E%7Bmf%7D&height=19&id=KwMKF">。因此，使用<img src="https://cdn.nlark.com/yuque/__latex/8815ee798b77afe5f9b7d67cb52b8a00.svg#card=math&code=D%5E%7Bmf%7D%E2%86%92D%5Ef&height=19&id=ZGjyH">线性投影层将拼接后维度为<img src="https://cdn.nlark.com/yuque/__latex/ba29a5be3f4e486bb88e09e18ef7acdc.svg#card=math&code=D%5E%7Bmf%7D&height=19&id=b04NV">的输入嵌入映射到维度<img src="https://cdn.nlark.com/yuque/__latex/eb3ef62e83afbbf23ba5ab90e6f4c22e.svg#card=math&code=D%5Ef&height=19&id=mjRan">上。<br><strong>Student-Related Features Extractor (SRFE)</strong><br>学生相关特征提取器(SRFE)从交互序列中总结了学生的固有属性，针对 Pivot 模块的假设 1 用来个性化解码器的参数。具体来说，SRFE 包含一个注意层和几个线性层，其中注意层被用来从编码器提供的信息中提取与学生相关的特征，线性层被用来细化和重塑这些特征。值得注意的是，在 LANA 模型中，主要有两个 SRFEs:memory-SRFE 和 performance-SRFE，前者用于推导用于 PMA 模块的学生记忆相关特征(稍后介绍)，后者致力于提取用于 PC-FFN 模块的学生的表现特征(即逻辑思维能力、推理能力、整合能力等)(稍后介绍)。为了更好地说明这一重塑过程，我们绘制了图 4，其中<img src="https://cdn.nlark.com/yuque/__latex/4348d82ff2b652210339d36bd731386e.svg#card=math&code=bs%E3%80%81n_%7Bheads%7D%E3%80%81seq&height=24&id=jn15g">和<img src="https://cdn.nlark.com/yuque/__latex/71564af57d4da36e2c8d96c08da1c0fc.svg#card=math&code=d_%7Bpiv%7D&height=20&id=smMAi">分别表示模型的批处理大小、注意头的数量[31]、输入序列的长度和成绩相关特征的维度。与记忆相关的特征有第二个维度<img src="https://cdn.nlark.com/yuque/__latex/ce9b5c88141fbb7abb24abc73fda517e.svg#card=math&code=n_%7Bheads%7D&height=14&id=KoLUc">的直觉来自于每个注意力头只关注特征的一个角度的理论。因此，每个学生对于不同的注意力头有不同的记忆技能(例如不同的概念)是合理的。<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1626931338581-8a11fce4-70f2-4292-81c1-e656abdc0369.png#height=345&id=udf4d0b69&margin=%5Bobject%20Object%5D&name=image.png&originHeight=345&originWidth=412&originalType=binary%E2%88%B6=1&size=37286&status=done&style=none&width=412" alt="image.png"><br><strong>Pivot Module</strong><br>给定输入 x、学生相关特征 p 并预测目标输出 y，Pivot 模块将学习如何基于 p 将 x 投影到 y，而不是简单地学习将 x 投影到 y，如公式 5 所示。<br><img src="https://cdn.nlark.com/yuque/__latex/f1676927a877e6a54b387d2fe5f9ddf8.svg#card=math&code=y%20%3D%20%28f%28p%29%29%28x%29%20%5Ctag%7B5%7D&height=20&id=YGJph"><br>其中 f(·)是 Pivot 模块学会学习的函数（解码器）。也就是说，x 的投影矩阵是适应于 p 的，而不是固定的。为了实现该动态映射，x 的权重和偏置需要是来自 p 的投影。假设<img src="https://cdn.nlark.com/yuque/__latex/a5c49fa5fb79fdc2010d9240ce04611f.svg#card=math&code=p%E2%88%88R%5E%7BD_p%7D%E3%80%81x%E2%88%88R%5E%7BD_x%7D&height=25&id=mRWCJ">和<img src="https://cdn.nlark.com/yuque/__latex/85addcce04c46228bd2d49c92a4bba8a.svg#card=math&code=y%E2%88%88R%5E%7BD_y%7D&height=21&id=JPX6i">，公式 5 可以在公式 6 中正式表示：<br><img src="https://cdn.nlark.com/yuque/__latex/471eeae2a1ac09301b04a2e1ac67fea0.svg#card=math&code=y%20%3D%20W%5Exx%20%2B%20b%5Ex%20%5Ctag%7B6%7D&height=20&id=UR6Ys"><br>其中<img src="https://cdn.nlark.com/yuque/__latex/b37527493bed89be63a3c25d9503816e.svg#card=math&code=W%5Ex%E2%88%88R%5E%7BD_y%C3%97D_x%7D&height=19&id=jFm6r">和<img src="https://cdn.nlark.com/yuque/__latex/21ee71ca5dab92282aa2f4fc2c95e394.svg#card=math&code=b%5Ex%E2%88%88R%5E%7BD_y%7D&height=19&id=b0pSW">.。因为<img src="https://cdn.nlark.com/yuque/__latex/86f0778f0ed7f6777f19d3a0392744d1.svg#card=math&code=W%5Ex&height=16&id=FHXYq">和<img src="https://cdn.nlark.com/yuque/__latex/f5f4f096eda3472b2c8a419312b27796.svg#card=math&code=b%5Ex&height=16&id=X4Onk">是从 p 推导出的，所以可以在公式 7 中显示详细的转换，图 5 中也描述了该公式，以便更好地说明。<br><img src="https://cdn.nlark.com/yuque/__latex/ac20004e6a0bba61d7f60ee2c1193611.svg#card=math&code=W%5E%7Bx%7D%3DW_%7B1%7D%5E%7Bp%7D%20p%2Bb_%7B1%7D%5E%7Bp%7D%2C%20%5Cquad%20b%5E%7Bx%7D%3DW_%7B2%7D%5E%7Bp%7D%20p%2Bb_%7B2%7D%5E%7Bp%7D%20%5Ctag%7B7%7D&height=23&id=pD6TL"><br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1626931930087-d822d6b7-47c0-44f6-ab4e-e39fd78cca5d.png#height=271&id=u774260e6&margin=%5Bobject%20Object%5D&name=image.png&originHeight=271&originWidth=422&originalType=binary%E2%88%B6=1&size=19245&status=done&style=none&width=422" alt="image.png"><br>通过简化，公式可以定义为公式 8，被命名为<img src="https://cdn.nlark.com/yuque/__latex/3e8bd210d276ff755cc0319dd51be562.svg#card=math&code=P%20ivotLinear%28x%2C%20p%29&height=20&id=DerrP">。<br><img src="https://cdn.nlark.com/yuque/__latex/312d9123313814e91f53b7cea3ce293f.svg#card=math&code=y%3D%28W%20p%29%20x%2Bb%3D%5Coperatorname%7BPivotLinear%7D%28x%2C%20p%29%20%5Ctag%7B8%7D&height=20&id=FBXfz"><br>其中<img src="https://cdn.nlark.com/yuque/__latex/c014ea0554dadd7d954c7d5f42764af8.svg#card=math&code=%20W%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BD_%7By%7D%20%5Ctimes%20D_%7Bx%7D%20%5Ctimes%20D_%7Bp%7D%7D%0A&height=19&id=jC0ST">和<img src="https://cdn.nlark.com/yuque/__latex/c561cba48d320a14ae72baf1374a5164.svg#card=math&code=b%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BD_%7By%7D%7D&height=19&id=IK9ME">。</p><p>在 LANA 模型中，主要有两个模块与 Pivot 模块相关：Pivot Memory Attention (PMA)模块和 Pivot Classification Feed Forward Network (PC-FFN) 模块。在许多方法[7，20]中，Vanilla Memory Attention (VMA)模块被用来考虑学生的“遗忘”行为，这在 KT 的语境中是至关重要的，因为学生很可能做过与他要做的练习相似的练习，如果学生能记住之前类似练习的答案，他正确回答未来相关练习的可能性将大大增加。受艾宾浩斯遗忘曲线[17]和前人工作[20，7]的启发，学生的“遗忘”行为被定义为在时间线上对应交互的权重呈指数衰减。具体而言，在最初注意力模块中，项目 j 在项目 k 上的权重，即<img src="https://cdn.nlark.com/yuque/__latex/2d874e0f62514d18807682a323590692.svg#card=math&code=%CE%B1_%7Bj%EF%BC%8Ck%7D&height=20&id=keCNV">，由项目 j 与项目 k 的相似度的 Sigmoid 结果确定：<br><img src="https://cdn.nlark.com/yuque/__latex/8d5a69431f9ac3f9e6a07375cf72588d.svg#card=math&code=%5Calpha_%7Bj%2C%20k%7D%3D%5Cfrac%7B%5Coperatorname%7Bsim%7D%28j%2C%20k%29%7D%7B%5Csum_%7Bk%5E%7B%5Cprime%7D%7D%20%5Coperatorname%7Bsim%7D%5Cleft%28j%2C%20k%5E%7B%5Cprime%7D%5Cright%29%7D%20%5Ctag%7B9%7D%0A&height=47&id=xIFVb"><br>其中<img src="https://cdn.nlark.com/yuque/__latex/0f903c844c0b7d33f8e596710909059c.svg#card=math&code=%5Coperatorname%7Bsim%7D%28%5Ccdot%29%20&height=20&id=LVrsM">是通过点积来计算项目 i 和项目 j 之间相似度的函数。为了将“遗忘”行为考虑到<img src="https://cdn.nlark.com/yuque/__latex/2d874e0f62514d18807682a323590692.svg#card=math&code=%CE%B1_%7Bj%EF%BC%8Ck%7D&height=20&id=h6ZwV">中(例如，离 j 越远，权重<img src="https://cdn.nlark.com/yuque/__latex/2d874e0f62514d18807682a323590692.svg#card=math&code=%CE%B1_%7Bj%EF%BC%8Ck%7D&height=20&id=I2JEi">将越低)，我们将公式 9 替换为公式 10：<br><img src="https://cdn.nlark.com/yuque/__latex/20611a08ee7336e0f1a2e626beaa2443.svg#card=math&code=%0A%5Calpha_%7Bj%2C%20k%2C%20m%7D%3D%5Cfrac%7Be%5E%7B-%28%5Ctheta%2Bm%29%20%5Ccdot%20d%20i%20s%28j%2C%20k%29%7D%20%5Ccdot%20%5Coperatorname%7Bsim%7D%28j%2C%20k%29%7D%7B%5Csum_%7Bk%5E%7B%5Cprime%7D%7D%20%5Coperatorname%7Bsim%7D%5Cleft%28j%2C%20k%5E%7B%5Cprime%7D%5Cright%29%7D%20%5Ctag%7B10%7D%0A&height=49&id=VFPP6"><br>其中 m 是在 Memory-sRFE 中提取的学生的记忆相关特征，θ 是描述所有学生在 PMA 模块中的平均记忆技能的私有可学习常量，<img src="https://cdn.nlark.com/yuque/__latex/afa87dc0b1b3ed4d5b03af4d3b9b2f52.svg#card=math&code=dis%28%C2%B7%29&height=20&id=ahTBG">计算项目 j 和项目 k 之间的时间距离(例如，项目 j 在项目 k 完成后 dis(j，k)分钟完成)。用两个可学习参数表示记忆技能的原因是为了降低模型收敛的难度，因为与 θ 相比，m 具有更长的反向传播路径。当引入 θ 来拟合所有学生的平均记忆技能时，m 的分布变为高斯分布，这使得模型更容易学习。</p><p>另一方面，PC-FFN 用于根据与成绩相关的特征来进行最终预测，其体系结构如图 6 所示。该模块的想法来自于大量研究，即深度神经网络的早期层通常用作特征提取器，而后几层通常用作决策制定器，以确定哪些特征对模型的输出有用。因此，这些研究指出，许多模型实际上具有相似的早期层，正是后者使这些模型在使用上与众不同。因此，该模型中的 PC-FFN 被用作个性化的决策器，根据学生独特的内在属性自适应地做出最终预测：<br><img src="https://cdn.nlark.com/yuque/__latex/ec82fc0de07f6868fc4f6d3461d62af4.svg#card=math&code=P%20C%E2%88%92F%20F%20N%28x%2C%20p%29%20%3D%20x%2BP%20ivotLinear%28P%20ivotLinear%28x%2C%20p%29%2C%20p%29%20%5Ctag%7B11%7D&height=20&id=CTswW"><br>其中 p 是在 Performance-SRFE 中提取的与学生成绩相关的特征。<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1626934223975-1084669b-352b-48ec-9809-526b5fd5d9c5.png#height=383&id=ud970a8ff&margin=%5Bobject%20Object%5D&name=image.png&originHeight=383&originWidth=414&originalType=binary%E2%88%B6=1&size=24162&status=done&style=none&width=414" alt="image.png"><br><strong>Leveled Learning</strong><br>虽然 Pivot 模块使解码器可以针对不同的学生进行转换，但是为 Pivot 模块提供必要信息的 LANA 模型的编码器和 SRFE 对于所有学生来说都是相同的。如果输入序列的长度足够大，这不是问题，因为假设 1 确保长序列总是可区分的，除非它们都属于同一时间段的同一学生。然而，DKT，尤其是基于变压器的 DKT，由于内存大小有限且计算复杂度高，一次只能输入最新的 n 次(通常 n=100)次交互。因此，编码器和 SRFE 可能会为两个不同的学生输出类似的结果，从而导致解码器无法适应。为了缓解这个问题，自然会想到给不同的学生分配不同的编码器和 SRFE，这些编码器和 SRFE 对所分配的学生的模式高度专门化、定制化(敏感)。然而，在实践中，考虑到有限的训练时间和有限的训练数据，为每个学生训练一个唯一的编码器是不可行的。因此，一种新的分层学习方法被提出来解决这个问题，该方法最初的灵感来自于迁移学习中的微调机制[28]，在该机制中，我们将每个学生视为一个独特的任务，我们希望将一个适合所有学生的模型有效地转移到一个学生身上。</p><p>分层学习的观点是，模型的早期层对于类似的任务是相似的。因此，为了节省训练时间和扩大训练集，我们考虑将能力水平相似的学生分组在一起，共享他们的私人训练数据，拥有相同的编码器和 SRFE，而不是用每个学生的私人训练数据来训练每个学生一个独有的编码器和 SRFE。因此，LANA 首先利用可解释的 Rasch 模型分析每个学生<img src="https://cdn.nlark.com/yuque/__latex/e406ac4d7c470823a8619c13dd7101be.svg#card=math&code=s_i&height=14&id=fZrTV">的能力水平<img src="https://cdn.nlark.com/yuque/__latex/37c6f640e3443b3fa5b645cc43076f48.svg#card=math&code=a%5E%7Bs_i%7D&height=16&id=Olr48">，然后将学生分组到不同的独立层<img src="https://cdn.nlark.com/yuque/__latex/b39335b6584e8455ab4de3c86b439e21.svg#card=math&code=l_i&height=18&id=TaFQ9">。假设所有学生的能力分布和<img src="https://cdn.nlark.com/yuque/__latex/b39335b6584e8455ab4de3c86b439e21.svg#card=math&code=l_i&height=18&id=M5RDx">能力水平学生的能力分布分别为高斯分布<img src="https://cdn.nlark.com/yuque/__latex/5fdc8bf3e1b21c70753cc9cca5d6e039.svg#card=math&code=N%28%C2%B5_a%EF%BC%8C%20%CF%83%5E2_a%29&height=25&id=R6Km0">和<img src="https://cdn.nlark.com/yuque/__latex/07ef04a95e8a0bc3598fc5db947e4177.svg#card=math&code=N%28%C2%B5_i%EF%BC%8C%20%CF%83%5E2_i%29&height=25&id=Zcq6T">，则有公式 12：<br><img src="https://cdn.nlark.com/yuque/__latex/db94420c13c439aa1027f19cd95e685a.svg#card=math&code=%5Cmu_%7Ba%7D%3D%5Cfrac%7B%5Csum_%7Bi%7D%20%5Cmu_%7Bi%7D%7D%7BL%7D%2C%20%5Cquad%20%5Csigma_%7Ba%7D%5E%7B2%7D%3D%5Csum_%7Bi%7D%20%5Csigma_%7Bi%7D%5E%7B2%7D%20%5Ctag%7B12%7D&height=49&id=MGd9S"><br>在 LANA 中，为简便起见，我们考虑所有层的方差<img src="https://cdn.nlark.com/yuque/__latex/10e16c6a764d367ca5077a54bf156f7e.svg#card=math&code=%5Csigma%5E2%20&height=19&id=d0GOt">相同，连续层之间的平均<img src="https://cdn.nlark.com/yuque/__latex/b061e8ca781368c8cf255e860a9d6ba2.svg#card=math&code=%C2%B5_i&height=25&id=Buen5">差为常数<img src="https://cdn.nlark.com/yuque/__latex/a788b57bb5319497a4de11f3392dcbc3.svg#card=math&code=%CF%84&height=12&id=QTxWz">。因此，<img src="https://cdn.nlark.com/yuque/__latex/b061e8ca781368c8cf255e860a9d6ba2.svg#card=math&code=%C2%B5_i&height=25&id=OUoKD">和<img src="https://cdn.nlark.com/yuque/__latex/df6d0c469ed9e22612cfd4b5a1fa7357.svg#card=math&code=%5Csigma%5E2_i&height=23&id=gFl5n">由:<br><img src="https://cdn.nlark.com/yuque/__latex/26b8b33563b5946a9d62cda65d3429b1.svg#card=math&code=%5Cmu_%7Bi%7D%3D%5Cmu_%7Ba%7D-%5Cfrac%7BL-1%7D%7B2%7D%20%5Ctimes%20%5Ctau%2Bi%20%5Ctimes%20%5Ctau%2C%20%5Cquad%20%5Csigma_%7Bi%7D%5E%7B2%7D%3D%5Cfrac%7B%5Csigma_%7Ba%7D%5E%7B2%7D%7D%7BL%7D%20%5Ctag%7B13%7D&height=40&id=fTi8x"><br>其中<img src="https://cdn.nlark.com/yuque/__latex/c8669cec6a92c1be66d22a2ad7d37dce.svg#card=math&code=L%20%3D%20%7C%7Cl_i%7C%7C&height=20&id=w9U1W">为层数。将每一层<img src="https://cdn.nlark.com/yuque/__latex/b39335b6584e8455ab4de3c86b439e21.svg#card=math&code=l_i&height=18&id=CR1KX">的<img src="https://cdn.nlark.com/yuque/__latex/b061e8ca781368c8cf255e860a9d6ba2.svg#card=math&code=%C2%B5_i&height=25&id=rbMcj">和<img src="https://cdn.nlark.com/yuque/__latex/df6d0c469ed9e22612cfd4b5a1fa7357.svg#card=math&code=%5Csigma%5E2_i&height=23&id=WkwKx">都提取出来，给定一个学生的能力常数<img src="https://cdn.nlark.com/yuque/__latex/37c6f640e3443b3fa5b645cc43076f48.svg#card=math&code=a%5E%7Bs_i%7D&height=16&id=GVLyE">，我们现在可以通过公式 14 计算<img src="https://cdn.nlark.com/yuque/__latex/e406ac4d7c470823a8619c13dd7101be.svg#card=math&code=s_i&height=14&id=D4Lcc">被分组到不同层的概率:<br><img src="https://cdn.nlark.com/yuque/__latex/30c52fce2025e99f528b61cd0ec33cf9.svg#card=math&code=p_%7Bi%7D%5E%7Bs_%7Bi%7D%7D%3D%5Cfrac%7B%5Cphi_%7Bi%7D%5Cleft%28a%5E%7Bs_%7Bi%7D%7D%5Cright%29%7D%7B%5Csum_%7Bi%5E%7B%5Cprime%7D%7D%20%5Cphi_%7Bi%5E%7B%5Cprime%7D%7D%5Cleft%28a%5E%7Bs_%7Bi%7D%5E%7B%5Cprime%7D%7D%5Cright%29%7D%2C%20%5Cquad%20%5Cphi_%7Bi%7D%5Cleft%28a%5E%7Bs_%7Bi%7D%7D%5Cright%29%3D%5Cfrac%7B1%7D%7B%5Csigma_%7Bi%7D%20%5Csqrt%7B2%20%5Cpi%7D%7D%20e%5E%7B-%5Cfrac%7B%5Cleft%28a%5E%7Bs_%7Bi%7D%7D-%5Cmu_%7Bi%7D%5Cright%29%5E%7B2%7D%7D%7B2%20%5Csigma_%7Bi%7D%5E%7B2%7D%7D%7D%20%5Ctag%7B14%7D&height=64&id=xTgqu"><br>其中<img src="https://cdn.nlark.com/yuque/__latex/9f6a281f37c0be60bd1d5c740d273c20.svg#card=math&code=p_%7Bi%7D%5E%7Bs_%7Bi%7D%7D&height=23&id=sfyXD">是指学生<img src="https://cdn.nlark.com/yuque/__latex/e406ac4d7c470823a8619c13dd7101be.svg#card=math&code=s_i&height=14&id=csuFI">被分组到<img src="https://cdn.nlark.com/yuque/__latex/b39335b6584e8455ab4de3c86b439e21.svg#card=math&code=l_i&height=18&id=I9Th8">层的概率。由式 14 可以看出，能力水平高的学生并不一定被分为能力期望水平高的层次。相反，这些能力高的学生只是比能力低的学生更有可能被归为能力高的层次，这符合现实的规律(例如，能力高的学生也可能来自师范学校)。</p><p>然后，将在所有学生数据上预先训练好的 LANA 模型复制 L 次，将每个克隆的模型<img src="https://cdn.nlark.com/yuque/__latex/342e772474b691ac87dac30aeef596c0.svg#card=math&code=m_i&height=14&id=Xj1hO">分配给一个层<img src="https://cdn.nlark.com/yuque/__latex/b39335b6584e8455ab4de3c86b439e21.svg#card=math&code=l_i&height=18&id=xlHuK">，通过加权反向传播，利用<img src="https://cdn.nlark.com/yuque/__latex/b39335b6584e8455ab4de3c86b439e21.svg#card=math&code=l_i&height=18&id=zMD5s">的私有训练数据专门、定制化地进行微调:<br><img src="https://cdn.nlark.com/yuque/__latex/c8d3ff7051f1e599b0c3e50460ad095f.svg#card=math&code=%5Ctext%20%7B%20loss%20%7D_%7Bi%7D%3Dp_%7Bi%7D%20%2A%20%5Ctext%20%7B%20loss%20%7D%5Cleft%28%5Ctext%20%7B%20predict%20%7D_%7Bi%7D%20%5Ctext%20%7B%20%2C%20target%20%7D%5Cright%29%2C%20%5Ctag%7B15%7D&height=20&id=liRZ2"><br>其中<img src="https://cdn.nlark.com/yuque/__latex/11eb18f95a54ae5966a08b0ec60e404f.svg#card=math&code=predict_i&height=18&id=m5nK6">为模型<img src="https://cdn.nlark.com/yuque/__latex/342e772474b691ac87dac30aeef596c0.svg#card=math&code=m_i&height=14&id=NcgsD">的预测。</p><p>虽然分层学习的训练阶段似乎很有希望，但它的推断阶段却存在问题。第一个问题是如何使用多个专门化模型进行预测。在 LANA 中，采用 top - k 模型融合的方法进行预测。具体来说，当需要预测学生 si 的未来反应时，LANA 首先计算 pi，然后将 si 的交互序列输入所有满足<img src="https://cdn.nlark.com/yuque/__latex/2bc1ea6bce1de0a2e3379826009e9b20.svg#card=math&code=p_i%E2%88%88top%E2%88%92k%28p%29&height=20&id=RJMdE">的模型 mi，其中 k 需要手动设置来控制预测时间。然后，这些模型的输出将乘以<img src="https://cdn.nlark.com/yuque/__latex/1fecc21bfec2c9b80680e3b2572b4eac.svg#card=math&code=sigmoid%28p_i%29&height=20&id=wohfj">，形成最终的预测。分层学习推理步骤的工作流程如公式 16 所示：<br><img src="https://cdn.nlark.com/yuque/__latex/56667db09fcd822ea8a74db716c510bb.svg#card=math&code=r_%7Bi%7D%3D%5Csum_%7Bi%5E%7B%5Cprime%7D%7D%5Cleft%28m_%7Bi%7D%28x%29%20%5Ctimes%20%5Csum_%7Bh%20%5Cin%20i%5E%7B%5Cprime%7D%7D%20%5Cfrac%7Bp_%7Bh%7D%7D%7B%5Csum_%7Bh%5E%7B%5Cprime%7D%7D%20p_%7Bh%5E%7B%5Cprime%7D%7D%7D%5Cright%29%2C%20%5Cquad%20i%5E%7B%5Cprime%7D%20%5Cin%5Cleft%5C%7Bi%20%5Cmid%20p_%7Bi%7D%20%5Cin%20t%20o%20p-k%28p%29%5Cright%5C%7D%20%5Ctag%7B16%7D&height=54&id=DkWTp"><br>其中 ri 为分层学习的最终预测，x 为模型的输入。这个工作流似乎类似于将多个模型合并以生成最终答案的集成。尽管如此，LANA 中模型的权重是来自可解释的 Rasch 模型的概率，因此很清楚哪个模型对 x 是主导的。此外，不像在集成中每个模型的作用是模棱两可的，在 LANA 中，每个模型都有其可解释的效果(例如，<img src="https://cdn.nlark.com/yuque/__latex/4db38079930b62a5a549d9d569258dcd.svg#card=math&code=l_L&height=18&id=ZCWcb">致力于高能力学生，因此<img src="https://cdn.nlark.com/yuque/__latex/59b475e165fafc6726d0335b32619573.svg#card=math&code=p_L&height=14&id=KoxRt">大的学生表明他一定与<img src="https://cdn.nlark.com/yuque/__latex/4db38079930b62a5a549d9d569258dcd.svg#card=math&code=l_L&height=18&id=itxF0">中的高能力学生相似)，这表明分层学习在可解释性方面明显优于集成。具体比较见表 1。<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1626938549114-ef3bb22f-fb02-44bc-944a-12e787a48aed.png#height=179&id=uf7b7e3b6&margin=%5Bobject%20Object%5D&name=image.png&originHeight=179&originWidth=416&originalType=binary%E2%88%B6=1&size=19128&status=done&style=none&width=416" alt="image.png"><br>另一方面，分层学习的第二个问题是如何计算 LANA 在训练中从未遇到的学生的<img src="https://cdn.nlark.com/yuque/__latex/eca91c83a74a2373ca5f796700e99fd3.svg#card=math&code=p_i&height=14&id=Ovnms">值，即“冷启动”问题[34]。在 vanilla KT 环境中，我们只能将新来的学生的能力水平提升到所有学生的平均能力水平。然而，在实践中，我们可以通过让他们做几个样本练习或使用学校排名来更准确地估计他们的能力水平。</p><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p><strong>Datasets</strong><br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1626938763025-97c45ade-6b19-47cf-879c-c8272477b94a.png#height=177&id=dNQaI&margin=%5Bobject%20Object%5D&name=image.png&originHeight=177&originWidth=432&originalType=binary%E2%88%B6=1&size=19680&status=done&style=none&width=432" alt="image.png"><br><strong>Baselines</strong><br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1626938917051-df1c78da-138d-4a89-b205-399bb2f2ed6c.png#height=46&id=u48ec9805&margin=%5Bobject%20Object%5D&name=image.png&originHeight=46&originWidth=390&originalType=binary%E2%88%B6=1&size=7919&status=done&style=none&width=390" alt="image.png"><br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1626938972908-d3de3338-7c5c-4b93-aeac-30f85613aacc.png#height=428&id=u78fa989e&margin=%5Bobject%20Object%5D&name=image.png&originHeight=428&originWidth=400&originalType=binary%E2%88%B6=1&size=69956&status=done&style=none&width=400" alt="image.png"><br><strong>Experimental Results</strong><br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1626939361471-7f2bf725-0c15-4347-91df-42dc9d935fd6.png#height=342&id=u430074ac&margin=%5Bobject%20Object%5D&name=image.png&originHeight=342&originWidth=356&originalType=binary%E2%88%B6=1&size=38340&status=done&style=none&width=356" alt="image.png"></p><h3 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h3><p>个人理解：</p><ul><li><strong>Workflow：</strong>LANA 方法由一个 LANA 模型（Transformer+SRFE+Pivot（PMA+PC-FFN））和一个训练机制（分层学习，定制化了针对群体的编码器从而实现了微调在整个数据集上预训练好的 LANA 模型）组成。编码器的目的是从模型的输入嵌入中检索任何有用的信息，然后 SRFE 进一步提取这些信息以获得与学生相关的特征（memory-SRFE 和 performance-SRFE）(假设 1)。最后，通过 Pivot 模块重构了针对单个学生的解码器（使用了从分层学习后的 SRFE 和编码器中收集到的信息）以及检索到的知识状态和其他上下文信息，对未来的练习进行相应的个性化反应预测，实现了个性化的 DKT。</li><li><strong>Base Modifications of Transformer：</strong>LANA 模型是一个基于 Transformer 的 DKT 模型，在 Transformer 架构中做了两处修改：1、位置信息(如位置编码、位置嵌入等)直接输入到带有私有线性投影的注意力模块中，而不是添加到输入嵌入中与输入层的其他特征共享同一个线性投影矩阵；2、在 LANA 模型中，多个输入嵌入(即问题 ID 嵌入、学生 ID 嵌入等)采用拼接而不是 Add 相加。</li><li><strong>SRFE：</strong>提出了一种新颖的学生相关特征提取器(Student-Related Features Extractor, SRFE)，由 memory-SRFE 和 performance-SRFE 两组件组成，来从学生各自的交互序列中提取学生独特的固有属性。memory-SRFE 用于推导用于 PMA 模块的学生记忆相关特征，performance-SRFE 致力于提取用于 PC-FFN 模块的学生的表现特征(即逻辑思维能力、推理能力、整合能力等。</li><li><strong>Pivot Module：</strong>利用枢轴模块（Pivot Module，PMA+PC-FFN），根据提取的特征动态重构神经网络的解码器（针对个体学生的个性化解码器），成功地区分了不同学生随时间的表现。</li><li><strong>Leveled Learning（训练机制）：</strong>尽管 Pivot 模块可以帮助 LANA 模型根据学生的固有属性对解码器进行变换、重构，但另一方面，1、编码器在训练后对所有学生而言是固定的。2、而为了降低计算要求，输入序列只是学生整个交互序列的一部分，从输入序列中提取特征的难度增加，因此 LANA 模型区分不同阶段学生的能力需要大大增强。为此，本文提出了一种分层学习机制来解决这一问题，即针对不同的学生群体专门设置不同的编码器和 SRFEs。此外，受项目反应理论(IRT)的启发，可解释的 Rasch 模型被用来根据学生的能力水平对学生进行分类，从而利用分层学习将不同的编码器分配给不同的学生群体。</li></ul><p>本文提出了一种新的分层注意力知识追踪(LANA)方法，致力于将适应性带回 DKT。具体地说，受 BKT 和 IRT 的启发，LANA 旨在通过为不同阶段的不同学生提供不同的模型参数来实现适应性。然而，为每个学生单独训练大量独特的模型显然不是一个实用的解决方案，因此，LANA 不是直接学习不同学生的模型参数，而是通过一种新颖的 SRFE 从他们各自的交互序列中提取学生的固有属性，并学习到使用这些提取的学生相关特征重新参数化模型的函数。因此，提出了一种创新的 Pivot 模块来产生自适应解码器。此外，为了减少输入序列的模糊性，捕捉学生个体的长期特征，引入了一种新的分层学习训练机制，通过可解释的 Rasch 模型定义能力水平对学生进行聚类，不仅使编码器专门化，从而增强了学生潜在特征的重要性，而且节省了大量的训练时间。在教育领域最大的两个公共基准数据集上进行的广泛实验有力地评估了提议的 LANA 的可行性和有效性。特征可视化还暗示了 LANA 的额外影响，无论是学习阶段转移还是学习路径推荐。</p><p>然而，LANA 也有一些缺点：首先，分层学习中的方差配置需要大量的人力，这需要有一个自动的工作流程来设置这些参数。其次，虽然图 7 特征可视化说明了提议的 SRFE 模块的有效性，但还需要想出一种更系统的方法来量化学生的特征。因此，解决这些问题将是我们下一步的工作。</p><h2 id="1-3-Dynamic-Key-Value-Memory-Networks-With-Rich-Features-for-Knowledge-Tracing"><a href="#1-3-Dynamic-Key-Value-Memory-Networks-With-Rich-Features-for-Knowledge-Tracing" class="headerlink" title="1.3 Dynamic Key-Value Memory Networks With Rich Features for Knowledge Tracing"></a>1.3 Dynamic Key-Value Memory Networks With Rich Features for Knowledge Tracing</h2><h1 id="三、遗忘因素"><a href="#三、遗忘因素" class="headerlink" title="三、遗忘因素"></a>三、遗忘因素</h1><h2 id="2-1-Augmenting-Knowledge-Tracing-by-Considering-Forgetting"><a href="#2-1-Augmenting-Knowledge-Tracing-by-Considering-Forgetting" class="headerlink" title="2.1 Augmenting Knowledge Tracing by Considering Forgetting"></a>2.1 Augmenting Knowledge Tracing by Considering Forgetting</h2><h2 id="2-2-AKT：Context-Aware-Attentive-Knowledge-Tracing"><a href="#2-2-AKT：Context-Aware-Attentive-Knowledge-Tracing" class="headerlink" title="2.2 AKT：Context-Aware Attentive Knowledge Tracing"></a>2.2 AKT：Context-Aware Attentive Knowledge Tracing</h2><h2 id="2-3-RKT：Relation-Aware-Self-Attention-for-Knowledge-Tracing"><a href="#2-3-RKT：Relation-Aware-Self-Attention-for-Knowledge-Tracing" class="headerlink" title="2.3 RKT：Relation-Aware Self-Attention for Knowledge Tracing"></a>2.3 RKT：Relation-Aware Self-Attention for Knowledge Tracing</h2><h2 id="2-4-Learning-or-Forgetting-A-Dynamic-Approach-for-Trackingthe-Knowledge-Proficiency-of-Students"><a href="#2-4-Learning-or-Forgetting-A-Dynamic-Approach-for-Trackingthe-Knowledge-Proficiency-of-Students" class="headerlink" title="2.4 Learning or Forgetting? A Dynamic Approach for Trackingthe Knowledge Proficiency of Students"></a>2.4 Learning or Forgetting? A Dynamic Approach for Trackingthe Knowledge Proficiency of Students</h2><h2 id="2-5-KPT：Tracking-Knowledge-Proficiency-of-Students-with-Educational-Priors"><a href="#2-5-KPT：Tracking-Knowledge-Proficiency-of-Students-with-Educational-Priors" class="headerlink" title="2.5 KPT：Tracking Knowledge Proficiency of Students with Educational Priors"></a>2.5 KPT：Tracking Knowledge Proficiency of Students with Educational Priors</h2><h3 id="研究背景-or-问题-2"><a href="#研究背景-or-问题-2" class="headerlink" title="研究背景 or 问题"></a>研究背景 or 问题</h3><p>诊断学生的知识水平，即练习中某一特定知识点的掌握程度，对于许多教育应用，如有针对性的知识培训和练习推荐，都是一个至关重要的问题。教育理论一致认为，学生一次次地学习知识，但又会随时间忘记知识。因此，有必要跟踪他们随着时间的推移对知识的掌握情况。然而，传统的诊断方法要么忽视了诊断结果对知识点的解释力，要么依赖于静态假设。<br>​</p><p>这类系统中的一个关键问题是学生的知识熟练程度诊断(KPD)，即发现学生在每个知识点上的潜在掌握程度[32]。图 1 显示了此 KPD 任务的示例。从图中可以看出，2016 年 3 月至 5 月，有两名学生(u1 和 u2)在做不同的数学练习。每个练习包含不同的知识点，可以用教育专家提供的 Q 矩阵来表示[8]。具体地说，Q 矩阵中的数字 1 表示对应的练习包含知识点，否则表示 0。如图所示，练习 e1 包含知识点函数，练习 e9 涉及知识点函数和不等式。教育领域的 KPD 任务问：给定学生的历史练习记录和提供的 Q 矩阵，如何诊断学生对知识点(即图 1 中的函数和不等式)的掌握程度？事实上，由于这些诊断结果对许多应用是有益的，例如有针对性的知识培训[12]和个性化的练习推荐[26]，因此已经为这项 KPD 任务付出了很多努力。一方面，教育心理学领域的认知诊断模型通常用潜在的特质值[11]或二元技能掌握向量[8]来表征学生的知识水平。另一方面，通过将 KPD 任务视为数据挖掘问题(即，学生成绩预测)，矩阵分解技术将每个学生投影到描述学生的隐性知识状态的潜在空间[16]。综上所述，这两个研究方向通常模拟用户的历史记录，没有任何时间信息，因此它们善于从静态的角度预测学生的水平。<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1628151318137-62ad5083-9f96-402e-a86c-4010993a706c.png#clientId=ub1ed9b4c-f6a6-4&from=paste&height=308&id=u097fe9b0&margin=%5Bobject%20Object%5D&name=image.png&originHeight=308&originWidth=752&originalType=binary%E2%88%B6=1&size=90377&status=done&style=none&taskId=u315035d8-88e9-44c2-80b0-5db01b182d5&width=752" alt="image.png"></p><h3 id="研究目的-or-解决方案-2"><a href="#研究目的-or-解决方案-2" class="headerlink" title="研究目的 or 解决方案"></a>研究目的 or 解决方案</h3><p>为此，本文提出了一种解释性概率知识熟练度追踪(KPT)模型，通过利用教育先验来跟踪学生随着时间的推移的知识熟练程度。具体地说，我们首先通过利用教育先验(即 Q 矩阵)将每个练习与一个知识向量相关联，其中每个元素表示一个明确的知识点。相应地，每个学生在同一知识空间中的每个时刻都被表示为知识向量。其次，在给定学生随时间变化的知识向量的情况下，我们借用了两个经典的教育理论(即学习曲线和遗忘曲线)作为先验，以捕捉每个学生的熟练程度随时间的变化。然后，我们设计了一个结合学生先验和习题先验的概率矩阵因式分解框架来跟踪学生的知识水平。在三个真实数据集上的大量实验证明了我们提出的模型的有效性和解释力。</p><h3 id="本文贡献-or-创新点-2"><a href="#本文贡献-or-创新点-2" class="headerlink" title="本文贡献 or 创新点"></a>本文贡献 or 创新点</h3><p>1、无论是因子分解模型的潜在向量，还是神经网络的隐含层，都不能对应于任何显式的知识点。相反，我们的模型通过利用教育先验(即 Q-矩阵、学习曲线和遗忘曲线)改进了传统的矩阵分解，保证了模型的解释力。据我们所知，这是首次将三种教育先验(Q-矩阵、学习曲线和遗忘曲线)纳入概率矩阵因式分解框架，以同时具有精确和解释能力来跟踪 KPD 任务。<br>​</p><p>2、目前广泛使用的 KPD 方法可以分为两个方面：一维模型（IRT）和多维模型（Deterministic Inputs, Noisy-And gate model, FuzzyCDM）。然而，据我们所知，对于 KPD 任务，这些方法都依赖于静态假设，而忽略了时间因素。在这项工作中，我们关注学生的动态学习过程，捕捉每个学生的知识水平随时间的变化。<br>​</p><p>3、基于大多数学生每个练习只做一次的学习情景，本文旨在利用基本理论(即 Q 矩阵、学习曲线和遗忘曲线)跟踪和解释学生在多个知识点上的知识熟练程度。</p><h3 id="提出模型-2"><a href="#提出模型-2" class="headerlink" title="提出模型"></a>提出模型</h3><p><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1628154899925-7259fddb-968c-444c-930c-fd78e9c3fc16.png#clientId=ub1ed9b4c-f6a6-4&from=paste&height=158&id=ubbce6926&margin=%5Bobject%20Object%5D&name=image.png&originHeight=158&originWidth=735&originalType=binary%E2%88%B6=1&size=29131&status=done&style=none&taskId=ud9a9be03-cc58-4ace-a7c3-2748d78faa2&width=735" alt="image.png"><br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1628154808351-cba42f5d-4b2d-4c30-8ee1-091a011d6241.png#clientId=ub1ed9b4c-f6a6-4&from=paste&height=331&id=u4a1298ed&margin=%5Bobject%20Object%5D&name=image.png&originHeight=331&originWidth=379&originalType=binary%E2%88%B6=1&size=49993&status=done&style=none&taskId=u93df88ad-c929-41d9-ae02-fd5c2aa7070&width=379" alt="image.png"><br>如图 2 所示，我们的解决方案是一个两阶段框架，包括建模阶段和预测阶段：1)在建模阶段，给定学生的练习响应日志(表 1)和专家标注的 Q 矩阵，首先利用教育专家提供的 Q 矩阵将每个学生的潜在向量投影到知识空间。然后，我们提出 KPT 通过整合学习曲线和遗忘曲线理论来解决学生随时间的 KPD 问题。之后，我们可以得到学生在不同时间的知识熟练度 U 和每个练习的知识向量 V。2)在预测阶段，KPT 预测学生未来的反应和知识水平。<br><strong>Probabilistic Modeling with Priors</strong><br>对于每个学生和每个练习，KPT 将响应张量 R 建模为：</p><p>其中是均值为 µ，方差为 σ2 的高斯分布。I 是指示张量，如果学生 i 在时间窗口 t 内做练习 j，则等于 1，反之为 0。是学生 i 在时间窗 t 内的知识水平，表示习题与知识点之间的关系。是练习 j 的难度偏差，在 KPT 任务建模中被广泛采用[11]。<br>​</p><p>给出这个似然函数，在下面，我们将详细说明如何在建模过程中结合教育先验。我们首先解释如何将 Q 矩阵的知识嵌入到 V 模型中。具体地说，我们在将每个练习与一个知识向量相关联之前加入 Q 矩阵，其中每个元素代表一个明确的知识点。然后结合两种教育理论(遗忘和学习)建立 U 模型，作为先验来跟踪学生的动态学习过程。<br>​</p><p>**Modeling V with the Q-matrix prior.**传统的概率矩阵分解模型存在解释问题，因为学习到的潜在维数是无法解释的。相比之下，教育界利用基于 Q-矩阵的先验知识来构建解释模型已经做了很多努力。然而，这种传统的 Q-矩阵有两个缺点：1)人工标注不可避免的误差或主观偏差[18]；2)二进制项的稀疏性，不能很好地适应概率建模。为了缓解这些存在的问题，我们改进并利用基于 Q 矩阵的偏序[23]来减少专家的主观影响，并将每个练习与知识点集合相关联。至于练习 j，偏序可以定义为：</p><p>具体地说，对于练习 j，如果知识点 q 被标记为 1，则我们假设 q 与练习 j 比具有标记 0 的所有其他知识点更相关。请注意，我们不能推断具有相同分数的知识点的可比性。然后，我们可以将原始 Q 矩阵变换成一组具有可比性的：</p><p>因此，DT 不像 Q-矩阵那样稀疏，能够更准确地捕捉到基于练习 j 的两个知识点(q，p)之间的成对关系，并且具有良好的解释力。我们通过合并这个先验偏序来学习潜在训练矩阵。找到所有知识点对(q，p)的正确偏序的贝叶斯公式变为最大化以下后验概率：</p><p>所有的练习都被认为是由教育专家独立批改的。我们还假设针对特定练习的每对知识点(q，p)的排序与其他每对知识点(q，p)的排序无关。因此，似然函数可以如下给出：</p><p>为了得到 V 上正确的偏序关系，我们将习题 j 与知识点 q 比知识点 p 更相关的概率定义为：</p><p>此外，在传统的贝叶斯处理之后，我们还假设 V 服从零均值高斯先验。结合公式。(4)、(5)和(6)，我们可以将对应于 V 的 上的对数后验分布表示为：</p><p>**Modeling U with two dynamic learning theories. **现在我们具体说明学生的潜在张量 U 的建模。如前所述，在学生的动态学习过程中，教育心理学中有两个广为接受的理论可以在建模过程中指导我们：1)学习曲线。[2]描述了我们所学的知识可以通过几个练习来增强。2)艾宾豪斯遗忘曲线[28]假设我们学到的知识会随着时间的推移逐渐被遗忘。<br>​</p><p>结合这两种理论，我们假设学生目前的知识水平主要受两个潜在原因的影响：1)练习越多，相关知识水平越高。2)时间过得越久，她忘记的知识就越多。形式上，我们将每个学生在时间窗口 t=2，3，…，T 的知识熟练程度的两个影响建模为：</p><p>其中，，即学生 i 在时间窗口 t 中的知识熟练程度，服从具有均值 Ut i 和方差 σ2 UI 的高斯分布。是学生 i 在 t 时刻在知识点 k 上的知识熟练程度，lt(<em>)是学习因子，表示经过多次练习后在 t 时刻学到的知识，ft(</em>)是遗忘因子，表示在 t 时刻剩余的知识，α 平衡了这两个因子，以反映学生的学习特点。直觉上，如果学生 i 有一个大的 αi，她可能会很勤奋。因此，lt(<em>)而不是 ft(</em>)对她未来的知识熟练程度影响更大，反之亦然。下面，我们正式定义 lt(<em>)和 ft(</em>)。<br>​</p><p>通过练习捕捉知识的增长：</p><p>其中表示在时间窗口 t 中检测的知识点 k 的频率，r 和 D 是两个超参数，它们分别控制增长的幅度和倍数。<br>​</p><p>描述了知识随着时间的推移而下降的情况：</p><p>其中 ∆t 是时间窗 t−1 和时间窗 t 之间的时间间隔，S 是表示记忆强度的超参数。<br>​</p><p>在初始时间 t=1，我们不知道每个学生的初始水平。因此，我们假设当时学生的知识水平服从零均值高斯分布。然后，我们将用户潜在张量上的先验总结为：</p><p><strong>Model Learning and Prediction</strong><br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1628159850408-6f4e01f1-7e9f-4a35-ad47-e801464e02d4.png#clientId=ub1ed9b4c-f6a6-4&from=paste&height=263&id=u422dca05&margin=%5Bobject%20Object%5D&name=image.png&originHeight=263&originWidth=387&originalType=binary%E2%88%B6=1&size=34273&status=done&style=none&taskId=ud408e19d-9261-4e58-93bb-c9afd273d64&width=387" alt="image.png"><br>我们总结了图 3 中提出的潜在模型的图形表示，其中阴影变量和未阴影变量表示观察变量和潜在变量。给定学生的响应张量 R 和基于 Q 矩阵的偏序，我们的目标是学习参数。具体地说，结合方程。(1)、(4)和(11)，Φ 上的后验分布为：</p><p>最大化上述方程的后验对数相当于最小化以下目标：</p><p>其中和。其中，是响应预测损失和偏序损失之间的折衷系数，λU 是衡量学生知识水平随时间变化的系数。λU1 和 λV 是学生在时间 1 的知识熟练程度和练习-知识相关矩阵的正则化参数。<br>​</p><p>具体地说，每个参数的导数为：</p><p>这里，是一个指示器函数，如果 x 为真，则该函数等于 1。<br>​</p><p>我们可以使用随机梯度下降(SGD)方法直接更新 U、V 和 b[4]。在 αi 的边界约束下，可以通过投影梯度(PG)方法找到局部最小值[17]。具体地说，对于每个 αi∈[0，1]，PG 方法按照以下规则将第 k 次迭代中的当前解更新为：</p><p>根据学生的知识熟练程度 U1，U2，…，UT 和相关参数，可以预测学生在时间 T+1 的反应和知识熟练程度为：</p><p>在获得时刻的)和后，我们可以为学生 i 推荐高概率错误反应或遗忘的相关练习。综上所述，我们给出了算法 1 中的 KPT 的训练算法。<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1628161155004-45982488-ac55-409f-904b-4f2432d82e38.png#clientId=u165ecd73-f4fe-4&from=paste&height=205&id=u3662ae43&margin=%5Bobject%20Object%5D&name=image.png&originHeight=205&originWidth=367&originalType=binary%E2%88%B6=1&size=20792&status=done&style=none&taskId=ub2b50f88-5342-4e38-899c-e5e60c7e471&width=367" alt="image.png"><br>**Time Complexity. **KPT 的大部分时间都花在计算每个学生的知识熟练程度和平衡参数上。假设响应张量 R 中有 r 个非空条目，则每个学生在每个时间窗内的平均响应记录为。在每次迭代中，U 的时间复杂度为，V 的时间复杂度为，平衡参数的时间复杂度为。因此，每次迭代参数学习的总复杂度为，它与记录和时间窗呈线性关系。</p><h3 id="实验结果-1"><a href="#实验结果-1" class="headerlink" title="实验结果"></a>实验结果</h3><p><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1628161811521-4d8d50fe-03c9-42c1-994f-ea4ee50190c2.png#clientId=u165ecd73-f4fe-4&from=paste&height=168&id=u27295bb5&margin=%5Bobject%20Object%5D&name=image.png&originHeight=168&originWidth=343&originalType=binary%E2%88%B6=1&size=19306&status=done&style=none&taskId=uceacdfbb-6cb9-4345-b5d6-4ac0642c17c&width=343" alt="image.png"><br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1628162107101-005ad4cc-a030-4cd0-8d32-9ca98015ff95.png#clientId=u165ecd73-f4fe-4&from=paste&height=456&id=ue4a69c4f&margin=%5Bobject%20Object%5D&name=image.png&originHeight=456&originWidth=368&originalType=binary%E2%88%B6=1&size=72193&status=done&style=none&taskId=u93980adc-fbe3-4ada-89b0-556a9e2e538&width=368" alt="image.png"><br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1628162461572-a815ed5c-0536-4eee-b307-35753e621963.png#clientId=u165ecd73-f4fe-4&from=paste&height=154&id=u6a6939a6&margin=%5Bobject%20Object%5D&name=image.png&originHeight=154&originWidth=709&originalType=binary%E2%88%B6=1&size=23239&status=done&style=none&taskId=u8963a301-02e0-40e9-be81-1b73917d0ab&width=709" alt="image.png"><br><strong>Experimental Setup</strong><br>我们首先介绍了 lt(<em>)和 ft(</em>)的参数设置，即学习曲线和 Ebbinghaus 遗忘曲线。具体地说，对于 lt(<em>)，我们设置 D=2 来控制增长乘数和所有知识点在 Math1、Math2、Assist 中的平均频率 r 分别为 4、9、6；对于 ft(</em>)，我们将 ∆t 设置为 1，对于时间窗口 t−1 和 t 之间的所有时间间隔，将记忆强度 S 设置为 5 以拟合遗忘曲线。然后，对于 KPT 模型中的几个正则化参数，我们设定了 λU1=λV=0.0 1。在 Math1、Math2 和 Assist 中，λU 分别设置为 3、1 和 2，在 Math1、Math2 和 Assist 中，λP 分别设置为 1.5、1 和 2(我们将在下一小节讨论参数的敏感度)。<br><strong>Experimental Results</strong><br>**Students’ Responses Prediction.**图 5 显示了所有模型在学生成绩预测任务中的总体结果。有几个观察结果：首先，我们提出的模型 KPT 在所有三个数据集上执行得最好。第二，QMIRT 和 QPMF 优于传统的 IRT 和 PMF，表明了引入偏序 Q 矩阵先验的有效性。第三，KPT 和 LFA 作为动态模型比静态假设(IRT，DINA，PMF)的表现更好，这表明从演化的角度来诊断学生的知识水平更有效。然而，BKT 在这项任务上表现不佳。我们猜测一个可能的原因是 BKT 关注的是学生们一直在做同样的练习的情景。但在我们的数据中，大多数学生只做了一次特定的练习，因此学生的练习序列长度不足以满足 BKT 的要求。综上所述，这些证据证明了三个先验(即 Q-矩阵、学习曲线和遗忘曲线)的合理性。<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1628162417125-a10d27b8-8135-4468-bec9-3e5163530e8c.png#clientId=u165ecd73-f4fe-4&from=paste&height=212&id=ue9d06da8&margin=%5Bobject%20Object%5D&name=image.png&originHeight=212&originWidth=352&originalType=binary%E2%88%B6=1&size=22043&status=done&style=none&taskId=ua9d1ac94-1902-4a11-868a-0ad1ef766c1&width=352" alt="image.png"><br>**Knowledge Proficiency Diagnosis.**直观地说，如果学生 A 在时间 T+1 的特定知识点上比学生 B 掌握得更好(由公式 19 计算)，在 T+1 时刻，她获得相关练习正确答案的概率比学生 b 高。我们采用契合度(DOA)[13，19]度量来评估这一排名表现。具体地，对于特定的知识 k，关于 k 的 DOA 结果被定义为：</p><p><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1628162661344-120bb725-5fa7-4497-86ed-3e787724d304.png#clientId=u165ecd73-f4fe-4&from=paste&height=194&id=uf5217f19&margin=%5Bobject%20Object%5D&name=image.png&originHeight=194&originWidth=717&originalType=binary%E2%88%B6=1&size=56779&status=done&style=none&taskId=u59820ee4-fddc-4482-92be-d00ec827056&width=717" alt="image.png"><br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1628162912320-8df0d9af-b3d1-494d-ad37-ce0f37287944.png#clientId=u165ecd73-f4fe-4&from=paste&height=219&id=u3093a42e&margin=%5Bobject%20Object%5D&name=image.png&originHeight=219&originWidth=377&originalType=binary%E2%88%B6=1&size=17513&status=done&style=none&taskId=u1a6de19f-210c-4940-acef-d3c4351f879&width=377" alt="image.png"><br>**Sensitivity of Parameters. **在我们的知识传授模型中，有四个参数起着至关重要的作用：λU1、λV、λu 和 λP。其中，λU1 和 λV1 分别是学生在时间 T=1 时的知识熟练程度向量的正则化参数和与练习相关的知识向量的正则化参数。由于 λU1 和 λV 具有与 PMF 模型相似的形式，因此我们将它们调到 PMF 上，并将它们设置在 PMF 上的最佳性能设置下。在下文中，我们报告了上述两个任务的设置参数 λu 和 λp 以及评估指标 Rmse 和 DOA-Avg。<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1628162933165-8424afe8-b70a-48b3-8ac2-3f550a1e9cdf.png#clientId=u165ecd73-f4fe-4&from=paste&height=402&id=u5e3bb462&margin=%5Bobject%20Object%5D&name=image.png&originHeight=402&originWidth=361&originalType=binary%E2%88%B6=1&size=52334&status=done&style=none&taskId=u0748e44e-d6d7-4e5c-b6cd-f90cff129fa&width=361" alt="image.png"><br>​</p><h3 id="总结-2"><a href="#总结-2" class="headerlink" title="总结"></a>总结</h3><p>在本文中，我们设计了一个解释概率 KPT 模型，用于解决学生在一段时间内的 KPD 任务，并利用教育先验。具体地说，我们将每个练习与一个具有 Q 矩阵先验的知识向量相关联。在同一知识空间中，每个学生在每个时刻也被表示为一个知识向量。然后，我们嵌入经典的教育理论(即学习曲线和遗忘曲线)作为先验，以捕捉每个学生的熟练程度随时间的变化。在此基础上，将学生先验知识和习题先验知识相结合，设计了一个概率矩阵因式分解框架。在三个真实数据集上的广泛实验清楚地证明了我们提出的模型的有效性和解释力。<br>​</p><p>并对今后的研究方向进行了展望。首先，我们将考虑为 KPD 任务组合更多类型的用户行为(例如，阅读记录)。第二，由于学生可能会在一些基本知识点(例如集合)之后学习困难的知识点(例如函数)，因此在 KPD 任务中考虑这种知识关系是很有趣的。</p><h1 id="四、试题困难度"><a href="#四、试题困难度" class="headerlink" title="四、试题困难度"></a>四、试题困难度</h1><p>Rasch 模型（1PL IRT）</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;整理最新的 KT 论文，从中找到自己的小论文开题方向，确定模型的改进之处和论文的创新点。&lt;/p&gt;</summary>
    
    
    
    <category term="知识追踪" scheme="https://ytno1.github.io/categories/%E7%9F%A5%E8%AF%86%E8%BF%BD%E8%B8%AA/"/>
    
    <category term="小论文开题汇总" scheme="https://ytno1.github.io/categories/%E7%9F%A5%E8%AF%86%E8%BF%BD%E8%B8%AA/%E5%B0%8F%E8%AE%BA%E6%96%87%E5%BC%80%E9%A2%98%E6%B1%87%E6%80%BB/"/>
    
    
    <category term="知识追踪" scheme="https://ytno1.github.io/tags/%E7%9F%A5%E8%AF%86%E8%BF%BD%E8%B8%AA/"/>
    
    <category term="小论文开题汇总" scheme="https://ytno1.github.io/tags/%E5%B0%8F%E8%AE%BA%E6%96%87%E5%BC%80%E9%A2%98%E6%B1%87%E6%80%BB/"/>
    
  </entry>
  
  <entry>
    <title>第二周：优化算法</title>
    <link href="https://ytno1.github.io/archives/2ad80c71.html"/>
    <id>https://ytno1.github.io/archives/2ad80c71.html</id>
    <published>2021-07-12T09:18:50.000Z</published>
    <updated>2021-08-31T07:14:52.278Z</updated>
    
    <content type="html"><![CDATA[<p>通过优化算法使得 loss 函数梯度下降平稳且快，从而达到快速训练出好的模型的效果。</p><span id="more"></span><h1 id="2-1-Mini-batch-梯度下降（Mini-batch-gradient-descent-）"><a href="#2-1-Mini-batch-梯度下降（Mini-batch-gradient-descent-）" class="headerlink" title="2.1 Mini-batch 梯度下降（Mini-batch gradient descent ）"></a>2.1 Mini-batch 梯度下降（Mini-batch gradient descent ）</h1><p>在没有提出 Mini-batch 梯度下降算法之前，都是在巨大的数据集上直接训练（整个巨大的数据集直接作为一个 batch，进行 batch 梯度下降），这就导致模型训练速度很慢，使得深度学习在之前难以发挥重大作用。</p><p>而 mini-batch 梯度下降法具体思想是将整个大数据集划分为 n 个 mini-batch 样本数据集，一个 epoch 里分别对这个 n 个 mini-batch 样本数据集进行 n 次迭代，求出每个 mini-batch 的梯度。具体细节如下：<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1626319421326-bfe7273f-a7a5-4d90-9e0a-b9715ff6baa5.png#align=left&display=inline&height=337&margin=%5Bobject%20Object%5D&name=image.png&originHeight=337&originWidth=600&size=191820&status=done&style=none&width=600" alt="image.png"></p><p>batch 梯度下降法：使用向量化，能有效地对所有 m 个样本进行计算，允许你处理整个训练集，<br>而无需某个明确的公式。把训练样本放到巨大的矩阵 X 当中去，<img src="https://cdn.nlark.com/yuque/__latex/ebe7e1fda5cd618ecc27c875fc8054ca.svg#card=math&code=X%3D%5Cleft%5Bx%5E%7B%281%29%7D%20x%5E%7B%282%29%7D%20x%5E%7B%283%29%7D%20%5Cldots%20%5Cldots%20x%5E%7B%28m%29%7D%5Cright%5D&height=35&width=210">。Y 也是如此，<img src="https://cdn.nlark.com/yuque/__latex/629a5ac5433b5ea416fbd4ca1f7a8093.svg#card=math&code=Y%3D%5Cleft%5By%5E%7B%281%29%7D%20y%5E%7B%282%29%7D%20y%5E%7B%283%29%7D%20%5Cldots%20%5Cldots%20y%5E%7B%28m%29%7D%5Cright%5D%20&height=35&width=204">。𝑌 的维数是<img src="https://cdn.nlark.com/yuque/__latex/13a1a88b5fa8505bdd1591672892ab14.svg#card=math&code=%28n_x%2Cm%29&height=20&width=53">，Y 的维数是<img src="https://cdn.nlark.com/yuque/__latex/3fdb6f1eac160fdfa13fa1e26c5a4207.svg#card=math&code=%281%2Cm%29&height=20&width=43">，使用向量化来快速处理所有 m 个样本。</p><p>mini-batch 梯度下降法：若 m 很大的话，比如 500 万甚至是 5000 万或者更大的数，则用 batch 梯度下降法来训练、处理数据，速度会很慢。因此可以把训练集分割为小一点的子集训练，这些子集被取名为 mini-batch，假设每一个子集中只有 1000 个样本，那么把其中的 𝑦 (1) 到 𝑦 (1000) 取出来，将其称为第一个子训练集，也叫做 mini-batch，然后你再取出接下来的 1000 个样本，从 𝑦 (1001) 到 𝑦 (2000) ，然后再取 1000 个样本，以此类推。</p><p>其具体的原理或做法如下图所示：<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1626324592883-65b1fbcb-8cb5-4479-9a4c-12b582fbec2a.png#align=left&display=inline&height=337&margin=%5Bobject%20Object%5D&name=image.png&originHeight=337&originWidth=600&size=199026&status=done&style=none&width=600" alt="image.png"><br>其中，同样使用向量化去几乎同时处理 1000 个样本。</p><p><strong>batch vs mini-batch：</strong> 使用 batch 梯度下降法，一次遍历训练集只能让你做一个梯度下降，使用 mini-batch 梯度下降法，一次遍历训练集，能让你做 5000 个梯度下降。</p><h1 id="2-2-理解-mini-batch-梯度下降法（Understanding-mini-batch-gradient-descent-）"><a href="#2-2-理解-mini-batch-梯度下降法（Understanding-mini-batch-gradient-descent-）" class="headerlink" title="2.2 理解 mini-batch 梯度下降法（Understanding mini-batch gradient descent ）"></a>2.2 理解 mini-batch 梯度下降法（Understanding mini-batch gradient descent ）</h1><p><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1626324960526-c1decbb5-9fbe-4b58-b5b4-8046154ccb55.png#align=left&display=inline&height=253&margin=%5Bobject%20Object%5D&name=image.png&originHeight=253&originWidth=600&size=99735&status=done&style=none&width=600" alt="image.png"><br><strong>batch 梯度下降法：</strong>每次迭代你都需要历遍整个训练集，可以预期每次迭代成本<br>都会下降，所以如果成本函数 J 是迭代次数的一个函数，它应该会随着每次迭代而减少，如会下降，所以如果成本函数 𝐾 是迭代次数的一个函数，它应该会随着每次迭代而减少，如<br>果 J 在某次迭代中增加了，那肯定出了问题，也许你的学习率太大。<br>**<br><strong>mini-batch 梯度下降法：</strong>如果你作出成本函数在整个过程中的图，则并不是每次迭<br>代都是下降的，特别是在每次迭代中，你要处理的是 X^ {t} 和 Y ^{t} ，如果要作出成本函数 J^ {t} 的图，而 J^ {t} 只和 X^ {t} ，Y ^{t} 有关，也就是每次迭代下你都在训练不同的样本集或者说训练不同的 mini-batch。<br><strong>注：</strong>没有每次迭代都下降是不要紧的，但走势应该向下。</p><p><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1626325311085-aea2bf13-e943-4971-8946-ffe80d0e6094.png#align=left&display=inline&height=114&margin=%5Bobject%20Object%5D&name=image.png&originHeight=114&originWidth=600&size=86222&status=done&style=none&width=600" alt="image.png"><br>mini-batch 的大小等于 𝑛，其实就是 batch 梯度下降法（缺点：样本数量巨大的时候，要处理整个数据集，单次迭代耗时太长）；mini-batch 大小为 1，就有了新的算法，叫做随机梯度下降法（失去所有向量化带给你的加速，因为一次性只处理了一个训练样本，这样效率过于低下）；因此一般的 mini-batch 大小为 64 到 512，考虑到电脑内存设置和使用的方式，如果 mini-batch 大小是 2 的 n 次方，代码会运行地快一些。</p><p>需要注意的是在你的 mini-batch 中，要确保 X^ {t} 和 Y ^{t} 要符合 CPU/GPU 内存，这取决<br>于你的应用方向以及训练集的大小。</p><h1 id="2-3-指数加权平均数（Exponentially-weighted-averages-）"><a href="#2-3-指数加权平均数（Exponentially-weighted-averages-）" class="headerlink" title="2.3 指数加权平均数（Exponentially weighted averages ）"></a>2.3 指数加权平均数（Exponentially weighted averages ）</h1><p>除了 batch 梯度下降法和 mini-batch 梯度下降法，还有一些更高效的梯度下降优化算法，这些都会使用到指数加权平均这一概念。<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1626325743328-f75934d3-0683-4ec8-a521-e4c79bc7d3a6.png#align=left&display=inline&height=170&margin=%5Bobject%20Object%5D&name=image.png&originHeight=170&originWidth=300&size=54174&status=done&style=none&width=300" alt="image.png"><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1626325754652-ea822ed9-65cd-4ab5-aeca-234afffea115.png#align=left&display=inline&height=167&margin=%5Bobject%20Object%5D&name=image.png&originHeight=167&originWidth=300&size=57816&status=done&style=none&width=300" alt="image.png"></p><p>使用公式<img src="https://cdn.nlark.com/yuque/__latex/bac85af1c453e25d06a254ad83480f75.svg#card=math&code=v_%7Bt%7D%3D%5Cbeta%20v_%7Bt-1%7D%2B%281-%5Cbeta%29%20%5Ctheta_%7Bt%7D&height=20&width=161">来计算每日的加权平均温度值，β=0.9 的话则得到红色曲线。<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1626325930946-b1cf66e1-3b36-40dc-86d1-3592641666a9.png#align=left&display=inline&height=214&margin=%5Bobject%20Object%5D&name=image.png&originHeight=214&originWidth=300&size=76634&status=done&style=none&width=300" alt="image.png"></p><p>注意，在计算时可视 v_t 大概是 1/(1−β) 的每日温度，如果 β 是 0.9，则可以将每日温度 v_t 看作是十天的平均值，也就是红线部分。</p><p><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1626326062035-74dfb89c-d033-4f23-946a-026e4113153f.png#align=left&display=inline&height=153&margin=%5Bobject%20Object%5D&name=image.png&originHeight=153&originWidth=300&size=56844&status=done&style=none&width=300" alt="image.png"><br>β = 0.98，即 β 较大时，相当于平均了更多天的温度，所以指数加权平均值适应地更缓慢一些，为绿线部分，这个曲线，波动更小，更加平坦，缺点是曲线进一步右移，因为现在平均的温度值更多，要平均更多的值，指数加权平均公式在温度变化时，适应地更缓慢一些，所以会出现一定延迟，；若 β = 0.5，则相当于平均了两天的温度，如黄色曲线，会有更多的噪声，有可能出现异常值，但是这个曲线能够更快适应温度变化。</p><p><strong>总结：</strong>指数加权平均数经常被使用，再说一次，它在统计学中被称为指数加权移动平均值，<br>我们就简称为指数加权平均数。这个参数（β）也是一个需要调整的超参数。</p><h1 id="2-4-理解指数加权平均数（Understanding-exponentially-weighted-averages"><a href="#2-4-理解指数加权平均数（Understanding-exponentially-weighted-averages" class="headerlink" title="2.4 理解指数加权平均数（Understanding exponentially weighted averages"></a>2.4 理解指数加权平均数（Understanding exponentially weighted averages</h1><p><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1626326486010-1a300a07-05bd-4022-9df5-3c3d4904d451.png#align=left&display=inline&height=113&margin=%5Bobject%20Object%5D&name=image.png&originHeight=113&originWidth=200&size=18905&status=done&style=none&width=200" alt="image.png"><br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1626326498108-33f68959-fd41-4dc9-87b2-0a07c11b965c.png#align=left&display=inline&height=109&margin=%5Bobject%20Object%5D&name=image.png&originHeight=109&originWidth=600&size=71118&status=done&style=none&width=600" alt="image.png"><br>所以<img src="https://cdn.nlark.com/yuque/__latex/fe27b3eab32a0e4e3298b14147ff0cb8.svg#card=math&code=v_%7B100%7D%3D0.1%20%5Ctheta_%7B100%7D%2B0.1%20%5Ctimes%200.9%20%5Ctheta_%7B99%7D%2B0.1%20%5Ctimes%280.9%29%5E%7B2%7D%20%5Ctheta_%7B98%7D%2B0.1%20%5Ctimes%280.9%29%5E%7B3%7D%20%5Ctheta_%7B97%7D%2B0.1%20%5Ctimes%280.9%29%5E%7B4%7D%20%5Ctheta_%7B96%7D%2B%5Ccdots&height=23&width=624">，这是一个加和并平均。有指数和加权,所以叫指数加权平均.</p><p>从 0.1 开始，到 0.1 × 0.9，到 0.1 × (0.9) 2 ，以此类推，所以就有了这个指数衰减函数。<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1626326598809-76b70db3-bfb7-4e13-8a0d-3b1090ff1d33.png#align=left&display=inline&height=86&margin=%5Bobject%20Object%5D&name=image.png&originHeight=86&originWidth=244&size=29750&status=done&style=none&width=244" alt="image.png"></p><p><strong>注意：</strong>实际上，β=0.9，ε=1-β=0.1，而<img src="https://cdn.nlark.com/yuque/__latex/05b41621bf35107c4b932af1039c029e.svg#card=math&code=%281-%5Cvarepsilon%29%5E%7B%5Cleft%28%5Cfrac%7B1%7D%7B%5Cvarepsilon%7D%5Cright%29%7D%3D%5Cfrac%7B1%7D%7Be%7D&height=38&width=113">,大约是 0.34，0.35，换句话说,10 天后曲线高度下降到 1/3,相当于峰值的 1/e.</p><p><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1626326960930-10c102ef-7858-451b-ab88-343c6cf1155b.png#align=left&display=inline&height=293&margin=%5Bobject%20Object%5D&name=image.png&originHeight=293&originWidth=300&size=77168&status=done&style=none&width=300" alt="image.png"><br>因此,若是梯度下降算法中,具体做法是先初始化 v<em>𝜃 = 0(v 相当于要更新的 w 或 b)，然后每一天，拿到第 t 天的数据，把 v 更新为 v:= βv</em>𝜃 + (1 − β)𝜃_t 。<br>**梯度下降时更新 w 和 b 使用指数加权平均数的好处:**它占用极少内存，电脑内存中只占用一行数字而已，然后把最新数据代入公式，不断覆盖就可以了.</p><h1 id="2-5-指-数-加-权-平-均-的-偏-差-修-正-（-Bias-correction-in-exponentially-weighted-averages"><a href="#2-5-指-数-加-权-平-均-的-偏-差-修-正-（-Bias-correction-in-exponentially-weighted-averages" class="headerlink" title="2.5 指 数 加 权 平 均 的 偏 差 修 正 （ Bias correction in exponentially weighted averages"></a>2.5 指 数 加 权 平 均 的 偏 差 修 正 （ Bias correction in exponentially weighted averages</h1><p>计算指数加权平均数，你还需要知道一个专业概念，叫做偏差修正,可以让(梯度的)平均数运算更准确.<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1626327304050-fdd844d0-bc79-48ad-8578-31fadf164ad2.png#align=left&display=inline&height=175&margin=%5Bobject%20Object%5D&name=image.png&originHeight=175&originWidth=500&size=71660&status=done&style=none&width=500" alt="image.png"><br>在上一个视频中，这个（红色）曲线对应 β 的值为 0.9，这个（绿色）曲线对应的 β=0.98，<br>如果你执行公式<img src="https://cdn.nlark.com/yuque/__latex/bac85af1c453e25d06a254ad83480f75.svg#card=math&code=v_%7Bt%7D%3D%5Cbeta%20v_%7Bt-1%7D%2B%281-%5Cbeta%29%20%5Ctheta_%7Bt%7D&height=20&width=161">，在 β 等于 0.98 的时候，得到的并不是绿色曲线，而是紫色曲线，你可以注意到紫色曲线的起点较低，我们来看看怎么处理.</p><p>可以看到偏差主要在估测初期，所以计算时不用 v*t 而是用<img src="https://cdn.nlark.com/yuque/__latex/4b6a8ed497dc203182e0ee293a756eb5.svg#card=math&code=%5Cfrac%7Bv*%7Bt%7D%7D%7B1-%5Cbeta%5E%7Bt%7D%7D&height=38&width=50">,t 就是现在的天数,则能使得估计变得更好\更准确,特别对于估计初期很有效.</p><h1 id="2-6-动量梯度下降法（Gradient-descent-with-Momentum"><a href="#2-6-动量梯度下降法（Gradient-descent-with-Momentum" class="headerlink" title="2.6 动量梯度下降法（Gradient descent with Momentum"></a>2.6 动量梯度下降法（Gradient descent with Momentum</h1><p>后续所说的这些梯度下降优化算法都用到 mini-batch 思想和指数加权平均这一概念,而具体的操作细节不同.<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1626327741384-62af14b9-3420-45a4-9b37-2659e5f6d10a.png#align=left&display=inline&height=106&margin=%5Bobject%20Object%5D&name=image.png&originHeight=106&originWidth=400&size=42835&status=done&style=none&width=400" alt="image.png"><br>在纵轴上，你希望学习慢一点，因为你不想要这些摆动，但是在横轴上，你希望加快学习，你希望快速从左向右移，移向最小值，移向红点。所以就有了动量梯度下降法.它 batch 或 mini-batch 下降法训练快很多.<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1626328205444-c1123c69-0a35-4110-8125-4d362ffbd38e.png#align=left&display=inline&height=218&margin=%5Bobject%20Object%5D&name=image.png&originHeight=218&originWidth=519&size=106319&status=done&style=none&width=519" alt="image.png"><br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1626328387365-38a4cd71-cce3-4646-a128-72cf008d146b.png#align=left&display=inline&height=360&margin=%5Bobject%20Object%5D&name=image.png&originHeight=360&originWidth=600&size=112675&status=done&style=none&width=600" alt="image.png"><br><strong>公式:</strong><br>先计算 v*dW 和 v_db:<img src="https://cdn.nlark.com/yuque/__latex/f4962ea9df79321bdc7d7b4d8a3391ea.svg#card=math&code=v*%7BdW%7D%3D%5Cbeta%20v*%7BdWb%7D%2B%281-%5Cbeta%29%20dW%2C%20v*%7Bdb%7D%3D%5Cbeta%20v*%7Bdb%7D%2B%281-%5Cbeta%29%20db&height=20&width=363"><br>然后重新更新权重:<img src="https://cdn.nlark.com/yuque/__latex/ef0372c7c344bbdd5f6faf35b84892c5.svg#card=math&code=W%3A%3DW-a%20v*%7Bd%20W%7D%2C%20b%3A%3Db-a%20v_%7Bd%20b%7D&height=18&width=220">,这样就可以减缓梯度下降的幅度。</p><p><strong>动量梯度下降法由来:</strong><br>可以理解为如果你要最小化碗状函数,想象你从山上往下滚的一个球，这些微分项 dW 和 db 提供了加速度，Momentum 项相当于速度。而因为 β 稍小于 1，表现出一些摩擦力，所以球不会无限加速下去，所以不像梯度下降法，每一步都独立于之前的步骤，你的球可以向下滚，获得动量，可以从碗向下加速获得动量。</p><h1 id="2-7-RMSprop"><a href="#2-7-RMSprop" class="headerlink" title="2.7 RMSprop"></a>2.7 RMSprop</h1><p>全称是 root mean square prop 算法，它也可以加速梯度下降.<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1626328504001-54152499-63ba-4692-a146-2edb7dcbaad2.png#align=left&display=inline&height=283&margin=%5Bobject%20Object%5D&name=image.png&originHeight=283&originWidth=560&size=148411&status=done&style=none&width=560" alt="image.png"><br>记得在横轴方向或者在例子中的 W 方向，我们希望学习速度快，而在垂直方向，也<br>就是例子中的 b 方向，我们希望减缓纵轴上的摆动，所以有了 S_dW 和 S_db ，我们希望 S_dW 会相对较小，所以我们要除以一个较小的数，而希望 S_db 又较大，所以这里我们要除以较大的数字，这样就可以减缓纵轴上的变化。</p><h1 id="2-8-Adam-优化算法-Adam-optimization-algorithm"><a href="#2-8-Adam-优化算法-Adam-optimization-algorithm" class="headerlink" title="2.8 Adam 优化算法(Adam optimization algorithm)"></a>2.8 Adam 优化算法(Adam optimization algorithm)</h1><p>Adam 优化算法基本上就是将 Momentum 和 RMSprop 结合在一起.<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1626328769053-5949b50b-9cc0-424e-b365-df17d34487fa.png#align=left&display=inline&height=317&margin=%5Bobject%20Object%5D&name=image.png&originHeight=317&originWidth=600&size=187119&status=done&style=none&width=600" alt="image.png"><br>Adam 里面同样使用了偏差修正,为了避免除以很小的数,几乎为零,在分母上加上一项 ε=1e10-8.</p><p>Adam 算法超参数值选择<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1626329006069-f22b1885-368b-4137-bf5e-e8ddcc7fac4e.png#align=left&display=inline&height=283&margin=%5Bobject%20Object%5D&name=image.png&originHeight=283&originWidth=400&size=81589&status=done&style=none&width=400" alt="image.png"></p><h1 id="2-9-学习率-衰减-Learning-rate-decay"><a href="#2-9-学习率-衰减-Learning-rate-decay" class="headerlink" title="2.9 学习率 衰减(Learning rate decay)"></a>2.9 学习率 衰减(Learning rate decay)</h1><p>加快学习算法的一个办法就是随时间慢慢减少学习率，我们将之称为学习率衰减。<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1626329095954-75a1bdab-547f-4457-b3f0-6794c42096fc.png#align=left&display=inline&height=193&margin=%5Bobject%20Object%5D&name=image.png&originHeight=193&originWidth=400&size=68144&status=done&style=none&width=400" alt="image.png"><br>学习率衰减,即慢慢减少 α 的本质在于，在学习初期，你能承受较大的步伐，但当开始收敛的时候，小一些的学习率能让你步伐小一些,如绿线部分。</p><p><strong>学习率衰减方法/公式:</strong><br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1626329203398-c3e8a502-32d3-4523-bb3b-f1927b3e3af2.png#align=left&display=inline&height=245&margin=%5Bobject%20Object%5D&name=image.png&originHeight=245&originWidth=400&size=73659&status=done&style=none&width=400" alt="image.png"><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1626329223635-f250199d-cf98-4385-ad45-7497c65ecae2.png#align=left&display=inline&height=363&margin=%5Bobject%20Object%5D&name=image.png&originHeight=363&originWidth=600&size=125150&status=done&style=none&width=600" alt="image.png"><br>其中,decay-rate 为衰减率;epoch−num 为当前 epoch 数,k 为某常熟,t 为 mini-batch 的数字.</p><h1 id="2-10-局部最优的问题-The-problem-of-local-optima"><a href="#2-10-局部最优的问题-The-problem-of-local-optima" class="headerlink" title="2.10 局部最优的问题(The problem of local optima)"></a>2.10 局部最优的问题(The problem of local optima)</h1><p><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1626329529889-d2cb0a19-2194-4f7f-a74c-6263e6575f42.png#align=left&display=inline&height=241&margin=%5Bobject%20Object%5D&name=image.png&originHeight=241&originWidth=288&size=77774&status=done&style=none&width=288" alt="image.png"><br>在深度学习领域,在高维度空间函数中,想要得到局部最优几乎不可能,因为如果你在 2 万维空间中，那么想要得到局部最优，所有的 2 万个方向都需要是这样，但发生的机率也许很小，也许是 2^−20000; 事实上，如果你要创建一个神经网络，通常梯度为零的点并不是这个图中的局部最优点，实际上成本函数的零梯度点，通常是<strong>鞍点</strong>。</p><p><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1626329560386-46277f97-a06d-41df-8004-02b2d5b7c616.png#align=left&display=inline&height=83&margin=%5Bobject%20Object%5D&name=image.png&originHeight=83&originWidth=300&size=43191&status=done&style=none&width=300" alt="image.png"><br>如果局部最优不是问题，那么问题是什么？结果是平稳段会减缓学习，平稳段是一块区域，其中导数长时间接近于 0，如果你在此处，梯度会从曲面从从上向下下降，因为梯度等于或接近 0，曲面很平坦，你得花上很长时间慢慢抵达平稳段的这个点，因为左边或右边的随机扰动，我换个笔墨颜色，大家看得清楚一些，然后你的算法能够走出平稳段（红色笔）。</p><p>**总结:**所以此次视频的要点是，</p><ul><li>第一点,首先你<strong>不太可能困在极差的局部最优中</strong>，条件是你在训练较大的神经网络，存在大量参数，并且成本函数 J 被定义在较高的维度空间。</li><li>第二点，<strong>平稳段是一个问题，这样使得学习十分缓慢</strong>，这也是像 Momentum 或是 RMSprop，Adam 这样的算法，能够加速学习算法的地方。在这些情况下，更成熟的优化算法，如 Adam 算法，能够加快速度，让你尽早往下走出平稳段。</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;通过优化算法使得 loss 函数梯度下降平稳且快，从而达到快速训练出好的模型的效果。&lt;/p&gt;</summary>
    
    
    
    <category term="笔记" scheme="https://ytno1.github.io/categories/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="吴恩达深度学习" scheme="https://ytno1.github.io/categories/%E7%AC%94%E8%AE%B0/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="第二周：优化算法" scheme="https://ytno1.github.io/categories/%E7%AC%94%E8%AE%B0/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%AC%AC%E4%BA%8C%E5%91%A8%EF%BC%9A%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/"/>
    
    
    <category term="笔记" scheme="https://ytno1.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="吴恩达深度学习" scheme="https://ytno1.github.io/tags/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="第二周：优化算法" scheme="https://ytno1.github.io/tags/%E7%AC%AC%E4%BA%8C%E5%91%A8%EF%BC%9A%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>2020-EDM论文汇总</title>
    <link href="https://ytno1.github.io/archives/cec66dd7.html"/>
    <id>https://ytno1.github.io/archives/cec66dd7.html</id>
    <published>2021-07-10T06:36:27.000Z</published>
    <updated>2021-08-31T07:14:52.264Z</updated>
    
    <content type="html"><![CDATA[<h1 id="DMKT-Learning-from-Non-Assessed-Resources-Deep-Multi-Type-Knowledge-Tracing"><a href="#DMKT-Learning-from-Non-Assessed-Resources-Deep-Multi-Type-Knowledge-Tracing" class="headerlink" title="DMKT-Learning from Non-Assessed Resources: Deep Multi-Type Knowledge Tracing"></a>DMKT-Learning from Non-Assessed Resources: Deep Multi-Type Knowledge Tracing</h1><h2 id="ABSTRACT"><a href="#ABSTRACT" class="headerlink" title="ABSTRACT"></a>ABSTRACT</h2><p>最新的知识追踪方法大多使用学生在可评估的学习资源类型(如测验、作业和练习)中的表现来建模学生的知识，而忽略了非评估的学习资源。然而，许多学生活动是未经评估的，如观看视频讲座，参加论坛，阅读教科书的一节，所有这些都可能有助于学生的知识增长。在本文中，我们提出了第一个新颖的基于深度学习的知识追踪模型(DMKT)，该模型明确地模拟了学生在评估和非评估的学习活动中的知识转移。利用 DMKT，我们可以发现每个非评估和可评估的学习材料的潜在概念，并更好地预测学生在未来评估的学习资源中的表现。我们在四个真实的数据集上将我们提出的方法与各种最新的知识跟踪方法进行了比较，显示了它在预测学生表现、表示学生知识和发现潜在领域模型方面的有效性。</p><span id="more"></span><h2 id="INTRODUCTION"><a href="#INTRODUCTION" class="headerlink" title="INTRODUCTION"></a>INTRODUCTION</h2><p>尽管学习资源类型具有异质性，但现有的学生知识追踪模型大多侧重于评估的学习资源，而忽略了非评估的学习资源。在评估的学习资源类型中，例如测验和作业，可以根据学生的答案和解决方案来评估他们的表现。这些类型的学习资源通过观察学生的表现提供了了解学生知识的窗口。相反，在非评价性学习资源中，如阅读材料和视频讲座，则不存在这样的观察。因此，在与这些学习资源互动的同时评估学生的知识和表现是一项艰巨的任务。</p><p>在本文中，我们认为，在追踪学生知识的情况下，建模非评估学习资料是必不可少的，也是不可缺少的。学生从各种类型的活动中学习，忽略大部分学生活动是学生知识追踪中错过的机会。特别是，以前的研究表明，使用各种学习活动类型对学生学习具有相当大的好处。因此，建模评估和非学校的学习活动应更准确地估计学生知识状态和对未来评估学习资源的表现预测。</p><p>因此，我们提出了深度多类型知识跟踪(DMKT)模型，该模型不仅跟踪学生在各种学习活动类型上的知识状态，而且为发现评估和非评估的学习资源的潜在模式或概念提供了一种可行的解决方案。为此，DMKT 根据学生在学习活动上的表现，估计每两次连续的评估性学习活动之间的学生知识增长。同时，它在非评估的学习活动和最近评估的活动之间分配这一估计的知识增益。我们使用一种注意力机制来进行这种分配。因此，DMKT 可以对每一种评估和非评估的学习资源的潜在概念进行建模，在与这些学习资源交互后评估学生的知识，并预测学生在评估的学习资源上的表现。</p><p>我们在四个真实数据集上对我们提出的模型进行了评估，结果表明不同学习资源类型的建模对学生成绩预测任务有显著的影响。此外，在 DMKT 使用各种学习资源类型的情况下，我们还通过可视化学生知识来展示 DMKT 的可解释性。最后，我们展示了 DMKT 在发现学习资源的相似性和潜在概念方面的能力。</p><p><strong>总结：</strong>利用学生在多种类型学习资源上的学习活动能有效提高模型预测学生知识状态方面的性能</p><h1 id="Going-Online-A-simulated-student-approach-for-evaluating-knowledge-tracing-in-the-context-of-mastery-learning"><a href="#Going-Online-A-simulated-student-approach-for-evaluating-knowledge-tracing-in-the-context-of-mastery-learning" class="headerlink" title="Going Online: A simulated student approach for evaluating knowledge tracing in the context of mastery learning"></a>Going Online: A simulated student approach for evaluating knowledge tracing in the context of mastery learning</h1><h2 id="ABSTRACT-1"><a href="#ABSTRACT-1" class="headerlink" title="ABSTRACT"></a>ABSTRACT</h2><p>在智能教学系统(ITS)中嵌入了知识跟踪算法，用于跟踪学生的学习过程。虽然知识追踪模型已经在离线环境下得到了广泛的研究，但很少有人探索它们在在线环境中的用途。这主要是因为在课堂环境中进行评估和选择知识追踪模型的实验成本很高。为了填补这一空白，我们引入了一种使用机器学习模型来生成模拟学生的新方法。我们使用学徒学习架构（Apprentice Learner Architecture）生成的代理进行实验，以调查不同知识跟踪模型(贝叶斯知识跟踪、Streak 模型和深度知识跟踪)的在线使用情况。对我们的模拟结果的分析发现，我们的贝叶斯知识跟踪模型的初始实现中存在一个错误，在我们之前的工作中没有发现这个错误。我们的模拟还揭示了深度知识追踪的一个更根本的局限性，该局限性阻碍了该模型支持对多步骤问题的掌握学习。总而言之，这两个发现表明，学徒代理（Apprentice agents）提供了一种在成本更高的课堂测试之前评估知识追踪模型的实用手段。最后，我们的分析发现，从人类数据估计的贝叶斯知识追踪参数与从模拟学习者估计的参数之间存在正相关。这表明，当没有人-学生数据可用时，可以使用模拟数据来初始化模型参数。</p><h2 id="INTRODUCTION-1"><a href="#INTRODUCTION-1" class="headerlink" title="INTRODUCTION"></a>INTRODUCTION</h2><p>我们的目标是了解哪些知识追踪模型在在线环境下产生最大的掌握学习效率。此外，我们还希望了解如何在收集人类数据之前选择知识跟踪模型的参数。为了满足我们研究知识溯源问题的多个实验的需要，我们引入了一种新颖的方法，使用学习的计算模型，即模拟学生模型，像人类学生一样从与导师的互动中学习，来模拟我们的知识溯源实验。我们使用 Apprentice Learner 架构，这是一个机器学习框架，旨在模拟人类如何从示例和反馈中学习，以生成模拟学生并进行实验。</p><p>为了探索这种方法的可行性，我们进行了实验，将贝叶斯知识跟踪(BKT)[2]与 Streak 模型[7]和深度知识跟踪(DKT)[15]进行了比较。我们的模拟显示，BKT 和 Streak 在给出所有问题之前都会停下来，但 BKT 比 Streak 稍微更具攻击性，似乎认为学生掌握技能的时间比预期的要早一些。进一步检查后，我们的分析发现 BKT 的底层实现中存在一个错误(我们为本研究修复了该错误)。此外，我们发现 DKT 表现出奇怪的行为，这使得它在掌握学习和问题选择的某些情况下无法使用。DKT 对掌握学习的这一限制在以前的工作中没有被发现。这些发现表明，在成本更高的课堂部署之前，模拟学生可能会在测试知识追踪模型方面发挥有价值的作用。</p><p>我们还探索了使用这些实验的模拟数据来估计 BKT 模型的初始参数。在收集人工数据之前，通常会将知识跟踪参数设置为合理的手工选择的默认值。一个更好的方法是对人类学生进行一项试点研究，为模型训练收集数据，这需要额外的时间和人力。我们的分析表明，从模拟数据估计的 BKT 参数与从人体数据估计的 BKT 参数之间存在正相关，表明可以使用模拟数据来初始化参数。</p><h2 id="CONCLUSIONS-AND-FUTURE-WORK"><a href="#CONCLUSIONS-AND-FUTURE-WORK" class="headerlink" title="CONCLUSIONS AND FUTURE WORK"></a>CONCLUSIONS AND FUTURE WORK</h2><p>我们能够成功地应用模拟学生测试不同的知识追踪模型。当我们将三种知识追踪模型(BKT、Streak 和 DKT)与无知识追踪基线(Random)进行比较时，我们发现 BKT 给出的问题最少，Streak 给出的问题第二少，随机给出的问题最多，DKT 在一种问题类型中给出的问题几乎与随机的一样多，在其他两种问题类型中给出的问题最少。总的来说，我们发现 BKT 似乎是最有效的方法，但是 Streak 给出了合理的结果，尽管它很简单。通过使用模拟学生，我们还发现了 BKT 实施中的一些问题，以及 DKT 的一个基本问题。尽管 BKT 实现被广泛使用，并且最近对 DKT 模型进行了大量调查，但在以前的工作中没有发现这些问题。综上所述，这些结果支持了我们的初步主张，即模拟学生是调查和评估在线知识追踪方法的有效工具。</p><p>我们的分析还发现了证据支持这样的观点，即当没有人-学生数据可用时，可以使用模拟的学生数据来初始化 BKT 参数。特别是，我们发现，从模拟数据估计的 BKT 学习率与从人类数据估计的学习率有显著的相关性。虽然这些初步结果是有希望的，但还需要做更多的工作来进一步探索这些想法。具体地说，我们希望尝试运行人体实验，将使用模拟学生数据初始化的 BKT 模型与使用默认参数的 BKT 模型进行比较。一个令人惊讶的发现是，BKT Default 的表现如此出色；尽管参数有些随意，但它比 Streak 更有效率。未来的工作应该探索如何手动为 BKT 选择健壮的默认值。</p><p>我们还有许多其他的未来方向想要探索。我们打算将学徒模型个性化，使其更好地模拟不同类型的学习者(例如，表现优异的学生和表现不佳的学生)、学习动机不同的学生以及患有学习障碍的学生的行为。我们还应该探索 DKT 的变体，以解决我们已经确定的问题，并使其能够在在线掌握学习中使用。最后，我们应该超越模拟，探索我们的模拟学生预测哪些知识追踪方法将为人类学生带来最好的学习效果。</p><p><strong>总结：</strong>使用 Apprentice Learner Architecture 生成模拟学生来测试不同的知识追踪模型，以确定合适的模型参数</p><h1 id="Knowledge-Tracing-Models’-Predictive-Performance-when-a-Student-Starts-a-Skill"><a href="#Knowledge-Tracing-Models’-Predictive-Performance-when-a-Student-Starts-a-Skill" class="headerlink" title="Knowledge Tracing Models’ Predictive Performance when  a Student Starts a Skill"></a>Knowledge Tracing Models’ Predictive Performance when  a Student Starts a Skill</h1><h2 id="ABSTRACT-2"><a href="#ABSTRACT-2" class="headerlink" title="ABSTRACT"></a>ABSTRACT</h2><p>以前关于知识追踪模型准确性的研究通常会考虑所有学生行为的表现。然而，这种做法忽略了学生最初和后来对同一技能的尝试之间的差异。为了有效地用于掌握学习等用途，知识追踪模型应该能够在学生练习某项技能几次后推断出该技能的知识和表现。然而，模型的初始性能预测——在第一次尝试新技能时——有不同的含义。它表明一个模型在从学生在其他技能上的表现以及从学生遇到的第一个项目的难度和其他属性来推断学生在一项技能上的表现方面有多成功。因此，在评估知识追踪模型时，区分这两种情况下的预测可能是相关的。在本文中，我们在一个更精细的层次上描述模型性能，并检查在给定技能上，模型性能在学生实例数量上的一致性。我们的研究结果表明，与诸如动态键值存储网络的现代算法相比，诸如 BKT(贝叶斯知识追踪)和 PFA(性能因素分析)的经典算法之间的性能差异很大程度上归因于一项技能的首次尝试。当学生第三次尝试某项技能时，模型表现更具可比性。因此，虽然使用当代知识追踪算法有许多好处，但就掌握学习而言，它们可能没有以前想象的那么不同。</p><h2 id="INTRODUCTION-2"><a href="#INTRODUCTION-2" class="headerlink" title="INTRODUCTION"></a>INTRODUCTION</h2><p>这些算法之间的比较通常集中在衡量指标上，比较在适用于优秀学生的学习系统中，预测未来项目的总体成功。在这些比较中，通常会使用多个大型数据集，但性能会在整个数据集中平均考虑。然而，有一些理由认为这可能是一种令人担忧的做法。一方面，即使使用的数据集通常很大，如果所有技能的样本都很大，这些论文通常不会报告。库切[4]指出，对于较大的数据集，BKT 参数估计比较小的数据集更精确。此外，Gervet [10]得出结论，基于逻辑回归的算法(如 PFA)倾向于对大数据集进行欠采样，而基于深度学习的算法(如 DKT)倾向于对大数据集进行过采样。</p><p>更令人担忧的是，学生建模中使用的许多数据集具有许多学生只遇到过一次或两次的技能，这可能是由于停顿[3]或很少标记的辅助技能。斯莱特和贝克[22]认为，除非有足够多的学生至少有三次机会练习每项技能，否则 BKT 模型无法可靠地适用。因此，大部分现有数据集可能反映了一种看似特殊的情况。事实上，对这些项目的准确预测很可能反映了与学生经过更多练习后的准确预测不同的东西。当一个学生还没有掌握一项技能时，预测他们在这一点上的表现就代表了所谓的“冷启动问题”——需要先有良好的表现，然后才能为当前的学生提供足够的数据[24]。在这些情况下，一些更新的算法可能比早期的算法表现得更好，要么是通过使用学生在其他技能上的表现信息，要么是特定项目的难度或其他属性信息。然而，这种更好的表现可能反映了一些不同于学生对当前正在学习的技能的知识。因此，在比较 KT 算法时，将冷启动情况(对于给定的学生和技能)与模型有足够数据自行估计当前技能的情况分开可能是有意义的。</p><h2 id="CONCLUSION-AND-DISCUSSION"><a href="#CONCLUSION-AND-DISCUSSION" class="headerlink" title="CONCLUSION AND DISCUSSION"></a>CONCLUSION AND DISCUSSION</h2><p>在过去几年中，人们对使用神经网络实现更高预测性能的知识追踪新变体的兴趣激增。然而，这项工作通常还没有探索这些算法何时何地表现更好，以及在实践中使用这些模型意味着什么。更具体地说，以前的实践对学生的整个学习历史进行了平均预测，忽略了一项技能的早期工作和后期工作之间的差异。</p><p>在这项研究中，我们考察了 BKT、PFA 和 DKVMN 三种 KT 模型在学生特定技能工作历史中的表现，并比较了这三种模型在最早和后来练习每种技能的机会中预测准确性的差异。将所有八个机会综合考虑，DKVMN 在 AUC 和 RMSE 的表现都优于 BKT 和 PFA。然而，dkwmn 更好的表现似乎很大程度上是因为它对一项技能第一次尝试的初步预测，其中 dkwmn 的 AUC 比 BKT 高 0.16，比 PFA 高 0.13，RMSE 好 0.02-0.04。第一次尝试后，BKT 和 PFA 的预测性能大幅提升，第三次尝试后，三种算法的模型性能变得更加接近，尽管 DKVMN 仍然略好。</p><p>结果表明，这些算法之间的性能差异在很大程度上是由于 DKVMN 通过使用当前技能掌握以外的因素来做出更准确的初始预测的能力，例如过去在其他技能上的表现和其他学生在同一项目上的表现。换句话说，算法之间的很大差异似乎是由于其他因素造成的，而不是从学生在该技能上的表现来估计他们对当前技能的掌握程度。在学生在特定技能[3]中途停学的数据集中，或者在系统构建后对技能模型进行添加或修改的数据集中，情况可能尤其如此。在这些情况下，许多学生/技能组合可能只出现一到两次，第一次尝试的性能相对较高会使 DKVMN 等模型的 AUC 和 RMSE 值膨胀。这就提出了一个问题：当学生看到一项新技能时，应用程序是为了在第一时间获得更好的知识预测。这种类型的预测改进对于决定学生下一步应该学习哪种技能的系统(即，[6，28])可能有用，但在具有学生要学习的技能的预定义顺序(即，[5，8])的系统中可能不太有用，并且学生在展示对当前技能的掌握之前不会继续学习。</p><p>总体而言，我们发现初步证据表明，与早期算法相比，DKVMN 性能更好的一个关键因素是它在学生有重大机会学习一项技能之前的情况下的表现。这一结果导致了关于如何更好地评估 KT 算法的建议，并表明该算法对某些应用程序(决定学生下一步应该学习哪种技能)的好处可能比其他应用程序(决定学生是否已经掌握了他们正在学习的当前技能)更大。根据这项研究的结果，未来进行 KT 模型研究的研究可能会发现，将学生在一项技能上的初始表现和后来的表现分开计算是有用的；这将为研究人员提供更多关于他们的模型是如何工作的信息，以及他们最大的好处和潜力在哪里。</p><h2 id="Behavioral-Testing-of-Deep-Neural-Network-Knowledge-Tracing-Models"><a href="#Behavioral-Testing-of-Deep-Neural-Network-Knowledge-Tracing-Models" class="headerlink" title="Behavioral Testing of Deep Neural Network Knowledge Tracing Models"></a>Behavioral Testing of Deep Neural Network Knowledge Tracing Models</h2><h2 id="ABSTRACT-3"><a href="#ABSTRACT-3" class="headerlink" title="ABSTRACT"></a>ABSTRACT</h2><p>知识追踪(KT)是一项在智能教学系统(ITS)中根据学生的课程互动对他们的知识进行建模的任务。最近，深度神经网络(DNN)在多个数据集基准上表现出了优于经典方法的性能。虽然大多数基于深度学习的知识跟踪(DLKT)模型都针对基准数据的准确性或 AUC 等一般目标指标进行了优化，但服务的正确部署需要额外的质量。此外，DNN 模型的黑箱性质使得它们在遇到意外行为时尤其难以诊断或改进。在此背景下，我们采用软件工程中的黑盒测试/行为测试的思想，(1)定义期望的 KT 模型行为；(2)提出一个 KT 模型分析框架来诊断模型的行为质量。基于提出的框架，我们在 7 个数据集上使用三种最先进的 DLKT 模型对框架进行了测试。结果突出了数据集大小和模型体系结构对模型行为质量的影响。该框架的评估结果可以单独作为模型性能的辅助度量，也可以用于通过数据扩充、架构设计和损失公式进行模型改进。</p><h2 id="INTRODUCTION-3"><a href="#INTRODUCTION-3" class="headerlink" title="INTRODUCTION"></a>INTRODUCTION</h2><p>因此，基于深度学习的知识追踪(Deep Learning Based Knowledge Tracing，DLKT)模型并不经常在教育界实施，因为缺乏模型的可解释性带来了潜在的风险。在这项研究中，我们提出行为测试作为缓解这一问题的一种方法。这项工作的贡献概括如下：</p><ul><li>提出了一种新的测试框架，通过对行为的测试来验证 DLKT 模型。我们的想法是定义 DLKT 模型所需要的一致和令人信服的行为。</li><li>作为应用该框架的一个例子，我们对所提出的验证框架中的三个最先进的 DLKT 模型进行了基准测试。积极的结果突出了 DLKT 模型的可靠性，鼓励了该模型的采用，而消极的结果则指出了 DLKT 模型的局限性，并显示了改进的空间。</li><li>我们介绍了利用该框架中的评估来设计和改进 DLKT 模型的方法。</li></ul><h2 id="CONCLUSION"><a href="#CONCLUSION" class="headerlink" title="CONCLUSION"></a>CONCLUSION</h2><p>在这项工作中，我们介绍了知识追踪模型所需的性质，并提出了一种新的基于深度学习的知识追踪模型验证框架(DLKT)。利用该框架，我们对三种流行的 DLKT 模型的行为特征进行了综合分析，并在 7 个不同的基准数据集中找出了它们的优缺点。我们相信，对该框架所诊断的优点和缺点的分析将对模型改进起到有用的指导作用。同样，基于建议框架的发现，定制采用符合数据性质和期望行为以及准确性的 DLKT 模型将成为可能。</p><p>我们认为未来可能的工作包括：(1)通过架构修改或模型组合来解决 DLKT 模型的不足；(2)使用类似于收敛测试中使用的聚合交互数据的虚拟边缘-案例数据来探索数据增强的益处；(3)将所提出的测试框架扩展到知识追踪(即学生成绩预测和项目推荐)之外。</p><h1 id="LANA-Towards-Personalized-Deep-Knowledge-Tracing-Through-Distinguishable-Interactive-Sequences"><a href="#LANA-Towards-Personalized-Deep-Knowledge-Tracing-Through-Distinguishable-Interactive-Sequences" class="headerlink" title="LANA: Towards Personalized Deep Knowledge Tracing Through Distinguishable Interactive Sequences"></a>LANA: Towards Personalized Deep Knowledge Tracing Through Distinguishable Interactive Sequences</h1><p>尚未整理，好好看一看</p><h1 id="Context-aware-Knowledge-Tracing-Integrated-with-The-Exercise-Representati-on-and-Association-in-Mathematics"><a href="#Context-aware-Knowledge-Tracing-Integrated-with-The-Exercise-Representati-on-and-Association-in-Mathematics" class="headerlink" title="Context-aware Knowledge Tracing Integrated with The  Exercise Representati on and Association in Mathematics"></a>Context-aware Knowledge Tracing Integrated with The  Exercise Representati on and Association in Mathematics</h1><h2 id="ABSTRACT-4"><a href="#ABSTRACT-4" class="headerlink" title="ABSTRACT"></a>ABSTRACT</h2><p>在新冠肺炎的影响下，在线学习已经成为世界上最重要的教育形式之一。在智能教育时代，知识溯源(KT)可以为个性化教学提供优秀的技术支持。对于在线学习，我们提出了一种结合数学习题表示和习题关联的知识追踪方法(ERAKT)。在习题表示方面，利用本体替换方法、语言模型和嵌入技术对习题的公式、文本、关联概念等多维特征进行表示，从而得到习题的统一内部表示。此外，我们还利用双向长短记忆神经网络来获取运动之间的关联，从而预测他在未来运动中的表现。在真实数据集上的大量实验清楚地证明了 ERAKT 方法的有效性，也验证了加入多维特征和运动关联确实可以提高预测的准确性。</p><h2 id="INTRODUCTION-4"><a href="#INTRODUCTION-4" class="headerlink" title="INTRODUCTION"></a>INTRODUCTION</h2><p>但传统的知识追踪模型大多只考虑到了学生的做题序列，采用知识技能代替试题，忽视了试题公式、文本和知识技能对于学生的知识状态的影响。我们认为学生表现的影响因素除了做题序列之外，试题的多维信息均会对学生表现产生重要影响。因此，为了应对以上问题，本文提出了一种融入学生试题表示与关联的数学知识追踪模型，以此来解决传统知识追踪中忽视多维试题表示与关联所造成的信息损失问题，提高模型的准确性。本文贡献如下：</p><ul><li>提出一种融合试题表示与关联的知识追踪模型，综合分析学生的做题序列、题目文本以及所包含的知识技能，来自动地学习预测他在下一次作答的表现情况以及对某一知识技能的掌握程度。</li><li>提出一种融合试题多维信息的表示技术，多维信息包括试题的文字文本、题目中的公式以及每个题目所关联的知识技能。</li><li>提出一种基于双向神经网络的序列试题关联挖掘技术，挖掘深层次的试题之间的关联内容。</li></ul><h2 id="CONCLUSION-AND-DISCUSSION-1"><a href="#CONCLUSION-AND-DISCUSSION-1" class="headerlink" title="CONCLUSION AND DISCUSSION"></a>CONCLUSION AND DISCUSSION</h2><p>在本文中，我们提出了一个改进之后的融入试题表示与关联的深度知识追踪模型，通过这些来对学生的试题表现情况做以预测，同时也预测他对于知识技能的掌握熟练程度，从而来帮助教师动态地调整自己的教学计划，做到真正的因材施教。实验验证了我们模型的有效性和可靠性，经过大量的实际调研，我们的研究是现在所有的学校实际教学过程中迫切的需求。在未来的研究中，我们计划在模型中融入多个知识点的联系与影响，从而加强模型的效果，提高预测的准确性，并将其系统化的推广，方便老师们教学工作的进行。</p><h2 id="FUTURE-WORK"><a href="#FUTURE-WORK" class="headerlink" title="FUTURE WORK"></a>FUTURE WORK</h2><p>目前，我们的研究已经取得了阶段性成果，可以应用于实际的教学环境中，辅助教师进行教学活动。我们下一步的工作将集中在两个方面：(1)探索集成多个知识概念的知识跟踪模型，同时整合它们之间的序列关联。(2)系统地展示学生对各个知识概念的掌握情况。从而提高预测的准确性，系统推广，方便教师的教学工作。</p><h1 id="Effects-of-Algorithmic-Transparency-in-Bayesian-Knowledge-Tracing-on-Trust-and-Perceived-Accuracy"><a href="#Effects-of-Algorithmic-Transparency-in-Bayesian-Knowledge-Tracing-on-Trust-and-Perceived-Accuracy" class="headerlink" title="Effects of Algorithmic Transparency in Bayesian Knowledge Tracing on Trust and Perceived Accuracy"></a>Effects of Algorithmic Transparency in Bayesian Knowledge Tracing on Trust and Perceived Accuracy</h1><h2 id="ABSTRACT-5"><a href="#ABSTRACT-5" class="headerlink" title="ABSTRACT"></a>ABSTRACT</h2><p>贝叶斯知识跟踪(Bayesian Knowledge Tracing，BKT)等知识跟踪算法可以为学生和教师提供有关他们实现学习目标的过程的有用信息。尽管 BKT 在研究界很受欢迎，但该算法在教育实践中并没有被广泛采用。这可能是由于用户的怀疑，以及如何向他们解释 BKT 以建立信任的不确定性。我们进行了一个预先登记的 2x2 调查实验(n=170)，以调查人们对 BKT 的态度，以及对算法的口头和视觉解释是如何影响这些态度的。我们发现，表面上的学习者更喜欢 BKT，而不是更简单的算法，他们认为 BKT 更值得信赖，更准确，更复杂。提供 BKT 的口头和视觉解释提高了对学习应用程序的信心、对 BKT 及其感知的准确性的信任。研究结果表明，人们对 BKT 的接受度可能比预期的要高，特别是在提供解释的情况下。</p><h2 id="INTRODUCTION-5"><a href="#INTRODUCTION-5" class="headerlink" title="INTRODUCTION"></a>INTRODUCTION</h2><p>知识溯源可以让学生和教师实时了解学生已经学到了什么，以及他们还在努力学习什么[7]。它提供了可操作的见解，可以带来更好的教育结果[16]。在众多类型的知识追踪算法中，贝叶斯知识追踪(BKT)被建立并得到了最广泛的研究，谷歌学者对“贝叶斯知识追踪”的 11.4 万个结果就证明了这一点，其中自 2020 年以来发表了 17500 个结果。BKT 已经过测试，以帮助学生自我监控他们的学习进度[4，23]，帮助教师了解学生尚未学到的内容[22]，并启用自适应学习技术，让学生跳过他们已掌握的内容[18]。与大量关于 BKT 的研究(包括数百篇致力于对原始模型[20]的增量增强的文章)相比，在实践中使用 BKT 的实际应用程序并不多。一些使用最广泛的 K-12 学习平台，如 ASSISTments 和 Khan Academy，决定不使用 BKT，转而使用更简单的模型，如 N-连续正确答案(N-CCR)[13]。这就提出了在教育实践中采用知识追踪算法的障碍问题。特别是，BKT 的相对复杂性和不透明性在多大程度上导致了它的缓慢采用？平台提供商可能会担心，教育者和学习者不会信任一种难以向他们解释的模型。</p><p>多年来，已经开发了大量的知识追踪算法，这些算法可以从如何向用户解释它们的经验证据中受益。人工智能的最新进展激发了对更复杂算法的研究，例如使用神经网络的深度知识追踪(DKT)[17，11]。随着更复杂的算法对其内部工作原理的洞察越来越少，理解人们对算法的信任及其感知的准确性如何影响对学习应用的有用性和可用性的感知变得更加重要[1]。除了 BKT 和 DKT 这两种适合建模理解和构思的学习模型外，还有逻辑斯蒂学习模型，如加性因素模型和绩效因素分析[5，19，20]，它们分别模拟记忆和流利性[20]。这两种类型的模型也可以集成到一个[15]中。虽然可以检验的模型类型很多，但我们选择了 BKT 算法作为一个例子，这种算法相对简单，在研究人员中也很受欢迎。</p><p>这项研究为解决三个重要的研究问题提供了因果证据。首先，在表面上高风险的测试场景中，人们更喜欢使用 BKT 或 N-CCR(N-连续正确答案)来学习吗？第二，他们的偏好与特定的态度有什么关系，包括他们对学习系统做好测试的信心，他们对算法的信任，以及对算法的感知准确性？第三，口头和/或视觉解释如何影响人们对知识追踪算法的态度和偏好？我们用预先登记的 2x2 析因调查实验收集的数据来回答这些研究问题。</p><p><strong>总结：</strong>偏实践应用，研究 BKT 算法透明度对其应用到学习系统中去的影响</p><h1 id="pyBKT-An-Accessible-Python-Library-of-Bayesian-Knowledge-Tracing-Models"><a href="#pyBKT-An-Accessible-Python-Library-of-Bayesian-Knowledge-Tracing-Models" class="headerlink" title="pyBKT: An Accessible Python Library of Bayesian Knowledge Tracing Models"></a>pyBKT: An Accessible Python Library of Bayesian Knowledge Tracing Models</h1><h2 id="ABSTRACT-6"><a href="#ABSTRACT-6" class="headerlink" title="ABSTRACT"></a>ABSTRACT</h2><p>贝叶斯知识追踪是一种用于认知掌握程度估计的模型，已成为适应性学习研究的一个标志，也是部署的智能教学系统(ITS)的一个重要组成部分。本文简要介绍了知识追踪模型的研究历史，并从文献中介绍了 pyBKT，一个可访问且计算效率高的模型扩展库。该库提供数据生成、拟合、预测和交叉验证例程，以及一个简单易用的数据助手接口来获取典型的导师日志数据集格式。我们评估了不同数据集大小的运行时，并与过去的实现进行了比较。此外，我们使用模拟数据的实验对模型进行了健全性检查，以评估其 EM 参数学习的准确性，并使用真实世界的数据来验证其预测，将 pyBKT 支持的模型变体与最初介绍它们的论文的结果进行了比较。该图书馆是开放源码和开放许可的，目的是使研究和实践社区更容易获得知识追踪，并通过更容易地复制过去的方法来促进该领域的进步。</p><h2 id="INTRODUCTION-6"><a href="#INTRODUCTION-6" class="headerlink" title="INTRODUCTION"></a>INTRODUCTION</h2><p>尽管它在研究界越来越受欢迎，但该模型及其文献中的许多变体的可访问和易于使用的实现仍然难以捉摸。在本文中，我们引入了一个基于 Python 的现代化库 pyBKT，使 BKT 模型和相应的适应性学习研究更容易为社区所接受。该库的接口和底层数据表示具有足够的表现力，可以复制过去的 BKT 变体，并允许提出新的模型。图书馆设计有数据助手和模型定义功能，允许方便地复制和与 BKT 模型变体进行比较，从而更好地对新的最先进的知识追踪方法进行科学进展和评估。</p><h2 id="CONCLUSIONS"><a href="#CONCLUSIONS" class="headerlink" title="CONCLUSIONS"></a>CONCLUSIONS</h2><p>我们展示了 pyBKT 是一个无缝安装的、高效的、可移植的 Python 库，具有模型扩展，如 KT-IDEM、KT-PPS、BKT+遗忘、项目顺序效果和项目学习效果。PyBKT 中的 Model 类抽象提供了一种表达方式，可以轻松地与 BKT 模型扩展交互，并使用一行方法创建、初始化、拟合、预测、评估和交叉验证 BKT 模型扩展的任何组合。我们测量到 pyBKT 的运行时间比它的前身 xBKT 快近 3 倍-4 倍，比标准 BKT 实现 BNT 快近 30,000 倍。通过所给出的分析，我们确定了 50 个学生作为合理的学生数量，以实现收敛到具有任意平均学生序列长度的规范参数值，并将 15 个作为合理的序列长度，以降低最坏情况下掌握估计的准确性。最后，通过对实际数据集的分析，验证了模型实现的有效性，并与已建立的软件进行了比较，验证了模型实现的有效性。</p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;DMKT-Learning-from-Non-Assessed-Resources-Deep-Multi-Type-Knowledge-Tracing&quot;&gt;&lt;a href=&quot;#DMKT-Learning-from-Non-Assessed-Resources-Deep-Multi-Type-Knowledge-Tracing&quot; class=&quot;headerlink&quot; title=&quot;DMKT-Learning from Non-Assessed Resources: Deep Multi-Type Knowledge Tracing&quot;&gt;&lt;/a&gt;DMKT-Learning from Non-Assessed Resources: Deep Multi-Type Knowledge Tracing&lt;/h1&gt;&lt;h2 id=&quot;ABSTRACT&quot;&gt;&lt;a href=&quot;#ABSTRACT&quot; class=&quot;headerlink&quot; title=&quot;ABSTRACT&quot;&gt;&lt;/a&gt;ABSTRACT&lt;/h2&gt;&lt;p&gt;最新的知识追踪方法大多使用学生在可评估的学习资源类型(如测验、作业和练习)中的表现来建模学生的知识，而忽略了非评估的学习资源。然而，许多学生活动是未经评估的，如观看视频讲座，参加论坛，阅读教科书的一节，所有这些都可能有助于学生的知识增长。在本文中，我们提出了第一个新颖的基于深度学习的知识追踪模型(DMKT)，该模型明确地模拟了学生在评估和非评估的学习活动中的知识转移。利用 DMKT，我们可以发现每个非评估和可评估的学习材料的潜在概念，并更好地预测学生在未来评估的学习资源中的表现。我们在四个真实的数据集上将我们提出的方法与各种最新的知识跟踪方法进行了比较，显示了它在预测学生表现、表示学生知识和发现潜在领域模型方面的有效性。&lt;/p&gt;</summary>
    
    
    
    <category term="知识追踪" scheme="https://ytno1.github.io/categories/%E7%9F%A5%E8%AF%86%E8%BF%BD%E8%B8%AA/"/>
    
    <category term="2020 EDM Paper" scheme="https://ytno1.github.io/categories/%E7%9F%A5%E8%AF%86%E8%BF%BD%E8%B8%AA/2020-EDM-Paper/"/>
    
    
    <category term="知识追踪" scheme="https://ytno1.github.io/tags/%E7%9F%A5%E8%AF%86%E8%BF%BD%E8%B8%AA/"/>
    
    <category term="2020 EDM Paper" scheme="https://ytno1.github.io/tags/2020-EDM-Paper/"/>
    
  </entry>
  
  <entry>
    <title>Python函数</title>
    <link href="https://ytno1.github.io/archives/c8f562d1.html"/>
    <id>https://ytno1.github.io/archives/c8f562d1.html</id>
    <published>2021-06-11T09:32:38.000Z</published>
    <updated>2021-08-31T07:14:52.120Z</updated>
    
    <content type="html"><![CDATA[<span id="more"></span>]]></content>
    
    
      
      
    <summary type="html">&lt;span id=&quot;more&quot;&gt;&lt;/span&gt;
</summary>
      
    
    
    
    <category term="笔记" scheme="https://ytno1.github.io/categories/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="Python函数" scheme="https://ytno1.github.io/categories/%E7%AC%94%E8%AE%B0/Python%E5%87%BD%E6%95%B0/"/>
    
    
    <category term="笔记" scheme="https://ytno1.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="Python函数" scheme="https://ytno1.github.io/tags/Python%E5%87%BD%E6%95%B0/"/>
    
  </entry>
  
  <entry>
    <title>Python入门</title>
    <link href="https://ytno1.github.io/archives/d9ef328.html"/>
    <id>https://ytno1.github.io/archives/d9ef328.html</id>
    <published>2021-06-11T07:09:24.000Z</published>
    <updated>2021-08-31T07:14:52.199Z</updated>
    
    <content type="html"><![CDATA[<p>1、注释：为代码添加注释是程序员良好的写代码习惯。对程序进行标注和说明。增加程序的可读性。程序运行的时候会自动忽略注释。<br>2、单行注释：使用#<br>3、多行注释：使用’’’注释’’’or”””注释”””形式</p><span id="more"></span>]]></content>
    
    
    <summary type="html">&lt;p&gt;1、注释：为代码添加注释是程序员良好的写代码习惯。对程序进行标注和说明。增加程序的可读性。程序运行的时候会自动忽略注释。&lt;br&gt;2、单行注释：使用#&lt;br&gt;3、多行注释：使用’’’注释’’’or”””注释”””形式&lt;/p&gt;</summary>
    
    
    
    <category term="笔记" scheme="https://ytno1.github.io/categories/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="Python入门" scheme="https://ytno1.github.io/categories/%E7%AC%94%E8%AE%B0/Python%E5%85%A5%E9%97%A8/"/>
    
    
    <category term="笔记" scheme="https://ytno1.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="Python入门" scheme="https://ytno1.github.io/tags/Python%E5%85%A5%E9%97%A8/"/>
    
  </entry>
  
  <entry>
    <title>知识追踪英文素材总结</title>
    <link href="https://ytno1.github.io/archives/c427ab9e.html"/>
    <id>https://ytno1.github.io/archives/c427ab9e.html</id>
    <published>2021-05-19T10:42:53.000Z</published>
    <updated>2021-08-31T07:14:52.127Z</updated>
    
    <content type="html"><![CDATA[<p>素材整理，每天进步一点点！</p><span id="more"></span><p>注：英语多使用被动句</p><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h2 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h2><p>To evaluate our model, the experiments are conducted on three widely-used datasets in KT and the detailed statistics are shown in Table 1.<br>​</p><p>介绍 public/benchmark datasets:assist09、assist12、Ednet etc. 页面底部插入脚注，介绍引用的数据集的网址</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>As the education landscape shifts toward distance learning, the online learning systems advance in complexity and capacity. <br>​</p><p>heterogeneous types of activities</p><p>评估是教育的中心任务，因为它涉及到元认知、跟踪技能轨迹、推荐内容、调整辅导策略]和评分。随着在线教育平台的出现，利用用户交互历史数据构建评估模型的需求越来越大。跟踪用户技能的一种方法是知识跟踪(KT)，它是根据学生在智能辅导系统(ITS)中的课程交互对他们的知识进行建模的任务。</p><h2 id="RELATED-WORK"><a href="#RELATED-WORK" class="headerlink" title="RELATED WORK"></a>RELATED WORK</h2><p>知识追踪定义：知识追踪的目的是捕捉学生的知识状态和知识状态转移模式，并进一步用于学生成绩预测、智能课程设计、学生任务结构的解释和发现等任务</p><h2 id="Baselines"><a href="#Baselines" class="headerlink" title="Baselines"></a>Baselines</h2><p>介绍要对比的模型 Baselines</p><h2 id="Implementation-Details"><a href="#Implementation-Details" class="headerlink" title="Implementation Details"></a>Implementation Details</h2><p>Other hyperparameters are chosen by grid search.</p><h2 id="Ablation-Studies"><a href="#Ablation-Studies" class="headerlink" title="Ablation Studies"></a>Ablation Studies</h2><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><p>参考文献在 30 个左右</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;素材整理，每天进步一点点！&lt;/p&gt;</summary>
    
    
    
    <category term="知识追踪" scheme="https://ytno1.github.io/categories/%E7%9F%A5%E8%AF%86%E8%BF%BD%E8%B8%AA/"/>
    
    <category term="素材" scheme="https://ytno1.github.io/categories/%E7%9F%A5%E8%AF%86%E8%BF%BD%E8%B8%AA/%E7%B4%A0%E6%9D%90/"/>
    
    
    <category term="知识追踪" scheme="https://ytno1.github.io/tags/%E7%9F%A5%E8%AF%86%E8%BF%BD%E8%B8%AA/"/>
    
    <category term="素材" scheme="https://ytno1.github.io/tags/%E7%B4%A0%E6%9D%90/"/>
    
  </entry>
  
  <entry>
    <title>Python字符串编码和解码</title>
    <link href="https://ytno1.github.io/archives/1fb1352.html"/>
    <id>https://ytno1.github.io/archives/1fb1352.html</id>
    <published>2021-05-13T09:31:53.000Z</published>
    <updated>2021-08-31T07:14:52.151Z</updated>
    
    <content type="html"><![CDATA[<p>在 Python3 中，字符串的类型为<code>str</code>，在内存中是以 unicode 编码形式表示；如果需要保存到硬盘或进行网络传输，则需把<code>str</code>转化为以字节为单位的<code>bytes</code>，是经过“可变长编码”utf-8、gbk 等编码后的字符串，是一种字节码。</p><span id="more"></span><h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>在<code>Python3</code>中，默认字符串都是<code>unicode</code>类型，<code>unicode</code>是一个万能的字符集，可以存储任意的字符，但是<code>unicode</code>字符串只能在内存中存在，不能在磁盘和网络间传输数据，如果要在文件或者网络间传输数据，必须要将<code>unicode</code>转换为<code>bytes</code>类型的字符串，因此我们在写代码的时候有时候要对<code>unicode</code>和<code>bytes</code>类型字符串进行转换，转换的函数如下：</p><ol><li><code>encode(&#39;utf-8&#39;)</code>：将<code>unicode</code>编码成<code>bytes</code>类型，并且编码方式采用的是<code>utf-8</code>。</li><li><code>decode(&#39;utf-8&#39;)</code>：将<code>bytes</code>解码成<code>unicode</code>类型，并且解码的方式采用的是<code>utf-8</code>。</li><li><code>utf-8</code>是编码的方式，还有其他编码方式，比如<code>gbk</code>、<code>ascii</code>等。</li></ol><h1 id="早期"><a href="#早期" class="headerlink" title="早期"></a>早期</h1><p>因为历史原因（计算机是美国人发明的，最早只有 127 个字符被编码到计算机里，也就是大小写英文字母、数字和一些符号，这个编码表被称为<code>ASCII</code>编码，比如大写字母<code>A</code>的编码是<code>65</code>，小写字母<code>z</code>的编码是<code>122</code>），而 Python 发布的时候 Unicode 编码还未出生，所以在<code>Python2</code>版本中，默认的字符串编码采用的是<code>ascii</code>编码；但是要处理中文等其他国家文的字显然一个字节是不够的，因此，Unicode 字符集应运而生。Unicode 把所有语言都统一到一套编码里，这样就不会再有乱码问题了。Unicode 标准也在不断发展，但最常用的是 UCS-16 编码，用两个字节表示一个字符（如果要用到非常偏僻的字符，就需要 4 个字节）。现代操作系统和大多数编程语言都直接支持 Unicode。</p><h1 id="Unicode-编码与-ASCII-编码的区别："><a href="#Unicode-编码与-ASCII-编码的区别：" class="headerlink" title="Unicode 编码与 ASCII 编码的区别："></a>Unicode 编码与 ASCII 编码的区别：</h1><p>ASCII 编码是 1 个字节，而 Unicode 编码通常至少是 2 个字节。<br>字母<code>A</code>用 ASCII 编码是十进制的<code>65</code>，二进制的<code>01000001</code>；<br>字符<code>0</code>用 ASCII 编码是十进制的<code>48</code>，二进制的<code>00110000</code>，注意字符<code>&#39;0&#39;</code>和整数<code>0</code>是不同的；<br>汉字<code>中</code>已经超出了 ASCII 编码的范围，用 Unicode 编码是十进制的<code>20013</code>，二进制的<code>01001110 00101101</code>。<br>如果把 ASCII 编码的<code>A</code>用 Unicode 编码，只需要在前面补 0 就可以，因此，<code>A</code>的 Unicode 编码是<code>00000000 01000001</code>。</p><h1 id="具体编码（utf-8、gbk、latin-1、ascii……）"><a href="#具体编码（utf-8、gbk、latin-1、ascii……）" class="headerlink" title="具体编码（utf-8、gbk、latin-1、ascii……）"></a>具体编码（utf-8、gbk、latin-1、ascii……）</h1><p>新的问题又出现了：如果统一成 Unicode 编码，乱码问题从此消失了。但是，如果你写的文本基本上全部是英文的话，用 Unicode 编码比 ASCII 编码需要多一倍的存储空间，在存储和传输上就十分不划算。</p><ul><li><code>unicode</code>是一个字符集，相当于一个字典，全世界所有的字符或者标点符号都对应一个数字。以后要在计算机中显示这个字符的时候，就使用<code>unicode</code>字符集中对应的那个数字就可以了。</li><li><code>utf-8</code>、<code>gbk</code>、<code>latin-1</code>、<code>ascii</code>都是具体的编码实现（动态编码，节省存储空间和传输流量）。 因为<code>unicode</code>中，将大部分的字符都用 2 个字节存储，但是对于英文字母，比如<code>a</code>，其实他只需要一个字节就够了，如果都用 2 个字节存储，那么比较浪费硬盘空间或者浪费流量，因此<code>unicode</code>并不适合用来存储。而<code>utf-8</code>则是<code>unicode</code>的一种实现方式，他默认会使用<code>8</code>位，也就是一个字节存储，如果存储不下了，则会动态的改变大小用来存储字符。因此<code>utf-8</code>比较节省空间，并且也可以包含全世界所有的字符。</li></ul><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://www.liaoxuefeng.com/wiki/1016959663602400/1017075323632896">https://www.liaoxuefeng.com/wiki/1016959663602400/1017075323632896</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;在 Python3 中，字符串的类型为&lt;code&gt;str&lt;/code&gt;，在内存中是以 unicode 编码形式表示；如果需要保存到硬盘或进行网络传输，则需把&lt;code&gt;str&lt;/code&gt;转化为以字节为单位的&lt;code&gt;bytes&lt;/code&gt;，是经过“可变长编码”utf-8、gbk 等编码后的字符串，是一种字节码。&lt;/p&gt;</summary>
    
    
    
    <category term="笔记" scheme="https://ytno1.github.io/categories/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="字符串编解码" scheme="https://ytno1.github.io/categories/%E7%AC%94%E8%AE%B0/%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%BC%96%E8%A7%A3%E7%A0%81/"/>
    
    
    <category term="笔记" scheme="https://ytno1.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="字符串编解码" scheme="https://ytno1.github.io/tags/%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%BC%96%E8%A7%A3%E7%A0%81/"/>
    
  </entry>
  
  <entry>
    <title>列表</title>
    <link href="https://ytno1.github.io/archives/9eaa2eb6.html"/>
    <id>https://ytno1.github.io/archives/9eaa2eb6.html</id>
    <published>2021-05-11T09:12:52.000Z</published>
    <updated>2021-08-31T07:14:52.238Z</updated>
    
    <content type="html"><![CDATA[<span id="more"></span>]]></content>
    
    
      
      
    <summary type="html">&lt;span id=&quot;more&quot;&gt;&lt;/span&gt;
</summary>
      
    
    
    
    <category term="笔记" scheme="https://ytno1.github.io/categories/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="Python List" scheme="https://ytno1.github.io/categories/%E7%AC%94%E8%AE%B0/Python-List/"/>
    
    
    <category term="笔记" scheme="https://ytno1.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="Python List" scheme="https://ytno1.github.io/tags/Python-List/"/>
    
  </entry>
  
  <entry>
    <title>2018-DKT_DSC-Deep Knowledge Tracing and Dynamic Student Classification for Knowledge Tracing(Sein Minn et al.)</title>
    <link href="https://ytno1.github.io/archives/aee0de63.html"/>
    <id>https://ytno1.github.io/archives/aee0de63.html</id>
    <published>2021-05-05T14:35:56.000Z</published>
    <updated>2021-08-31T07:14:52.183Z</updated>
    
    <content type="html"><![CDATA[<p>**Abstract **在智能导学系统(ITS)中，为了提供更具支持性的学习指导，对学生在学习过程中的知识状态进行跟踪的研究已有几十年历史。在本文中，我们提出了一种新的知识追踪模型，即：(1)捕捉学生的学习能力，并在固定的时间间隔内动态地将学生分配到具有相似能力的不同组中；(2)将这些信息与一种称为深度知识追踪(Deep Knowledge Tracing)的递归神经网络结构相结合。实验结果证实，该模型在预测学生表现方面明显优于目前最先进的学生建模技术。<br>**Index Terms ** Student model, Deep knowledge tracing, K-means clustering, RNNs, LSTMs</p><span id="more"></span><h1 id="I-INTRODUCTION"><a href="#I-INTRODUCTION" class="headerlink" title="I. INTRODUCTION"></a>I. INTRODUCTION</h1><p>ITS 是一个活跃的研究领域，旨在为学生提供个性化的指导。早期的工作可以追溯到 20 世纪 70 年代末。。人工智能和知识表示技术已经得到了广泛的探索，其中我们可以提到学生知识和误解的基于规则和贝叶斯表示，项目反应理论中使用 Logistic 回归的技能建模，基于案例的推理，以及最近的强化学习和深度学习[1][2]。甚至可以认为，人工智能和数据挖掘中的大多数主要技术都已经进入了智能导学系统(ITS)领域，特别是知识追踪问题，其目的是根据观察到的任务上的表现来模拟学生掌握概念性或程序性知识的状态[3]。</p><p>本文提出了一种新的知识追踪模型——基于动态学生分类的深度知识追踪模型(DKT-DSC)。在每个时间间隔，该模型首先将一个学生分配到一组具有相似学习能力的学生当中。然后，这些信息被馈送到递归神经网络(RNN)，该网络被称为 DKT 结构[4]，用于根据数据预测学生的表现。我们可以认为学生分类是对学生能力的长期记忆，因为 RNN 的输入改进了 DKT 的知识追踪，这是最先进的知识追踪方法之一。</p><p>本文的其余部分组织如下。第二节回顾了关于学生建模技术的相关工作。第三节提出了 DKT-DSC 模型。第四节描述了我们实验中使用的数据集。第五节给出了实验结果，第六节对本文进行了总结，并讨论了未来的研究方向。</p><h1 id="II-RELATED-WORK"><a href="#II-RELATED-WORK" class="headerlink" title="II. RELATED WORK"></a>II. RELATED WORK</h1><p>我们在这里回顾了四种最著名的、最先进的用于评估学生表现的学生建模方法，要么是因为它们在心理测量学(IRT)或教育数据挖掘(BKT)方面的优势，要么是因为它们是表现最好的方法(PFA，DKT)。详见[5]。</p><h2 id="A-Item-Response-Theory-IRT"><a href="#A-Item-Response-Theory-IRT" class="headerlink" title="A. Item Response Theory (IRT)"></a>A. Item Response Theory (IRT)</h2><p>IRT 假设学生的知识状态是静态的，用她在考试期间完成评估时的熟练程度表示[6]、[7]、[8]、[9]。IRT 对单个技能进行建模，并假设测试项目是一维的。它给学生 i 分配一个静态的熟练程度<img src="https://cdn.nlark.com/yuque/__latex/cfa3229b950bd20861dea4e3dc91abe2.svg#card=math&code=%CE%B8_i&height=18&width=13">，每个题目 j 都有它自己的难度<img src="https://cdn.nlark.com/yuque/__latex/1b3fbe2319ec5c105585847dab7782f2.svg#card=math&code=%CE%B2_j&height=21&width=16">。IRT 的主要思想是利用学生的能力和题目的难度来估计学生 i 正确回答题目 j 的概率。广泛使用的 IRT 的单参数版本，称为 Rasch 模型，是<br><img src="https://cdn.nlark.com/yuque/__latex/45f2bc1dfa57f76e09119fdcab9e6e65.svg#card=math&code=p_%7Bj%7D%5Cleft%28%5Ctheta_%7Bi%7D%5Cright%29%3D%5Cfrac%7B1%7D%7B1%2Be%5E%7B-%5Cleft%28%5Ctheta_%7Bi%7D-%5Cbeta_%7Bj%7D%5Cright%29%7D%7D%20%5Ctag%7B1%7D&height=45&width=724"><br>最近，Wilson[6]提出了一种 IRT 模型，该模型的性能优于最新的知识追踪模型。其中，<img src="https://cdn.nlark.com/yuque/__latex/cfa3229b950bd20861dea4e3dc91abe2.svg#card=math&code=%CE%B8_i&height=18&width=13">和<img src="https://cdn.nlark.com/yuque/__latex/1b3fbe2319ec5c105585847dab7782f2.svg#card=math&code=%CE%B2_j&height=21&width=16">的最大后验概率(MAP)估计是用 Newton-Raphson(牛顿-拉夫森方法)计算的。</p><h2 id="B-Bayesian-Knowledge-Tracing-BKT"><a href="#B-Bayesian-Knowledge-Tracing-BKT" class="headerlink" title="B. Bayesian Knowledge Tracing (BKT)"></a>B. Bayesian Knowledge Tracing (BKT)</h2><p>在抛弃静态知识状态假设的学习环境中引入 BKT 进行知识追踪的[3]、[10]。它还假设每个项目只测试一种技能，但这个假设在后来的 BKT 工作中有所放宽。学生对某项技能知识的标准 BKT 估计会用四种概率持续更新：[<img src="https://cdn.nlark.com/yuque/__latex/dd1e44245da6087894a99ac7a7cae82d.svg#card=math&code=P%28L_0%29&height=20&width=45">掌握的初始概率，P(T)从非掌握到掌握的转变，P(G)猜测和 P(S)失误]，一旦学生每次都给出她的回答：<br><img src="https://cdn.nlark.com/yuque/__latex/f8402b8c2edb7494aedaa23af0143eb0.svg#card=math&code=P%5Cleft%28L_%7Bn%7D%20%5Cmid%20%5Ctext%20%7B%20Correct%20%7D%5Cright%29%3D%5Cfrac%7BP%5Cleft%28L_%7Bn-1%7D%5Cright%29%281-P%28S%29%29%7D%7BP%5Cleft%28L_%7Bn-1%7D%5Cright%29%281-P%28S%29%29%2B%5Cleft%281-P%5Cleft%28L_%7Bn-1%7D%5Cright%29%5Cright%29%20P%28G%29%7D%20%5Ctag%7B2%7D&height=47&width=724"><br><img src="https://cdn.nlark.com/yuque/__latex/9e19fd10ce3bc24a00f300dd1ce06038.svg#card=math&code=P%5Cleft%28L_%7Bn%7D%20%5Cmid%20%5Ctext%20%7B%20Incorrect%20%7D%5Cright%29%3D%5Cfrac%7BP%5Cleft%28L_%7Bn-1%7D%5Cright%29%20P%28S%29%7D%7BP%5Cleft%28L_%7Bn-1%7D%5Cright%29%20P%28S%29%2B%5Cleft%281-P%5Cleft%28L_%7Bn-1%7D%5Cright%29%5Cright%29%281-P%28G%29%29%7D%20%5Ctag%7B3%7D&height=47&width=724"><br><img src="https://cdn.nlark.com/yuque/__latex/cbaf7629d679162c2a98e05f0ae2f0a9.svg#card=math&code=P%5Cleft%28L_%7Bn%7D%5Cright%29%3DP%5Cleft%28L_%7Bn-1%7D%20%5Cmid%20%5Ctext%20%7B%20Action%20%7D%5Cright%29%2B%5Cleft%281-P%5Cleft%28L_%7Bn-1%7D%20%5Cmid%20%5Ctext%20%7B%20Action%20%7D%5Cright%29%5Cright%29%20P%28T%29%20%5Ctag%7B4%7D&height=20&width=724"><br>在过去的几十年里，BKT 有了各种扩展[11]、[12]。</p><h2 id="C-Performance-Factor-Analysis-PFA"><a href="#C-Performance-Factor-Analysis-PFA" class="headerlink" title="C. Performance Factor Analysis (PFA)"></a>C. Performance Factor Analysis (PFA)</h2><p>PFA 被提出作为 BKT 的替代方案，它还放松了静态知识假设，并以其基本结构同时对多种技能进行建模[13]。它将学生 i 对项目 j 的成功概率定义为：<br><img src="https://cdn.nlark.com/yuque/__latex/36a815e0e8f3f2d7337f883888db226d.svg#card=math&code=P%5Cleft%28m_%7Bi%2C%20j%7D%5Cright%29%3D1%20%2F%5Cleft%281%2Be%5E%7B-%5Cell_%7Bi%2Cj%7D%7D%5Cright%29%20%5Ctag%7B5%7D&height=25&width=724"><br><img src="https://cdn.nlark.com/yuque/__latex/4672a57e935bab29a20239adbaeaa328.svg#card=math&code=%5Cell_%7Bi%2C%20j%7D%3D%5Cbeta_%7Bj%7D%2B%5Csum_%7Bk%20%5Cin%20%7BKC%7D_s%7D%5Cleft%28%5Cgamma_%7Bk%7D%20s_%7Bi%20k%7D%2B%5Crho_%7Bk%7D%20f_%7Bi%20k%7D%5Cright%29%20%0A%5Ctag%7B6%7D&height=42&width=724"><br>其中 β 是技能 k 的偏差，γ 和 ρ 分别代表技能 k 的每次成功尝试和失败尝试的学习增益。S 是学生 i 在技能 k 上成功尝试的次数，fik 是学生 i 在技能 k 上失败尝试的次数。</p><h2 id="D-Deep-Knowledge-Tracing-DKT"><a href="#D-Deep-Knowledge-Tracing-DKT" class="headerlink" title="D. Deep Knowledge Tracing (DKT)"></a>D. Deep Knowledge Tracing (DKT)</h2><p>DKT 在[4]中被引入。它使用长短期记忆(LSTM)[14]来动态地表示学生潜在的知识空间。学生通过作业增加的知识可以通过利用学生以前的表现历史来推断。DKT 使用大量的人工神经元来表示潜在的知识状态和时间动态结构，并允许模型从数据中学习潜在的知识状态。它由以下方程式定义：<br><img src="https://cdn.nlark.com/yuque/__latex/521eaa59a186f1a2fe655169ef821129.svg#card=math&code=h_%7Bt%7D%3D%5Ctanh%20%5Cleft%28W_%7Bh%20x%7D%20x_%7Bt%7D%2BW_%7Bh%20h%7D%20h_%7Bt-1%7D%2Bb_%7Bh%7D%5Cright%29%20%5Ctag%7B7%7D&height=20&width=724"><br><img src="https://cdn.nlark.com/yuque/__latex/af32671d7dd5da14adff177afe0e896b.svg#card=math&code=y_%7Bt%7D%3D%5Csigma%5Cleft%28W_%7By%20h%7D%20h_%7Bt%7D%2Bb_%7By%7D%5Cright%29%20%5Ctag%7B8%7D&height=23&width=724"><br>在 DKT 中，tanh 和 sigmoid 函数都是按元素应用的，并由输入权重矩阵 W、递归权重矩阵 W、初始状态 h 和读出权重矩阵 W 来参数化。潜在单元和读出单元的偏差由 b 和 b 表示。</p><h1 id="III-DEEP-KNOWLEDGE-TRACING-WITH-DYNAMIC-STUDENT-CLASSIFICATION"><a href="#III-DEEP-KNOWLEDGE-TRACING-WITH-DYNAMIC-STUDENT-CLASSIFICATION" class="headerlink" title="III. DEEP KNOWLEDGE TRACING WITH DYNAMIC STUDENT CLASSIFICATION"></a>III. DEEP KNOWLEDGE TRACING WITH DYNAMIC STUDENT CLASSIFICATION</h1><p>人类的学习是一个涉及练习的过程：我们通过练习变得熟练。然而，学习也受到个人学习能力的影响，或者说通过或多或少的练习变得熟练。我们把少练就熟练的能力称为学习能力。基于这一观点，我们提出了一个基于动态学生分类的深度知识追踪模型(DKT-DSC)，该模型评估学生的学习能力，并将其分配到一组能力相近的学生中，然后在不同的时间间隔调用一个 RNN 来跟踪她在每个不同组中的知识。它可以根据学生的学习能力追踪他们的表现，并随着时间的推移定期重新评估。</p><h2 id="A-Dynamic-assessment-of-student’s-learning-ability-and-grouping"><a href="#A-Dynamic-assessment-of-student’s-learning-ability-and-grouping" class="headerlink" title="A. Dynamic assessment of student’s learning ability and grouping"></a>A. Dynamic assessment of student’s learning ability and grouping</h2><p>根据学生以往在学习系统中不同内容上的表现，将学生划分为不同的学习能力相似的群体，已在教育领域的几项研究工作[15]、[16]中进行了探索，以便为每一组学习能力相似的学生提供更具适应性的指导。在每个时间间隔对学生学习能力的动态评估是通过基于在下一个时间间隔开始之前对其先前表现历史的评估进行聚类来执行的。</p><p>1)时间间隔：时间间隔是包含学生在系统中尝试回答的一批问题的分段。从这个角度来看，时间的一瞬就是对一个问题或练习的单个首次尝试。</p><p>2)对学生的尝试序列进行分段：将每个学生的反应序列分割成多个时间间隔，有两个目的：1)减少整个长序列学习的计算负担和记忆空间分配。2)在每个时间间隔后重新评估学生的学习能力，并在下一个时间间隔动态地将其分配到她所属的组中。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;**Abstract **在智能导学系统(ITS)中，为了提供更具支持性的学习指导，对学生在学习过程中的知识状态进行跟踪的研究已有几十年历史。在本文中，我们提出了一种新的知识追踪模型，即：(1)捕捉学生的学习能力，并在固定的时间间隔内动态地将学生分配到具有相似能力的不同组中；(2)将这些信息与一种称为深度知识追踪(Deep Knowledge Tracing)的递归神经网络结构相结合。实验结果证实，该模型在预测学生表现方面明显优于目前最先进的学生建模技术。&lt;br&gt;**Index Terms ** Student model, Deep knowledge tracing, K-means clustering, RNNs, LSTMs&lt;/p&gt;</summary>
    
    
    
    <category term="知识追踪" scheme="https://ytno1.github.io/categories/%E7%9F%A5%E8%AF%86%E8%BF%BD%E8%B8%AA/"/>
    
    <category term="DKT-DSC" scheme="https://ytno1.github.io/categories/%E7%9F%A5%E8%AF%86%E8%BF%BD%E8%B8%AA/DKT-DSC/"/>
    
    
    <category term="知识追踪" scheme="https://ytno1.github.io/tags/%E7%9F%A5%E8%AF%86%E8%BF%BD%E8%B8%AA/"/>
    
    <category term="DKT-DSC" scheme="https://ytno1.github.io/tags/DKT-DSC/"/>
    
  </entry>
  
  <entry>
    <title>爬取poj.org网站试题文本记录</title>
    <link href="https://ytno1.github.io/archives/dcab6b34.html"/>
    <id>https://ytno1.github.io/archives/dcab6b34.html</id>
    <published>2021-04-22T15:42:02.000Z</published>
    <updated>2021-08-31T07:14:52.288Z</updated>
    
    <content type="html"><![CDATA[<h1 id="开始之前"><a href="#开始之前" class="headerlink" title="开始之前"></a>开始之前</h1><p>学习爬虫并记录一下爬取试题文本时使用的代码，遇到的问题等。</p><span id="more"></span><h1 id="读取-txt-文件"><a href="#读取-txt-文件" class="headerlink" title="读取 txt 文件"></a>读取 txt 文件</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_file_as_str</span>(<span class="params">file_path</span>):</span></span><br><span class="line">    <span class="comment"># 判断路径文件存在</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.isfile(file_path):</span><br><span class="line">        <span class="keyword">raise</span> TypeError(file_path + <span class="string">&quot; does not exist&quot;</span>)</span><br><span class="line"></span><br><span class="line">    all_the_text = <span class="built_in">open</span>(file_path).read()</span><br><span class="line">    <span class="comment"># print type(all_the_text)</span></span><br><span class="line">    <span class="keyword">return</span> all_the_text</span><br><span class="line"></span><br><span class="line">file=read_file_as_str(<span class="string">&#x27;test.txt&#x27;</span>)</span><br><span class="line">re.split(<span class="string">&#x27;\d+#&#x27;</span>,file)[<span class="number">1</span>:]</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[&#39;Timeout\n&#39;,</span><br><span class="line"> &#39;Timeout\n&#39;,</span><br><span class="line"> &#39;Timeout\n&#39;,</span><br><span class="line"> &#39;Timeout\n&#39;,</span><br><span class="line"> &#39;Timeout\n&#39;,</span><br><span class="line"> &#39;Timeout\n&#39;,</span><br><span class="line"> &#39;Timeout\n&#39;,</span><br><span class="line"> &#39;Timeout\n&#39;,</span><br><span class="line"> &#39;Calculate a+b\n&#39;,</span><br><span class="line"> &#39;Problems involving the computation of exact values of very large magnitude and precision are common. For example, the computation of the national debt is a taxing experience for many computer systems.\n\nThis problem requires that you write a program to compute the exact value of Rn where R is a real number ( 0.0 &lt; R &lt; 99.999 ) and n is an integer such that 0 &lt; n &lt;&#x3D; 25.\n&#39;,</span><br><span class="line"> &quot;Businesses like to have memorable telephone numbers. One way to make a telephone number memorable is to have it spell a memorable word or phrase. For example, you can call the University of Waterloo by dialing the memorable TUT-GLOP. Sometimes only part of the number is used to spell a word. When you get back to your hotel tonight you can order a pizza from Gino&#39;s by dialing 310-GINO. Another way to make a telephone number memorable is to group the digits in a memorable way. You could order your pizza from Pizza Hut by calling their &#96;&#96;three tens&#39;&#39; number 3-10-10-10.\n\nThe standard form of a telephone number is seven decimal digits with a hyphen between the third and fourth digits (e.g. 888-1200). The keypad of a phone supplies the mapping of letters to numbers, as follows:\n\nA, B, and C map to 2\nD, E, and F map to 3\nG, H, and I map to 4\nJ, K, and L map to 5\nM, N, and O map to 6\nP, R, and S map to 7\nT, U, and V map to 8\nW, X, and Y map to 9\n\nThere is no mapping for Q or Z. Hyphens are not dialed, and can be added and removed as necessary. The standard form of TUT-GLOP is 888-4567, the standard form of 310-GINO is 310-4466, and the standard form of 3-10-10-10 is 310-1010.\n\nTwo telephone numbers are equivalent if they have the same standard form. (They dial the same number.)\n\nYour company is compiling a directory of telephone numbers from local businesses. As part of the quality control process you want to check that no two (or more) businesses in the directory have the same telephone number.\n&quot;,</span><br><span class="line"> &quot;How far can you make a stack of cards overhang a table? If you have one card, you can create a maximum overhang of half a card length. (We&#39;re assuming that the cards must be perpendicular to the table.) With two cards you can make the top card overhang the bottom one by half a card length, and the bottom one overhang the table by a third of a card length, for a total maximum overhang of 1&#x2F;2 + 1&#x2F;3 &#x3D; 5&#x2F;6 card lengths. In general you can make n cards overhang by 1&#x2F;2 + 1&#x2F;3 + 1&#x2F;4 + ... + 1&#x2F;(n + 1) card lengths, where the top card overhangs the second by 1&#x2F;2, the second overhangs tha third by 1&#x2F;3, the third overhangs the fourth by 1&#x2F;4, etc., and the bottom card overhangs the table by 1&#x2F;(n + 1). This is illustrated in the figure below.\n&quot;,</span><br><span class="line"> &quot;Larry graduated this year and finally has a job. He&#39;s making a lot of money, but somehow never seems to have enough. Larry has decided that he needs to grab hold of his financial portfolio and solve his financing problems. The first step is to figure out what&#39;s been going on with his money. Larry has his bank account statements and wants to see how much money he has. Help Larry by writing a program to take his closing balance from each of the past twelve months and calculate his average account balance.\n&quot;,</span><br><span class="line"> &#39;Fred Mapper is considering purchasing some land in Louisiana to build his house on. In the process of investigating the land, he learned that the state of Louisiana is actually shrinking by 50 square miles each year, due to erosion caused by the Mississippi River. Since Fred is hoping to live in this house the rest of his life, he needs to know if his land is going to be lost to erosion.\n\nAfter doing more research, Fred has learned that the land that is being lost forms a semicircle. This semicircle is part of a circle centered at (0,0), with the line that bisects the circle being the X axis. Locations below the X axis are in the water. The semicircle has an area of 0 at the beginning of year 1. (Semicircle illustrated in the Figure.)\n&#39;,</span><br><span class="line"> &quot;Some people believe that there are three cycles in a person&#39;s life that start the day he or she is born. These three cycles are the physical, emotional, and intellectual cycles, and they have periods of lengths 23, 28, and 33 days, respectively. There is one peak in each period of a cycle. At the peak of a cycle, a person performs at his or her best in the corresponding field (physical, emotional or mental). For example, if it is the mental curve, thought processes will be sharper and concentration will be easier.\nSince the three cycles have different periods, the peaks of the three cycles generally occur at different times. We would like to determine when a triple peak occurs (the peaks of all three cycles occur in the same day) for any person. For each cycle, you will be given the number of days from the beginning of the current year at which one of its peaks (not necessarily the first) occurs. You will also be given a date expressed as the number of days from the beginning of the current year. You task is to determine the number of days from the given date to the next triple peak. The given date is not counted. For example, if the given date is 10 and the next triple peak occurs on day 12, the answer is 2, not 3. If a triple peak occurs on the given date, you should give the number of days to the next occurrence of a triple peak.\n&quot;]</span><br></pre></td></tr></table></figure><h1 id="爬取-POJ-网站上的试题文本"><a href="#爬取-POJ-网站上的试题文本" class="headerlink" title="爬取 POJ 网站上的试题文本"></a>爬取 POJ 网站上的试题文本</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests  <span class="comment">#爬取网页的库</span></span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup <span class="comment">#用于解析网页的库</span></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;user-agent&#x27;</span>: <span class="string">&quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.70 Safari/537.36&quot;</span>,</span><br><span class="line">    &#125;  <span class="comment"># 构造请求头</span></span><br><span class="line">url = <span class="string">&#x27;http://tjj.changsha.gov.cn/tjxx/tjsj/tjgb/202010/t20201016_9060722.html&#x27;</span></span><br><span class="line">response = requests.request(<span class="string">&quot;GET&quot;</span>, url, headers=headers) <span class="comment"># 获取网页数据</span></span><br><span class="line">response.encoding = response.apparent_encoding <span class="comment"># 当获取的网页有乱码时加</span></span><br><span class="line">soup = BeautifulSoup(response.text, <span class="string">&#x27;html.parser&#x27;</span>)</span><br><span class="line">bf = soup.find(<span class="string">&#x27;div&#x27;</span>, class_=<span class="string">&#x27;view TRS_UEDITOR trs_paper_default trs_web&#x27;</span>)</span><br><span class="line">bf</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> bs4</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> sleep</span><br><span class="line">url_list = []</span><br><span class="line">headers = &#123;<span class="string">&#x27;User-Agent&#x27;</span>:<span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.70 Safari/537.36&#x27;</span>&#125; <span class="comment"># 构造请求头</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">url_all</span>():</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>,<span class="number">4055</span>):</span><br><span class="line">        url = <span class="string">&#x27;http://poj.org/problem?id=&#x27;</span>+<span class="built_in">str</span>(i)</span><br><span class="line">        url_list.append(url)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_path</span>():</span></span><br><span class="line">    s_path = <span class="string">&#x27;./text/&#x27;</span></span><br><span class="line">    <span class="keyword">if</span>  <span class="keyword">not</span> os.path.isdir(s_path):</span><br><span class="line">        os.mkdir(s_path)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="keyword">return</span> s_path</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_text</span>(<span class="params">urls,s_path</span>):</span> <span class="comment">#找到所有试题的文本描述。</span></span><br><span class="line">    num=<span class="number">1000</span></span><br><span class="line">    <span class="keyword">for</span> url <span class="keyword">in</span> urls:</span><br><span class="line">        response = requests.get(url, headers=headers)  <span class="comment"># 获取网页数据</span></span><br><span class="line">        response.encoding = response.apparent_encoding  <span class="comment"># 当获取的网页有乱码时加</span></span><br><span class="line">        soup = BeautifulSoup(response.text, <span class="string">&#x27;html.parser&#x27;</span>)</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            bf=soup.find(<span class="string">&#x27;div&#x27;</span>, class_=<span class="string">&#x27;ptx&#x27;</span>)</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(bf, bs4.element.Tag):</span><br><span class="line">                text = <span class="built_in">str</span>(num)+<span class="string">&#x27;#&#x27;</span>+<span class="built_in">str</span>(bf.text)+<span class="string">&#x27;\n&#x27;</span></span><br><span class="line">                print(<span class="string">&#x27;第&#x27;</span>+<span class="built_in">str</span>(num)+<span class="string">&#x27;题已爬！&#x27;</span>)</span><br><span class="line">                <span class="keyword">try</span>:</span><br><span class="line">                    file = <span class="built_in">open</span>(s_path + <span class="string">&#x27;poj_question_text.txt&#x27;</span>, <span class="string">&#x27;a&#x27;</span>)</span><br><span class="line">                    file.write(text)</span><br><span class="line">                    file.close()</span><br><span class="line">                <span class="keyword">except</span> BaseException <span class="keyword">as</span> a:</span><br><span class="line">                    print(a)</span><br><span class="line">        <span class="keyword">except</span> BaseException <span class="keyword">as</span> b:</span><br><span class="line">            print(b)</span><br><span class="line">        num+=<span class="number">1</span></span><br><span class="line">        sleep(random.randint(<span class="number">0</span>,<span class="number">3</span>))</span><br><span class="line">    print(<span class="string">&#x27;---------------所有页面遍历完成----------------&#x27;</span>)</span><br><span class="line"></span><br><span class="line">url_all()</span><br><span class="line">save_text(url_list,save_path())</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">1000#Calculate a+b</span><br><span class="line"></span><br><span class="line"> This problem requires that you write a program to compute the exact value of Rn where R is a real number ( 0.0 &lt; R &lt; 99.999 ) and n is an integer such that 0 &lt; n &lt;&#x3D; 25. perience for many computer systems.</span><br><span class="line"></span><br><span class="line"> Your company is compiling a directory of telephone numbers from local businesses. As part of the quality control process you want to check that no two (or more) businesses in the directory have the same telephone number. is 310-1010.LOP. Sometimes only part of the number is used to spell a word. When you get back to your hotel tonight you can order a pizza from Gino&#39;s by dialing 310-GINO. Another way to make a telephone number memorable is to group the digits in a memorable way. You could order your pizza from Pizza Hut by calling their &#96;&#96;three tens&#39;&#39; number 3-10-10-10.</span><br><span class="line"></span><br><span class="line">1003#How far can you make a stack of cards overhang a table? If you have one card, you can create a maximum overhang of half a card length. (We&#39;re assuming that the cards must be perpendicular to the table.) With two cards you can make the top card overhang the bottom one by half a card length, and the bottom one overhang the table by a third of a card length, for a total maximum overhang of 1&#x2F;2 + 1&#x2F;3 &#x3D; 5&#x2F;6 card lengths. In general you can make n cards overhang by 1&#x2F;2 + 1&#x2F;3 + 1&#x2F;4 + ... + 1&#x2F;(n + 1) card lengths, where the top card overhangs the second by 1&#x2F;2, the second overhangs tha third by 1&#x2F;3, the third overhangs the fourth by 1&#x2F;4, etc., and the bottom card overhangs the table by 1&#x2F;(n + 1). This is illustrated in the figure below.</span><br><span class="line"></span><br><span class="line">1004#Larry graduated this year and finally has a job. He&#39;s making a lot of money, but somehow never seems to have enough. Larry has decided that he needs to grab hold of his financial portfolio and solve his financing problems. The first step is to figure out what&#39;s been going on with his money. Larry has his bank account statements and wants to see how much money he has. Help Larry by writing a program to take his closing balance from each of the past twelve months and calculate his average account balance.</span><br><span class="line"></span><br><span class="line"> After doing more research, Fred has learned that the land that is being lost forms a semicircle. This semicircle is part of a circle centered at (0,0), with the line that bisects the circle being the X axis. Locations below the X axis are in the water. The semicircle has an area of 0 at the beginning of year 1. (Semicircle illustrated in the Figure.)f his land is going to be lost to erosion.</span><br><span class="line"></span><br><span class="line">Since the three cycles have different periods, the peaks of the three cycles generally occur at different times. We would like to determine when a triple peak occurs (the peaks of all three cycles occur in the same day) for any person. For each cycle, you will be given the number of days from the beginning of the current year at which one of its peaks (not necessarily the first) occurs. You will also be given a date expressed as the number of days from the beginning of the current year. You task is to determine the number of days from the given date to the next triple peak. The given date is not counted. For example, if the given date is 10 and the next triple peak occurs on day 12, the answer is 2, not 3. If a triple peak occurs on the given date, you should give the number of days to the next occurrence of a triple peak.</span><br><span class="line"></span><br><span class="line"> You are responsible for cataloguing a sequence of DNA strings (sequences containing only the four letters A, C, G, and T). However, you want to catalog them, not in alphabetical order, but rather in order of &#96;&#96;sortedness&#39;&#39;, from &#96;&#96;most sorted&#39;&#39; to &#96;&#96;least sorted&#39;&#39;. All the strings are of the same length.easure is called the number of inversions in the sequence. The sequence &#96;&#96;AACEDGG&#39;&#39; has only one inversion (E and D)---it is nearly sorted---while the sequence &#96;&#96;ZWQM&#39;&#39; has 6 inversions (it is as unsorted as can be---exactly the reverse of sorted).</span><br><span class="line"></span><br><span class="line">Help professor M. A. Ya and write a program for him to convert the dates from the Haab calendar to the Tzolkin calendar. hus, the first day was: b, 6 canac, 7 ahau, and again in the next period 8 imix, 9 ik, 10 akbal . . .r and the name of the day. They used 20 names: imix, ik, akbal, kan, chicchan, cimi, manik, lamat, muluk, ok, chuen, eb, ben, ix, mem, cib, caban, eznab, canac, ahau and 13 numbers; both in cycles.  cumhu. Instead of having names, the days of the months were denoted by numbers starting from 0 to 19. The last month of Haab was called uayet and had 5 days denoted by numbers 0, 1, 2, 3, 4. The Maya believed that this month was unlucky, the court of justice was not in session, the trade stopped, people did not even sweep the floor.</span><br><span class="line"></span><br><span class="line">Images contain 2 to 1,000,000,000 (109) pixels. All images are encoded using run length encoding (RLE). This is a sequence of pairs, containing pixel value (0-255) and run length (1-109). Input images have at most 1,000 of these pairs. Successive pairs have different pixel values. All lines in an image contain the same number of pixels.</span><br><span class="line"></span><br><span class="line"> To save money, the RPS would like to issue as few duplicate stamps as possible (given the constraint that they want to issue as many different types).  Further, the RPS won&#39;t sell more than four stamps at a time.e RPS has been known to issue several stamps of the same denomination in order to please customers (these count as different types, even though they are the same denomination).  The maximum number of different types of stamps issued at any time is twenty-five.</span><br><span class="line"></span><br><span class="line">---------------所有页面遍历完成----------------</span><br></pre></td></tr></table></figure><h1 id="追加文本"><a href="#追加文本" class="headerlink" title="追加文本"></a>追加文本</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 打开一个文件</span></span><br><span class="line">fo = <span class="built_in">open</span>(<span class="string">&quot;test.txt&quot;</span>, <span class="string">&quot;a&quot;</span>)</span><br><span class="line">fo.write( <span class="string">&quot;www.runoob.com!\nVery good site!\n&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 关闭打开的文件</span></span><br><span class="line">fo.close()</span><br></pre></td></tr></table></figure><h1 id="测试自己-IP-为多少"><a href="#测试自己-IP-为多少" class="headerlink" title="测试自己 IP 为多少"></a>测试自己 IP 为多少</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;http://httpbin.org/get&#x27;</span> <span class="comment"># 该网址会返回访问者的IP</span></span><br><span class="line">headers = &#123;<span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0&#x27;</span>&#125;</span><br><span class="line"><span class="comment"># 使用代理IP</span></span><br><span class="line">proxies = &#123;<span class="string">&#x27;http&#x27;</span>:<span class="string">&#x27;http://122.4.48.145:9999&#x27;</span>,<span class="string">&#x27;https&#x27;</span>:<span class="string">&#x27;https://122.4.48.145:9999&#x27;</span>&#125;</span><br><span class="line">html = requests.get(url, headers=headers,proxies=proxies, timeout=<span class="number">5</span>).text</span><br><span class="line">print(html)</span><br></pre></td></tr></table></figure><h1 id="使用-fake-useragent-随机生成-User-Agent"><a href="#使用-fake-useragent-随机生成-User-Agent" class="headerlink" title="使用 fake_useragent 随机生成 User-Agent"></a>使用 fake_useragent 随机生成 User-Agent</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> fake_useragent <span class="keyword">import</span> UserAgent</span><br><span class="line">ua = UserAgent()</span><br><span class="line">ua.ie</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#39;Mozilla&#x2F;5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident&#x2F;5.0; SLCC2; Media Center PC 6.0; InfoPath.3; MS-RTC LM 8; Zune 4.7&#39;</span><br></pre></td></tr></table></figure><h1 id="构建自己的-IP-池"><a href="#构建自己的-IP-池" class="headerlink" title="构建自己的 IP 池"></a>构建自己的 IP 池</h1><p>从快代理上面爬取 IP，迭代测试能否使用，建立一个自己的代理 IP 池，随时更新用来抓取网站数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> fake_useragent <span class="keyword">import</span> UserAgent</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GetProxyIP</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.url = <span class="string">&#x27;https://www.kuaidaili.com/free/inha/&#123;&#125;/&#x27;</span> <span class="comment"># &#x27;https://www.xicidaili.com/nn/&#x27;</span></span><br><span class="line">        self.proxies = &#123;</span><br><span class="line">            <span class="string">&#x27;http&#x27;</span>: <span class="string">&#x27;http://163.204.247.219:9999&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;https&#x27;</span>: <span class="string">&#x27;https://163.204.247.219:9999&#x27;</span>&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 随机生成User-Agent</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_random_ua</span>(<span class="params">self</span>):</span></span><br><span class="line">        ua = UserAgent()        <span class="comment"># 创建User-Agent对象</span></span><br><span class="line">        useragent = ua.random</span><br><span class="line">        <span class="keyword">return</span> useragent</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 从IP代理网站上获取随机的代理IP</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_ip_file</span>(<span class="params">self, url</span>):</span></span><br><span class="line">        headers = &#123;<span class="string">&#x27;User-Agent&#x27;</span>: self.get_random_ua()&#125;</span><br><span class="line">        <span class="comment"># 访问IP代理网站国内高匿代理，找到所有的tr节点对象</span></span><br><span class="line">        html = requests.get(url=url, headers=headers, timeout=<span class="number">5</span>).content.decode(<span class="string">&#x27;utf-8&#x27;</span>, <span class="string">&#x27;ignore&#x27;</span>)</span><br><span class="line">        parse_html = etree.HTML(html)</span><br><span class="line">        <span class="comment"># 基准xpath，匹配每个代理IP的节点对象列表</span></span><br><span class="line">        tr_list = parse_html.xpath(<span class="string">&#x27;//*[@id=&quot;list&quot;]/table/tbody/tr&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> tr <span class="keyword">in</span> tr_list:</span><br><span class="line">            ip = tr.xpath(<span class="string">&#x27;./td[1]/text()&#x27;</span>)[<span class="number">0</span>]</span><br><span class="line">            port = tr.xpath(<span class="string">&#x27;./td[2]/text()&#x27;</span>)[<span class="number">0</span>]</span><br><span class="line">            <span class="comment"># 测试ip:port是否可用</span></span><br><span class="line">            self.test_proxy_ip(ip, port)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 测试抓取的代理IP是否可用</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">test_proxy_ip</span>(<span class="params">self, ip, port</span>):</span></span><br><span class="line">        proxies = &#123;</span><br><span class="line">            <span class="string">&#x27;http&#x27;</span>: <span class="string">&#x27;http://&#123;&#125;:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(ip, port),</span><br><span class="line">            <span class="string">&#x27;https&#x27;</span>: <span class="string">&#x27;https://&#123;&#125;:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(ip, port), &#125;</span><br><span class="line">        test_url = <span class="string">&#x27;http://www.baidu.com/&#x27;</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            res = requests.get(url=test_url, proxies=proxies, timeout=<span class="number">8</span>)</span><br><span class="line">            <span class="keyword">if</span> res.status_code == <span class="number">200</span>:</span><br><span class="line">                print(ip, <span class="string">&quot;:&quot;</span>, port, <span class="string">&#x27;Success&#x27;</span>)</span><br><span class="line">                <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;proxies.txt&#x27;</span>, <span class="string">&#x27;a&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">                    f.write(ip + <span class="string">&#x27;:&#x27;</span> + port + <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">            print(ip, port, <span class="string">&#x27;Failed&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 主函数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2000</span>, <span class="number">2050</span>):</span><br><span class="line">            url = self.url.<span class="built_in">format</span>(i)</span><br><span class="line">            self.get_ip_file(url)</span><br><span class="line">            time.sleep(random.randint(<span class="number">5</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">spider = GetProxyIP()</span><br><span class="line">spider.main()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># import requests</span></span><br><span class="line"><span class="comment"># from lxml import etree</span></span><br><span class="line"><span class="comment"># import time</span></span><br><span class="line"><span class="comment"># import random</span></span><br><span class="line"><span class="comment"># from fake_useragent import UserAgent</span></span><br><span class="line"><span class="comment"># url=&#x27;https://www.kuaidaili.com/free/inha/&#123;&#125;/&#x27;</span></span><br><span class="line"><span class="comment"># url</span></span><br><span class="line"><span class="comment"># headers = &#123;&#x27;User-Agent&#x27;: &#x27;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.70 Safari/537.36&#x27;&#125;</span></span><br><span class="line"><span class="comment"># # 访问IP代理网站国内高匿代理，找到所有的tbody节点对象</span></span><br><span class="line"><span class="comment"># url = url.format(1)</span></span><br><span class="line"><span class="comment"># url</span></span><br><span class="line"><span class="comment"># html = requests.get(url=url, headers=headers, timeout=5).content.decode(&#x27;utf-8&#x27;, &#x27;ignore&#x27;)</span></span><br><span class="line"><span class="comment"># parse_html = etree.HTML(html)</span></span><br><span class="line"><span class="comment"># # 基准xpath，匹配每个代理IP的节点对象列表</span></span><br><span class="line"><span class="comment"># tr_list = parse_html.xpath(&#x27;//*[@id=&quot;list&quot;]/table/tbody/tr&#x27;)</span></span><br><span class="line"><span class="comment"># print(type(tr_list))</span></span><br><span class="line"><span class="comment"># ip = tr_list[0].xpath(&#x27;./td[1]/text()&#x27;)[0]</span></span><br><span class="line"><span class="comment"># ip</span></span><br><span class="line"><span class="comment"># port = tr_list[0].xpath(&#x27;./td[2]/text()&#x27;)[0]</span></span><br><span class="line"><span class="comment"># port</span></span><br></pre></td></tr></table></figure><h1 id="测试代理-IP-是否可用"><a href="#测试代理-IP-是否可用" class="headerlink" title="测试代理 IP 是否可用"></a>测试代理 IP 是否可用</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">headers = &#123;<span class="string">&#x27;user-agent&#x27;</span>: <span class="string">&quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.70 Safari/537.36&quot;</span>,&#125;  <span class="comment"># 构造请求头</span></span><br><span class="line">proxies = &#123;<span class="string">&#x27;http&#x27;</span>:<span class="string">&#x27;http://10.10.10.10:8765&#x27;</span>,<span class="string">&#x27;https&#x27;</span>:<span class="string">&#x27;https://10.10.10.10:8765&#x27;</span>&#125;</span><br><span class="line">url=<span class="string">&#x27;http://www.baidu.com/&#x27;</span></span><br><span class="line">resp = requests.get(url,headers=headers, proxies = proxies, timeout=<span class="number">5</span>)</span><br><span class="line"><span class="keyword">if</span> res.status_code == <span class="number">200</span>:</span><br><span class="line">        print(<span class="string">&quot;OK&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(res.status_code)</span><br><span class="line">        print(<span class="string">&quot;错误&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure><h1 id="从-IP-池中取-IP"><a href="#从-IP-池中取-IP" class="headerlink" title="从 IP 池中取 IP"></a>从 IP 池中取 IP</h1><p>从文件中随机获取代理 IP 写爬虫，防止同一个 IP 访问频繁被封</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BaiduSpider</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.url = <span class="string">&#x27;http://www.baidu.com/&#x27;</span></span><br><span class="line">        self.headers = &#123;<span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0&#x27;</span>&#125;</span><br><span class="line">        self.flag = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_proxies</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;proxies.txt&#x27;</span>, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            result = f.readlines()  <span class="comment"># 读取所有行并返回列表</span></span><br><span class="line">        proxy_ip = random.choice(result)[:-<span class="number">1</span>]       <span class="comment"># 获取了所有代理IP</span></span><br><span class="line">        L = proxy_ip.split(<span class="string">&#x27;:&#x27;</span>)</span><br><span class="line">        proxy_ip = &#123;</span><br><span class="line">            <span class="string">&#x27;http&#x27;</span>: <span class="string">&#x27;http://&#123;&#125;:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(L[<span class="number">0</span>], L[<span class="number">1</span>]),</span><br><span class="line">            <span class="string">&#x27;https&#x27;</span>: <span class="string">&#x27;https://&#123;&#125;:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(L[<span class="number">0</span>], L[<span class="number">1</span>])</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> proxy_ip</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_html</span>(<span class="params">self</span>):</span></span><br><span class="line">        proxies = self.get_proxies()</span><br><span class="line">        <span class="keyword">if</span> self.flag &lt;= <span class="number">3</span>:</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                html = requests.get(url=self.url, proxies=proxies, headers=self.headers, timeout=<span class="number">5</span>).text</span><br><span class="line">                print(html)</span><br><span class="line">            <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">                print(<span class="string">&#x27;Retry&#x27;</span>)</span><br><span class="line">                self.flag += <span class="number">1</span></span><br><span class="line">                self.get_html()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">spider = BaiduSpider()</span><br><span class="line">spider.get_html()</span><br></pre></td></tr></table></figure><h1 id="获取收费代理-IP"><a href="#获取收费代理-IP" class="headerlink" title="获取收费代理 IP"></a>获取收费代理 IP</h1><p>写一个获取收费开放 API 代理的接口</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获取开放代理的接口</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> fake_useragent <span class="keyword">import</span> UserAgent</span><br><span class="line"></span><br><span class="line">ua = UserAgent()  <span class="comment"># 创建User-Agent对象</span></span><br><span class="line">useragent = ua.random</span><br><span class="line">headers = &#123;<span class="string">&#x27;User-Agent&#x27;</span>: useragent&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ip_test</span>(<span class="params">ip</span>):</span></span><br><span class="line">    url = <span class="string">&#x27;http://www.baidu.com/&#x27;</span></span><br><span class="line">    ip_port = ip.split(<span class="string">&#x27;:&#x27;</span>)</span><br><span class="line">    proxies = &#123;</span><br><span class="line">        <span class="string">&#x27;http&#x27;</span>: <span class="string">&#x27;http://&#123;&#125;:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(ip_port[<span class="number">0</span>], ip_port[<span class="number">1</span>]),</span><br><span class="line">        <span class="string">&#x27;https&#x27;</span>: <span class="string">&#x27;https://&#123;&#125;:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(ip_port[<span class="number">0</span>], ip_port[<span class="number">1</span>]),</span><br><span class="line">    &#125;</span><br><span class="line">    res = requests.get(url=url, headers=headers, proxies=proxies, timeout=<span class="number">5</span>)</span><br><span class="line">    <span class="keyword">if</span> res.status_code == <span class="number">200</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 提取代理IP</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_ip_list</span>():</span></span><br><span class="line">    <span class="comment"># 快代理：https://www.kuaidaili.com/doc/product/dps/</span></span><br><span class="line">    api_url = <span class="string">&#x27;http://dev.kdlapi.com/api/getproxy/?orderid=946562662041898#=100&amp;protocol=1&amp;method=2&amp;an_an=1&amp;an_ha=1&amp;sep=2&#x27;</span></span><br><span class="line">    html = requests.get(api_url).content.decode(<span class="string">&#x27;utf-8&#x27;</span>, <span class="string">&#x27;ignore&#x27;</span>)</span><br><span class="line">    ip_port_list = html.split(<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> ip <span class="keyword">in</span> ip_port_list:</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;proxy_ip.txt&#x27;</span>, <span class="string">&#x27;a&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            <span class="keyword">if</span> ip_test(ip):</span><br><span class="line">                f.write(ip + <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    get_ip_list()</span><br></pre></td></tr></table></figure><h1 id="获取私密代理-IP"><a href="#获取私密代理-IP" class="headerlink" title="获取私密代理 IP"></a>获取私密代理 IP</h1><p>用户名和密码会在给你 API_URL 的时候给你。不是你的账号和账号密码。格式如下：</p><p>proxies = {‘协议’:’协议://用户名:密码@IP:端口号’}</p><p>proxies = {‘http’:’<a href="http://%E7%94%A8%E6%88%B7%E5%90%8D:%E5%AF%86%E7%A0%81@ip/">http://用户名:密码@IP</a>:端口号’, ‘https’:’<a href="https://%E7%94%A8%E6%88%B7%E5%90%8D:%E5%AF%86%E7%A0%81@ip/">https://用户名:密码@IP</a>:端口号’}</p><p>proxies = {‘http’: ‘<a href="http://309435365:szayclhp@106.75.71.140:16816/">http://309435365:szayclhp@106.75.71.140:16816</a>‘, ‘https’:’<a href="https://309435365:szayclhp@106.75.71.140:16816/">https://309435365:szayclhp@106.75.71.140:16816</a>‘}</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获取开放代理的接口</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> fake_useragent <span class="keyword">import</span> UserAgent</span><br><span class="line"></span><br><span class="line">ua = UserAgent()  <span class="comment"># 创建User-Agent对象</span></span><br><span class="line">useragent = ua.random</span><br><span class="line">headers = &#123;<span class="string">&#x27;User-Agent&#x27;</span>: useragent&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ip_test</span>(<span class="params">ip</span>):</span></span><br><span class="line">    url = <span class="string">&#x27;https://blog.csdn.net/qq_34218078/article/details/90901602/&#x27;</span></span><br><span class="line">    ip_port = ip.split(<span class="string">&#x27;:&#x27;</span>)</span><br><span class="line">    proxies = &#123;</span><br><span class="line">        <span class="string">&#x27;http&#x27;</span>: <span class="string">&#x27;http://1786088386:b95djiha@&#123;&#125;:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(ip_port[<span class="number">0</span>], ip_port[<span class="number">1</span>]),</span><br><span class="line">        <span class="string">&#x27;https&#x27;</span>: <span class="string">&#x27;http://1786088386:b95djiha@&#123;&#125;:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(ip_port[<span class="number">0</span>], ip_port[<span class="number">1</span>]),</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    res = requests.get(url=url, headers=headers, proxies=proxies, timeout=<span class="number">5</span>)</span><br><span class="line">    <span class="keyword">if</span> res.status_code == <span class="number">200</span>:</span><br><span class="line">        print(<span class="string">&quot;OK&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(res.status_code)</span><br><span class="line">        print(<span class="string">&quot;错误&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 提取代理IP</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_ip_list</span>():</span></span><br><span class="line">    <span class="comment"># 快代理：https://www.kuaidaili.com/doc/product/dps/</span></span><br><span class="line">    api_url = <span class="string">&#x27;http://dps.kdlapi.com/api/getdps/?orderid=986603271748760#=1000&amp;signature=z4a5b2rpt062iejd6h7wvox16si0f7ct&amp;pt=1&amp;sep=2&#x27;</span></span><br><span class="line">    html = requests.get(api_url).content.decode(<span class="string">&#x27;utf-8&#x27;</span>, <span class="string">&#x27;ignore&#x27;</span>)</span><br><span class="line">    ip_port_list = html.split(<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> ip <span class="keyword">in</span> ip_port_list:</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;proxy_ip.txt&#x27;</span>, <span class="string">&#x27;a&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            <span class="keyword">if</span> ip_test(ip):</span><br><span class="line">                f.write(ip + <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    get_ip_list()</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;开始之前&quot;&gt;&lt;a href=&quot;#开始之前&quot; class=&quot;headerlink&quot; title=&quot;开始之前&quot;&gt;&lt;/a&gt;开始之前&lt;/h1&gt;&lt;p&gt;学习爬虫并记录一下爬取试题文本时使用的代码，遇到的问题等。&lt;/p&gt;</summary>
    
    
    
    <category term="Python" scheme="https://ytno1.github.io/categories/Python/"/>
    
    <category term="爬虫" scheme="https://ytno1.github.io/categories/Python/%E7%88%AC%E8%99%AB/"/>
    
    
    <category term="Python" scheme="https://ytno1.github.io/tags/Python/"/>
    
    <category term="爬虫" scheme="https://ytno1.github.io/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>好用的PDF转word网站</title>
    <link href="https://ytno1.github.io/archives/8a435807.html"/>
    <id>https://ytno1.github.io/archives/8a435807.html</id>
    <published>2021-04-15T13:03:39.000Z</published>
    <updated>2021-08-31T07:14:52.193Z</updated>
    
    <content type="html"><![CDATA[<h1 id="开始之前"><a href="#开始之前" class="headerlink" title="开始之前"></a>开始之前</h1><p>推荐几个常用的免费 PDF 处理网站</p><span id="more"></span><h1 id="网站资源"><a href="#网站资源" class="headerlink" title="网站资源"></a>网站资源</h1><ul><li><a href="https://123apps.com/">https://123apps.com/</a> 不只是能处理 PDF 文件，还有关于视音频的处理工具，界面简介无广告，可切换语言</li><li><a href="https://www.ilovepdf.com/">https://www.ilovepdf.com/</a> 网站可以切换至中文网页，界面干净</li><li><a href="https://smallpdf.com/cn">https://smallpdf.com/cn</a> 同样不错</li><li><a href="https://www.onlinedoctranslator.com/zh-CN/">https://www.onlinedoctranslator.com/zh-CN/</a> 这个也可</li></ul>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;开始之前&quot;&gt;&lt;a href=&quot;#开始之前&quot; class=&quot;headerlink&quot; title=&quot;开始之前&quot;&gt;&lt;/a&gt;开始之前&lt;/h1&gt;&lt;p&gt;推荐几个常用的免费 PDF 处理网站&lt;/p&gt;</summary>
    
    
    
    <category term="资源贴" scheme="https://ytno1.github.io/categories/%E8%B5%84%E6%BA%90%E8%B4%B4/"/>
    
    <category term="PDF网站" scheme="https://ytno1.github.io/categories/%E8%B5%84%E6%BA%90%E8%B4%B4/PDF%E7%BD%91%E7%AB%99/"/>
    
    
    <category term="资源贴" scheme="https://ytno1.github.io/tags/%E8%B5%84%E6%BA%90%E8%B4%B4/"/>
    
    <category term="PDF网站" scheme="https://ytno1.github.io/tags/PDF%E7%BD%91%E7%AB%99/"/>
    
  </entry>
  
  <entry>
    <title>2019-DSCMN-Dynamic Student Classiffication on Memory Networks for Knowledge Tracing(Sein Minn et al.)</title>
    <link href="https://ytno1.github.io/archives/830526f8.html"/>
    <id>https://ytno1.github.io/archives/830526f8.html</id>
    <published>2021-04-14T09:57:09.000Z</published>
    <updated>2021-08-31T07:14:52.251Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>知识追踪(Knowledge Tracing, KT)是对学生的知识状态进行评估，并根据其学习过程中以前的一些练习和结果来预测该学生是否能正确回答下一个问题。KT 利用机器学习和数据挖掘技术来提供更好的评估、支持性学习反馈和适应性指导。本文提出了一种新的知识追踪模型–记忆网络动态学生分类(Dynamic Student Classification on Memory Networks, DSCMN)，该模型通过捕捉学生长期学习过程中每个时间间隔的时间学习能力来改进现有的 KT 方法。实验结果证实，该模型在预测学生表现方面明显优于目前最先进的 KT 建模技术。</p><span id="more"></span><p>**Keywords: **Massive open online courses, Knowledge tracing, Key-value memory networks, Student clustering, LSTMs</p><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>引导人高效、有效地解决问题是教育研究中反复出现的话题。知识追踪(KT)在这个研究界获得了可信度，在学习过程中提供了适当的和适应性的指导。KT 的目标是评估是否掌握了技能，并利用这些信息来定制学习经验，无论是在 MOOC，导学系统还是在网络上，结果应用的一些例子。例如，当“1+2×3.5=？”这样的问题给定一个学生，她必须掌握加法和乘法的技能才能解决这道题。得到正确答案的概率主要取决于这道题背后这两项技能的掌握程度。掌握一项技能可以通过做有关这项技能的练习来实现。知识追踪的目标是根据学生先前练习的观察结果来追踪他们的知识状态[5]。这项任务也称为学生建模。KT 的研究可以追溯到 20 世纪 70 年代末，人们已经探索了大量的人工智能和知识表示技术[3，14]。在学生在与系统交互时学习的环境中(特别是在 MOOC 等学习环境中)，对学生技能掌握情况进行建模涉及时间维度。例如，涉及相同技能集的序列问题一开始可能会失败，但后来会成功，因为学生的技能掌握程度有所提高。然而，其他因素也会影响成功的结果，比如两个问题的难度、遗忘、猜想和失误，以及一系列其他因素，如果不考虑这些因素，就会导致噪音[11，12]。</p><p>KT 在学习环境中的动态特性导致了能够对时态或顺序数据建模的方法。本文提出了一种新的知识追踪模型–动态学生分类记忆网络(DSCMN)。该模型能够捕捉学生长期记忆中的时间学习能力，同时评估学生对知识状态的掌握程度。时间学习能力是指特定技能的学习速度。这可能与车轮旋转(wheel spinning)之类的现象有关，在这种现象下，学生即使多次尝试也无法学会一项技能[17]。它依靠 RNN 架构来提高性能预测。我们的假设是，学习能力可以随着时间的推移而变化，追踪这一因素可以帮助预测未来的表现。</p><p>本文的其余部分组织如下。第二节回顾了通过数据预测学生表现的学生建模技术的相关工作。第三部分介绍了所提出的 DSCMN 模型。第 4 节提到了使用的实验数据集。第五节描述了实验结果，第六节总结了本文的工作，并讨论了未来的研究方向。</p><h1 id="2-Knowledge-Tracing"><a href="#2-Knowledge-Tracing" class="headerlink" title="2 Knowledge Tracing"></a>2 Knowledge Tracing</h1><p>成功的学习环境如认知导师系列和 ASSISTments 平台，都依赖于某种形式的 KT[6]。在这些系统中，每个问题都标有正确回答该问题所需的基本技能。KT 可以看作是有监督的顺序学习问题的任务，其中模型被给予学生过去与系统的交互，其中包括：技能<img src="https://cdn.nlark.com/yuque/__latex/450404883c139bb3d28c5a1a310e0f5b.svg#card=math&code=S%20%3D%20%5C%7Bs_1%2Cs_2%EF%BC%8C%E2%80%A6%EF%BC%8Cs_t%5C%7D&height=24&width=159">以及响应结果<img src="https://cdn.nlark.com/yuque/__latex/4e7005b4ee3862747d7810e3708d71e7.svg#card=math&code=R%20%3D%20%5C%7Br_1%2Cr_2%EF%BC%8C%E2%80%A6%EF%BC%8Cr_t%5C%7D&height=24&width=160">。KT 预测下一个问题得到正确答案的概率，这主要取决于与问题<img src="https://cdn.nlark.com/yuque/__latex/35a2146bc41b74b24834764fb16db86b.svg#card=math&code=P%3D%5C%7Bp_1%EF%BC%8Cp_2%EF%BC%8C%E2%80%A6%EF%BC%8Cp_t%5C%7D&height=24&width=170">相关的相应技能的掌握。因此，我们可以将得到正确答案的概率定义为<img src="https://cdn.nlark.com/yuque/__latex/1a9dc4bf17f81111f96495746e794334.svg#card=math&code=p%28r_t%3D1%7Cs_t%EF%BC%8CX%29&height=24&width=115">，其中<img src="https://cdn.nlark.com/yuque/__latex/53ce31c0e828fe176f4b6ac346d737c4.svg#card=math&code=X%3D%5C%7Bx_1%EF%BC%8Cx_2%EF%BC%8C...%EF%BC%8Cx_%7Bt%E2%88%921%7D%5C%7D&height=24&width=188">，<img src="https://cdn.nlark.com/yuque/__latex/131c2af5a9df3e7c507a2408a6f45363.svg#card=math&code=x_%7Bt%E2%88%921%7D%3D%28s_%7Bt%E2%88%921%7D%EF%BC%8Cr_%7Bt%E2%88%921%7D%29&height=24&width=140">是一个元组，包含时间 t−1 时对技能 s 的响应结果 r。然后，我们在这里回顾四种最著名的评估学生表现的最先进的 KT 建模方法。</p><h2 id="2-1-Bayesian-Knowledge-Tracing-BKT"><a href="#2-1-Bayesian-Knowledge-Tracing-BKT" class="headerlink" title="2.1  Bayesian Knowledge Tracing (BKT)"></a>2.1  Bayesian Knowledge Tracing (BKT)</h2><p>BKT 可以说是第一个放宽静态知识状态假设的模型。早期的方法，如 IRT，会假设学生不会在答案之间学习，这对于测试是合理的假设，但对于学习环境来说则不是。引入 BKT 是为了在学习环境中追踪知识[5]。在最初的形式中，它还假设每个项目都测试一个技能，但这个假设在之后的工作中被放宽。数据按技能进行划分，在每个数据集上学习一个模型就会得到每个技能的特定模型。标准的 BKT 模型由 4 个参数组成，这些参数通常是在为每个技能建立模型时从数据中学习的。该模型的推断概率主要取决于那些参数，这些参数被用来预测学生如何掌握一项技能，在给定该学生到目前为止对该技能下问题 s 的错误和正确尝试的时间顺序[1]。要估计一名学生在给定其表现历史的情况下掌握技能的概率，BKT 需要有四种概率：P(L0)，掌握技能 L0 的初始概率；P(T)，从非掌握状态到掌握状态的转移概率；P(S)，失误率，尽管掌握但回答错误的概率；P(G)，猜测率，尽管不掌握但回答正确的概率。</p><p><img src="https://cdn.nlark.com/yuque/__latex/7a7c3dc272219306f445788792fc1b1b.svg#card=math&code=%5Cbegin%7Baligned%7D%0AP%5Cleft%28L_%7Bn%7D%20%5Cmid%20%5Ctext%20%7B%20Correct%20%7D%5Cright%29%20%26%3D%5Cfrac%7BP%5Cleft%28L_%7Bn-1%7D%5Cright%29%281-P%28S%29%29%7D%7BP%5Cleft%28L_%7Bn-1%7D%5Cright%29%281-P%28S%29%29%2B%5Cleft%281-P%5Cleft%28L_%7Bn-1%7D%5Cright%29%5Cright%29%20P%28G%29%7D%0A%5Cend%7Baligned%7D%20%5Ctag%20%7B1%7D&height=47&width=724"><br><img src="https://cdn.nlark.com/yuque/__latex/50dc02e314546fb1c9a6e44279cd9b0e.svg#card=math&code=%5Cbegin%7Baligned%7D%0AP%5Cleft%28L_%7Bn%7D%20%5Cmid%20%5Ctext%20%7B%20Incorrect%20%7D%5Cright%29%20%26%3D%5Cfrac%7BP%5Cleft%28L_%7Bn-1%7D%5Cright%29%20P%28S%29%7D%7BP%5Cleft%28L_%7Bn-1%7D%5Cright%29%20P%28S%29%2B%5Cleft%281-P%5Cleft%28L_%7Bn-1%7D%5Cright%29%5Cright%29%281-P%28G%29%29%7D%0A%5Cend%7Baligned%7D%20%5Ctag%20%7B2%7D&height=47&width=724"><br><img src="https://cdn.nlark.com/yuque/__latex/0e0b14f04dddd408e169423f971b5d2d.svg#card=math&code=%5Cbegin%7Baligned%7D%20P%5Cleft%28L_%7Bn%7D%5Cright%29%3DP%5Cleft%28L_%7Bn-1%7D%20%5Cmid%20%5Ctext%20%7B%20Outcome%20%7D%5Cright%29%2B%5Cleft%281-P%5Cleft%28L_%7Bn-1%7D%20%5Cmid%20%5Ctext%20%7B%20Outcome%20%7D%5Cright%29%5Cright%29%20P%28T%29%0A%5Cend%7Baligned%7D%20%5Ctag%20%7B3%7D&height=21&width=724"></p><h2 id="2-2-Deep-Knowledge-Tracing-DKT"><a href="#2-2-Deep-Knowledge-Tracing-DKT" class="headerlink" title="2.2  Deep Knowledge Tracing (DKT)"></a>2.2  Deep Knowledge Tracing (DKT)</h2><p>与 BKT 类似，深度知识跟踪(Deep Knowledge Tracing, DKT)[13]对尝试(做题尝试)的技能序列进行跟踪，但作者利用神经网络的优势，打破了技能分离和二元状态假设的限制。它利用学生以前的尝试历史，并将每次尝试转换为一个 one-hot 编码的特征向量。然后，这些特征作为输入输入到神经网络中，并将信息通过网络的隐藏层传递到输出层。输出层提供学生在系统中正确回答该特定问题的预测概率。</p><p>DKT 使用长短期记忆(Long Short-Term Memory, LSTM)[8]来动态地表示学生的潜在知识空间和练习次数。学生通过作业增加的知识可以通过利用学生以前的表现历史来推断。DKT 在隐含层中将学生所有技能的知识状态归结为一个隐藏状态。学生在特定时间戳的技能掌握状态由以下公式定义：<br><img src="https://cdn.nlark.com/yuque/__latex/2a665b8b789ef73bd5d33934206c9e27.svg#card=math&code=%5Cbegin%7Barray%7D%7Bc%7D%0Ah_%7Bt%7D%3D%5Ctanh%20%5Cleft%28W_%7Bh%20x%7D%20x_%7Bt-1%7D%2BW_%7Bh%20h%7D%20h_%7Bt-1%7D%2Bb_%7Bh%7D%5Cright%29%20%5Ctag%20%7B4%7D%0A%5Cend%7Barray%7D&height=21&width=724"><br><img src="https://cdn.nlark.com/yuque/__latex/fab8f8ed835d31f3dfd50fb7f774d13c.svg#card=math&code=p%5Cleft%28s_%7Bt%7D%5Cright%29%20%5Cin%20y_%7Bt%7D%3D%5Csigma%5Cleft%28W_%7By%20h%7D%20h_%7Bt%7D%2Bb_%7By%7D%5Cright%29%20%5Ctag%20%7B5%7D&height=23&width=724"></p><p>在 DKT 中，tanh 和 sigmoid 函数都是按元素应用的，并由输入权重矩阵 Whx、递归权重矩阵 Whh、初始状态 h0 和读出权重矩阵 Wyh 来参数化。潜在单元和读出单元的偏偏差由 bh 和 by 表示。</p><h2 id="2-3-Dynamic-Key-Value-Memory-Network-DKVMN"><a href="#2-3-Dynamic-Key-Value-Memory-Network-DKVMN" class="headerlink" title="2.3  Dynamic Key-Value Memory Network (DKVMN)"></a>2.3  Dynamic Key-Value Memory Network (DKVMN)</h2><p>DKVMN 是对 DKT 的一种改进，它利用一个称为外部记忆槽(external memory slots)的神经网络模块来编码学生的知识状态，并将其作为 key 和 value 组件对学生的知识状态进行编码[19]。特定技能的学习或遗忘存储在这两个组件中，并通过额外的注意机制——读写操作控制。</p><p>与 DKT 不同，DKVMN 通过执行读写操作来执行局部状态转换，从而避免了隐藏层中的全局和非结构化状态到状态的转换。通过使用从输入技能和 key 记忆槽计算出的相关权重对 value 记忆槽进行读写来跟踪学生的知识状态。它由三个主要步骤组成：</p><p><strong>Correlation：</strong>利用 softmax 激活<img src="https://cdn.nlark.com/yuque/__latex/2d748fa42abdf5d8d84eb3beac40535c.svg#card=math&code=k_t&height=18&width=14">和 key 记忆槽<img src="https://cdn.nlark.com/yuque/__latex/b4425d371477f8c8e1c202ea8941406d.svg#card=math&code=M%5Ek%28i%29&height=23&width=45">之间的内积计算出输入技能<img src="https://cdn.nlark.com/yuque/__latex/86ad9159785a8f6f1c1a74c4eac26365.svg#card=math&code=s_&height=14&width=13">的相关权重：<br><img src="https://cdn.nlark.com/yuque/__latex/4b82ff7d660b197c38fd53178650a666.svg#card=math&code=w_t%3D%20Softmax%28k%5ET_tM%5Ek%28i%29%29%20%5Ctag%20%7B6%7D&height=23&width=724"></p><p>其中 kt 是 st 的连续嵌入向量，且<img src="https://cdn.nlark.com/yuque/__latex/8c299195504b669e9302f1e7d3656e28.svg#card=math&code=%20Softmax%28z_i%29%3De%5E%7Bz_i%7D%2F%20%5Csum_je%5E%7Bz_j%7D&height=42&width=194"> id differentiable。在后面的读写过程中都使用了相关权值。</p><p><strong>Reading：</strong>通过使用 wt 对值记忆槽中的值进行加权和来检索 st 的掌握 mt：<br><img src="https://cdn.nlark.com/yuque/__latex/9fa9937a61c15e3f252ab09e7225e22a.svg#card=math&code=m_t%3D%5Csum%5EN_%7Bi%3D1%7D%28w_t%28i%29M%5Ev_t%28i%29%29%20%20%5Ctag%20%7B7%7D&height=52&width=724"></p><p><strong>Prediction：</strong>通过使用掌握程度 mt 来计算用潜在技能回答对问题的概率 p(st)：<br><img src="https://cdn.nlark.com/yuque/__latex/c11571fef57fcceb8a1372704cee2b86.svg#card=math&code=%5Cbegin%7Barray%7D%7Bc%7D%0Af_%7Bt%7D%3D%5Ctanh%20%5Cleft%28W_%7B1%7D%5E%7BT%7D%5Cleft%5Bm_%7Bt%7D%2C%20k_%7Bt%7D%5Cright%5D%2Bb_%7B1%7D%5Cright%29%20%5Ctag%20%7B8%7D%0A%5Cend%7Barray%7D&height=23&width=724"><br><img src="https://cdn.nlark.com/yuque/__latex/2adb5150db099740bb2ab426410f52c2.svg#card=math&code=%5Cbegin%7Barray%7D%7Bc%7D%0Ap%5Cleft%28s_%7Bt%7D%5Cright%29%3D%5Csigma%5Cleft%28W_%7B2%7D%5E%7BT%7D%20f_%7Bt%7D%2Bb_%7B2%7D%5Cright%29%20%5Ctag%20%7B9%7D%20%0A%5Cend%7Barray%7D%20&height=23&width=724"><br>其中<img src="https://cdn.nlark.com/yuque/__latex/6672629c8ec582fd660332c4e7260bb9.svg#card=math&code=%5Ctanh%20%5Cleft%28z_%7Bi%7D%5Cright%29%3D%5Cleft%28e%5E%7Bz_%7Bi%7D%7D-e%5E%7B-z_%7Bi%7D%7D%5Cright%29%20%2F%5Cleft%28e%5E%7Bz_%7Bi%7D%7D%2Be%5E%7B-z_%7Bi%7D%7D%5Cright%29%20&height=23&width=265">和<img src="https://cdn.nlark.com/yuque/__latex/2c761c7f192daf2865cbd49eb1dcd32a.svg#card=math&code=%5Csigma%5Cleft%28z_%7Bi%7D%5Cright%29%3D1%20%2F%201%2Be%5E%7B-z_%7Bi%7D%7D%20&height=21&width=135">。</p><p><strong>Writing:</strong> 在学生回答问题后，模型会根据学生的回答(rt)更新 value 记忆。将 xt=(st，rt)的联合嵌入转换为 embedding values vt，并将其写入到带有相关权重 wt（与读取过程中所使用的相同）的值存储器中。使用以下公式在添加新信息之前执行擦除：<br><img src="https://cdn.nlark.com/yuque/__latex/d778e3ac11d35c10e1683417c71aa8c8.svg#card=math&code=%5Cbegin%7Barray%7D%7Bc%7D%0Aa_%7Bt%7D%3D%5Ctanh%20%5Cleft%28D%5E%7BT%7D%20v_%7Bt%7D%2Bb_%7Ba%7D%5Cright%29%5E%7BT%7D%20%5Ctag%20%7B10%7D%0A%5Cend%7Barray%7D&height=28&width=724"><br><img src="https://cdn.nlark.com/yuque/__latex/65c3cf9a7a3a881f0d016e232304b566.svg#card=math&code=M_%7Bt%7D%5E%7Bv%7D%28i%29%3DM_%7Bt-1%7D%5E%7Bv%7D%28i%29%2Bw_%7Bt%7D%28i%29%20a_%7Bt%7D%20%5Ctag%20%7B11%7D&height=23&width=724"></p><p>其中 E 和 D 是<img src="https://cdn.nlark.com/yuque/__latex/090db7cac07f1c241e73830c6bb8f12b.svg#card=math&code=d_v%C3%97d_v&height=18&width=53">形状的变换矩阵。这种先擦除后添加的机制允许遗忘和强化学生学习过程的知识状态[19]，这在其他基于 RNN 的模型中是不能做到的。</p><h2 id="2-4-Deep-Knowledge-Tracing-with-Dynamic-Student-Classification-DKT-DSC"><a href="#2-4-Deep-Knowledge-Tracing-with-Dynamic-Student-Classification-DKT-DSC" class="headerlink" title="2.4  Deep Knowledge Tracing with Dynamic Student Classification(DKT-DSC)"></a>2.4  Deep Knowledge Tracing with Dynamic Student Classification(DKT-DSC)</h2><p>尽管上述模型在评估技能掌握程度上比 DKT 有更好的准确性，但在处理 KT 任务时都有不足之处。DKT 和 DKVMN 都忽视了当时学生的长期学习能力。因此，该模型不能评估学生在一个长期的学习过程中，在给定的时间间隔内达到的学习能力水平。在 DKT 和 DKT-DSC 中，LSTM 使用单个状态向量在单个隐含层中编码具有相应学习能力的学生知识状态的时间信息。</p><p>为了对学习能力进行建模，我们结合 DKVMN 和 DKT-DSC 的优点，提出了一种新的基于记忆网络的动态学生分类模型(Dynamic Student Classification on Memory Networks, DSCMN)。DSCMN 在每个时间间隔同时根据评估的当时学生的长期能力和评估的技能掌握情况来预测学生的表现。</p><p><strong>Evaluating Temporal Student’s Learning Ability：</strong>学习是一个涉及练习的过程：学生在练习中变得熟练。此外，学习也受到个人学习能力的影响，或者说通过或多或少的练习来变得熟练[10]。</p><p>为了检测学生在长期学习过程中在一系列时间间隔上的当时学习能力的规律和变化，需要用 DKT-DSC 的公式 17 对学生过去的表现进行编码，以预测其在当前时间间隔内的学习能力。在每个时间间隔之后更新学生过去表现的编码向量。K-Means 算法[9]用于 通过测量在训练 DKT-DSC 过程[10]之后获得的质心之间的欧几里德距离，并分配最近的聚类标签<img src="https://cdn.nlark.com/yuque/__latex/b29572f86288b0c022b9b823dbdc5ea4.svg#card=math&code=c_z&height=14&width=14">作为学生在时间 z 的长期学习能力，来评估学生在每个时间间隔 z 的训练和测试中的当时长期学习能力。评估在学生进行前 20 次尝试之后开始，并且在学生每 20 次尝试之后进行更新。对于第一个时间间隔，每个学生都被分配了初始学习能力 1，如图 1 所示。<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618415172342-1d20cb78-c348-43e4-b51b-e574dfe2f444.png#align=left&display=inline&height=421&margin=%5Bobject%20Object%5D&name=image.png&originHeight=421&originWidth=960&size=145223&status=done&style=none&width=960" alt="image.png"></p><p><strong>Calculating Problem Difficulty：</strong>我们测量问题难度(10 个级别)[11，12]。请注意，在这项研究中，难度与问题有关，而与技能本身无关。问题<img src="https://cdn.nlark.com/yuque/__latex/cc156ebfc496edbc0da7fbe2dea8d339.svg#card=math&code=p_j%E2%88%88D&height=20&width=49">的难度确定为：</p><p><img src="https://cdn.nlark.com/yuque/__latex/4f1f1c29d44a7f12d75fc061ab04abc0.svg#card=math&code=%5Cbegin%7Baligned%7D%0A%26p%20d%5Cleft%28p_%7Bj%7D%5Cright%29%3D%5Cleft%5C%7B%5Cbegin%7Barray%7D%7Bll%7D%0A%5Cdelta%5Cleft%28p_%7Bj%7D%2C%20p%20d%5Cright%29%2C%20%26%20%5Ctext%20%7B%20if%20%7D%5Cleft%7CN_%7Bj%7D%5Cright%7C%20%5Cgeq%204%20%5C%5C%0Ap%20d%2C%20%26%20%5Ctext%20%7B%20else%20%7D%0A%5Cend%7Barray%7D%5Cright.%0A%5Cend%7Baligned%7D%20%5Ctag%20%7B20%7D&height=45&width=724"><br>其中：<br><img src="https://cdn.nlark.com/yuque/__latex/b6d6139c841f31f6c7faed28db5baf16.svg#card=math&code=%5Cbegin%7Baligned%7D%0A%26%5Cdelta%5Cleft%28p_%7Bj%7D%2C%20p%20d%5Cright%29%3D%5Cfrac%7B%5Csum_%7Bi%7D%5E%7B%5Cleft%7CN_%7Bj%7D%5Cright%7C%7D%5Cleft%7C%5Cleft%5C%7Bp_%7Bi%20j%7D%3D%3D0%5Cright%5C%7D%5Cright%7C%7D%7B%5Cleft%7CN_%7Bj%7D%5Cright%7C%7D%20%5Ccdot%20p%20d%0A%5Cend%7Baligned%7D%20%5Ctag%20%7B21%7D&height=57&width=724"><br>其中 Nj 是尝试问题 Pj 的学生集合，而 pij 是学生 i 第一次尝试 pj 问题的结果。结果为 0 表示失败。常量 pd 是我们希望保留的问题难度(级别)。它在函数 δ(pj，pd)中描述，如公式(20)所示。本质上，δ(Pj，Pd)是将问题 Pj 的平均成功率映射到(10)个级别的函数。对于那些没有得到至少 4 个不同学生回答的问题，即数据集中|Nj|&lt;4 的问题，我们对这些问题应用<img src="https://cdn.nlark.com/yuque/__latex/6101d132ae08c79bef11bbaac3fc7be6.svg#card=math&code=pd_t%3D5&height=18&width=54">，对应于 0.5 的难度。</p><h2 id="3-1-Assessing-Student’s-Mastery-of-Skill"><a href="#3-1-Assessing-Student’s-Mastery-of-Skill" class="headerlink" title="3.1  Assessing Student’s Mastery of Skill"></a>3.1  Assessing Student’s Mastery of Skill</h2><p>为了根据当时学习能力来评估技能掌握情况，我们像 DKVMN 一样，在两个 key 和 value 记忆槽中使用读写过程。DSCMN 还使用根据输入技能和 key 记忆计算出的相关权重来评估技能掌握情况。在 DSCMN 中，不使用嵌入值，而是使用公式(6)和(7)将 one-hot 编码的输入直接馈送到记忆网络中。在将 xt 写入到值存储器之前，从读取过程中获得技能 st 的掌握 mt。然后，学生在时间 t 回答问题后，模型使用公式(10)和(12)将 xt 写入值内存(图 2)。<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618418054258-e2e856e7-fa5b-44f6-99ca-729be94dd6f3.png#align=left&display=inline&height=420&margin=%5Bobject%20Object%5D&name=image.png&originHeight=420&originWidth=377&size=25325&status=done&style=none&width=377" alt="image.png"><br>预测：通过将学生在当时学习能力下之前的反应和技能掌握情况输入到额外的隐含层中，来估计在时间间隔 z 和当时学习能力 c 下学生使用学生潜在技能正确回答问题的概率<img src="https://cdn.nlark.com/yuque/__latex/03433ed49293d290fbc290b352340cc5.svg#card=math&code=p%28s_t%29&height=20&width=36">，并进行如下预测：<br><img src="https://cdn.nlark.com/yuque/__latex/51e5444dda250d64c2dee03dcb012bcb.svg#card=math&code=%5Cbegin%7Barray%7D%7Bc%7D%0Ah_%7Bt%7D%3D%5Ctanh%20%5Cleft%28W_%7Bh%7D%5Cleft%5Bx_%7Bt-1%7D%2C%20m_%7Bt%7D%2C%20p%20d_%7Bt%7D%5Cright%5D%2BW_%7Bh%20h%7D%20h_%7Bt-1%7D%2Bb_%7Bh%7D%5Cright%29%20%0A%5Cend%7Barray%7D%20%5Ctag%7B22%7D&height=21&width=724"><br><img src="https://cdn.nlark.com/yuque/__latex/bd302aa43700798bd47da750359af5a0.svg#card=math&code=%5Cbegin%7Barray%7D%7Bc%7D%0Ap%5Cleft%28s_%7Bt%7D%5E%7Bc_%7Bz%7D%7D%5Cright%29%20%5Cin%20y_%7Bt%7D%3D%5Csigma%5Cleft%28W_%7By%20h%7D%20h_%7Bt%7D%2Bb_%7By%7D%5Cright%29%0A%5Cend%7Barray%7D%5Ctag%7B23%7D&height=23&width=724"></p><p>其中 cz 是该学生在时间间隔 t∈z 的当时学习能力，<img src="https://cdn.nlark.com/yuque/__latex/510bb6505619ff66ec0f7364559391e4.svg#card=math&code=%5Bx_%7Bt%E2%88%921%7D%2C%20m_t%EF%BC%8Cpd_t%5D&height=24&width=107">编码了学生 i 在时间间隔 z 和当时学习能力下技能 st-1 的先前反应 xt-1、技能 id st 的掌握情况 mt 和相关问题难度 pdt。DSCMN 具有根据当时长期学习能力来评估技能掌握情况的能力。通过使用这些因素来执行预测，并将其存储在隐藏状态 ht 中。</p><p><strong>Optimization：</strong>为了提高基于 RNN 的模型的预测性能，对于所有基于 RNN 的模型，我们使用 pt 和实际响应 rt 之间的交叉熵损失 l 进行训练，如下所示：<br><img src="https://cdn.nlark.com/yuque/__latex/46cca75a2b12268732f3c84c7d11c332.svg#card=math&code=l%3D%5Csum_%7Bt%7D%5Cleft%28r_%7Bt%7D%20%5Clog%20p_%7Bt%7D%2B%5Cleft%281-r_%7Bt%7D%5Cright%29%20%5Clog%20%5Cleft%281-p_%7Bt%7D%5Cright%29%5Cright%29%20%5Ctag%7B24%7D&height=40&width=724"></p><h1 id="4-Datasets"><a href="#4-Datasets" class="headerlink" title="4 Datasets"></a>4 Datasets</h1><p>为了验证所提出的模型，我们在四个公共数据集上进行了测试，这些数据集来自两个不同的教学场景，其中学生在教育环境中与基于计算机的学习系统交互：(1)ASSISTments：一个最早于 2004 年创建的在线辅导系统，它让初中生和高中生带有脚手架的提示来参与到他们的数学问题中：(1)ASSISTments1：一个在线辅导系统，它最初创建于 2004 年，让初中生和高中生参与到他们的数学问题的脚手架提示中来。如果在 ASSISTments 上学习的学生正确回答了一道题，他们就会得到一道新题。如果他们回答错误，他们会被提供一个小型的辅导课程，在这个课程中，他们必须回答几个问题，将问题分解成几个步骤。数据集如下：ASSISTments 2009-2010(skill builder)、ASSISTments 2012-2013、ASSISTments 2014-2015。(2)Cognitive Tutor。Algebra 2005-2006[4]：是来自 PSLC DataShop 的卡内基学习(Carnegie Learning Of PSLC DataShop)在 KDD Cup 2010 比赛中发布的开发数据集。对于所有数据集，我们的实验只考虑对原始问题的第一次正确尝试。我们删除缺少技能值的数据和重复记录的问题。据我们所知，这些是最大的公开可用的知识跟踪数据集(表 1)。<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618450486330-402a49cc-15ea-4250-b28e-22b15602fb56.png#align=left&display=inline&height=215&margin=%5Bobject%20Object%5D&name=image.png&originHeight=215&originWidth=631&size=35610&status=done&style=none&width=631" alt="image.png"><br>**<em><a href="https://sites.google.com/site/assistmentsdata/">https://sites.google.com/site/assistmentsdata/</a>.</em><br>_**<a href="https://pslcdatashop.web.cmu.edu/KDDCup/downloads.jsp./">https://pslcdatashop.web.cmu.edu/KDDCup/downloads.jsp.\</a>_</p><h1 id="5-Experimental-Study"><a href="#5-Experimental-Study" class="headerlink" title="5 Experimental Study"></a>5 Experimental Study</h1><p>在这个实验中，我们假设学生每做 20 次尝试就是一个时间间隔。DKT-DSC 和 DSCMN 在实验中使用的学生学习能力时间值总数为 8 个(7 个簇，1 个为所有学生评估前在初始时间间隔内的初始能力的时间值)。在所有数据集上使用五折交叉验证进行预测。每个 fold 涉及将每个数据集随机分为 80%的训练学生数据和 20%的测试学生数据。对于 DKVMN 的输入，在训练过程中学习到 key 记忆和 value 记忆中的初始值。对于其他模型，应用 one-hot 编码。值记忆中的初始值将初始知识状态表示为每个技能的先验难度，并在测试过程中固定。</p><p>我们使用 TensorFlow 实现了所有模型，DKT，DKT-DSC 和 DSCMN 具有相同的全连接隐藏节点结构，即 DKT 和 DKT-DSC 的 LSTM 隐藏层大小以及 DSCMN 记忆网络的输出大小都为 200。为了加快训练过程，采用小批量随机梯度下降法最小化损失函数。我们的实施的批处理大小为 32，对应于 32 个来拆分每个学生的序列。我们用 0.01 的学习率训练模型，并采用了 dropout 来避免过拟合[16]。我们把 epochs 数设为 100。所有模型都在相同的训练和测试学生集上进行训练和测试。</p><p>对于 BKT，我们使用期望最大化(EM)算法，并将迭代次数限制为 200 次。我们学习每种技能的模型，并分别做出预测。每项技能的结果取平均值。<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618451882679-aa399a38-cafe-4ed0-9101-46062d48a8c0.png#align=left&display=inline&height=233&margin=%5Bobject%20Object%5D&name=image.png&originHeight=233&originWidth=707&size=48208&status=done&style=none&width=707" alt="image.png"></p><p>在表 2 中，DSCMN 在三个数据集中的性能明显优于最先进的模型。在认知导师数据集上，与与标准 DKT 的最大测试 AUC 为 78.4，DKT-DSC 的最大测试 AUC79.2，DKVMN 的最大测验 AUC 仅为 78.0 相比。DSCMN 模型的 AUC=86.0，比原始 DKT 和 DKVMN 模型显著提高 10%，比 DKT-DSC 模型提高 8%。对于 ASSISTments09 数据集，在 AUC=81.2 的情况下，DSCMN 也获得了约 10%的增益，高于 DKT-DSC(AUC=78.5)，远高于原始 DKT(AUC=71.3)和 DKVMN(AUC=70.7)时。在 ASSISTments12 数据集上，DSCMN 仅获得 AUC=0.71【数据有错误】。在最新的 ASSISTments14 数据集(与其他三个数据集相比，ASSISTments14 数据集包含的学生更多，数据更少并且缺乏问题信息)中，DSCNM 的 AUC 略低于 DKT-DSC。</p><p>在表 3 中，当我们比较上述模型的 RMSE 时，BKT 在 ASSISTments09 中最低为 0.46，ASSISTments12 和 ASSISTments14 为 0.51，Cognitive Tutor 为 0.44。DSCMN 在所有数据集中的 RMSE 结果最低为 0.40，而所有其他模型的 RMSE 结果都不超过 0.43(认知导师数据集中的 DKT 和 ASSISTments14 中的 DSCMN 除外)。根据这些结果，DSCMN 在 Cognitive Tutor、ASSISTments09、ASSISTments 12 上的表现优于 DKT-DSC，且显著优于其他模型，但在 ASSISTts 14 上略低于 DKT-DSC。<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618453578001-05ccb7cf-0d8d-4e5c-b115-a0879773e582.png#align=left&display=inline&height=210&margin=%5Bobject%20Object%5D&name=image.png&originHeight=210&originWidth=658&size=40173&status=done&style=none&width=658" alt="image.png"></p><h1 id="6-Conclusion-and-Future-Work"><a href="#6-Conclusion-and-Future-Work" class="headerlink" title="6 Conclusion and Future Work"></a>6 Conclusion and Future Work</h1><p>本文提出了一个新的模型 DSCMN，它通过收集技能、问题和学生的信息来预测学生的表现：学生在每个时间步长中对各种问题的技能掌握程度，以及每个时间间隔的学生学习能力。</p><p>在四个数据集上的实验表明，该模型比现有最先进的 KT 模型具有更好的预测性能。对学生在每个时间间隔的当时学习能力的动态评估起着至关重要的作用，它有助于 DSCMN 捕捉到数据中的更多差异，从而导致更准确的预测。</p><p>在我们未来的工作中，我们计划将这个模型应用于与多种技能相关的问题，并将其应用于相关问题的推荐。</p><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><p>……</p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;Abstract&quot;&gt;&lt;a href=&quot;#Abstract&quot; class=&quot;headerlink&quot; title=&quot;Abstract&quot;&gt;&lt;/a&gt;Abstract&lt;/h1&gt;&lt;p&gt;知识追踪(Knowledge Tracing, KT)是对学生的知识状态进行评估，并根据其学习过程中以前的一些练习和结果来预测该学生是否能正确回答下一个问题。KT 利用机器学习和数据挖掘技术来提供更好的评估、支持性学习反馈和适应性指导。本文提出了一种新的知识追踪模型–记忆网络动态学生分类(Dynamic Student Classification on Memory Networks, DSCMN)，该模型通过捕捉学生长期学习过程中每个时间间隔的时间学习能力来改进现有的 KT 方法。实验结果证实，该模型在预测学生表现方面明显优于目前最先进的 KT 建模技术。&lt;/p&gt;</summary>
    
    
    
    <category term="知识追踪" scheme="https://ytno1.github.io/categories/%E7%9F%A5%E8%AF%86%E8%BF%BD%E8%B8%AA/"/>
    
    <category term="DSCMN" scheme="https://ytno1.github.io/categories/%E7%9F%A5%E8%AF%86%E8%BF%BD%E8%B8%AA/DSCMN/"/>
    
    
    <category term="知识追踪" scheme="https://ytno1.github.io/tags/%E7%9F%A5%E8%AF%86%E8%BF%BD%E8%B8%AA/"/>
    
    <category term="DSCMN" scheme="https://ytno1.github.io/tags/DSCMN/"/>
    
  </entry>
  
  <entry>
    <title>2020-AKT-Context-Aware Attentive Knowledge Tracing(Aritra Ghosh et al.)</title>
    <link href="https://ytno1.github.io/archives/80333e1a.html"/>
    <id>https://ytno1.github.io/archives/80333e1a.html</id>
    <published>2021-04-12T16:36:19.000Z</published>
    <updated>2021-08-31T07:14:52.316Z</updated>
    
    <content type="html"><![CDATA[<h1 id="上下文感知的注意力集中的知识追踪"><a href="#上下文感知的注意力集中的知识追踪" class="headerlink" title="上下文感知的注意力集中的知识追踪"></a>上下文感知的注意力集中的知识追踪</h1><h1 id="ABSTRACT-摘要"><a href="#ABSTRACT-摘要" class="headerlink" title="ABSTRACT 摘要"></a>ABSTRACT 摘要</h1><p>知识追踪(KT)指的是根据学习者过去在教育应用中的表现来预测其未来表现的问题。使用灵活的深度神经网络模型的 KT 的最新发展擅长于这一任务。然而，这些模式的可解释性往往有限，因此不足以满足个性化学习的需要。个性化学习需要使用可解释的反馈和可操作的建议来帮助学习者获得更好的学习结果。在本文中，我们提出了注意力知识追踪(AKT)，它将灵活的基于注意力的神经网络模型与一系列受认知和心理测量模型启发的新颖的、可解释的模型组件相结合。AKT 使用了一种新的单调注意机制，将学习者未来对评估问题的反应与他们过去的反应联系起来；除了问题之间的相似性外，还使用指数衰减和上下文感知的相对距离度量来计算注意力权重。此外，我们使用 Rasch 模型来规则化概念和问题嵌入，这些嵌入能够在不使用过多参数的情况下捕捉同一概念上问题之间的个体差异。我们在几个真实的基准数据集上进行了实验，结果表明，AKT 在预测未来学习者的反应方面优于现有的 KT 方法(在某些情况下 AUC 高达 6%)。我们还进行了几个案例研究，表明 AKT 表现出极好的可解释性，因此在现实世界的教育环境中具有自动反馈和个性化的潜力。</p><span id="more"></span><h1 id="1-INTRODUCTION-介绍"><a href="#1-INTRODUCTION-介绍" class="headerlink" title="1 INTRODUCTION 介绍"></a>1 INTRODUCTION 介绍</h1><p>数据分析和智能导学系统[32]的最新进展使大规模学习者数据的收集和分析成为可能；这些进步暗示了大规模个性化学习的潜力，方法是通过分析每个学习者的学习历史数据，自动向每个学习者提供个性化反馈[24]和学习活动建议[11]。<br>学习者数据分析中的一个关键问题是根据学习者过去的表现来预测他们未来的表现(他们对评估问题的反应)，这被称为知识追踪(KT)问题[3]。在过去的 30 年里，基于两个共同的假设发展了许多解决 KT 问题的方法：i)学习者过去的表现可以用一组变量来概括，这些变量代表了他们在一组概念/技能/知识组件上的当前潜在知识水平；ii)学习者的未来表现可以用他们当前的潜在概念知识水平来预测。具体地说，让 t 表示一组离散的时间指数，我们有以下关于学习者知识和表现的通用模型。<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618245513339-6a1903e4-09d4-49b1-9a7a-b402ee59d178.png#height=28&id=hx4KF&margin=%5Bobject%20Object%5D&name=image.png&originHeight=28&originWidth=162&originalType=binary%E2%88%B6=1&size=1763&status=done&style=none&width=162" alt="image.png"><br>       其中 rt∈{0,1}表示学习者在时间步长 t 上对评估问题的评分响应，通常是二进制值（1 对应正确的答案，0 对应不正确的答案）并得到观察。潜在变量 ht 表示学习者当前的知识水平，不会被观察到。 f（·）和 g（·）是表征学习者知识如何决定其反应以及其发展方式的函数；它们有时分别称为响应模型和知识演化模型。<br>       2010 年前 KT 方法的早期发展可以分为两类。第一类以贝叶斯知识追踪(BKT)方法[19，35]为中心，其中知识(ht)是表征学习者是否掌握问题所涵盖的(单个)概念的二进制标量。由于响应(rt)也是二进制值的，响应和知识演化模型简单地是噪声的二进制通道，由猜测、滑动、学习和遗忘概率来参数化。第二类以项目反应理论(IRT)模型[16]为中心，使用这些模型(特别是 S 型连接函数)作为反应模型 f(·)；然后将学习者的知识水平建模为涵盖多个概念的问题的实值向量(ht)。在这些方法中，SPARFA-Trace 方法[13]使用一个简单的仿射变换模型作为显式知识演化模型 g(·)。其他方法，例如，加法因素模型[1]、性能因素分析[22]、难度、能力和学生历史(DASH)模型[15]，以及包括知识分解机器[30]和 DAS3H 模型的扩展的一些最近的方法，使用手工制作的特征，例如在其知识进化模型中的每个概念上的先前尝试、成功和失败的次数。这两个类中的方法都依靠专家标签将问题与概念关联起来，由于它们可以有效地估计每个学习者对专家定义的概念的知识水平，因此具有极好的可解释性。<br>       KT 的最新发展集中在使用更复杂和更灵活的模型来充分利用大规模学习者反应数据集中包含的信息。深度知识追踪(DKT)方法[23]是通过使用长期短期记忆网络[7]作为知识进化模型 g(·)来探索(可能是深度)神经网络用于 KT 的第一种方法。由于 LSTM 单元是非线性的、复杂的函数，它们比仿射变换更灵活，更能捕捉真实数据中的细微差别。<br>       动态键值记忆网络(dynamic key-value memory networks, DKVMN)方法扩展了 DKT，利用外部记忆矩阵(external memory matrix, Ht)来表征学习者知识[36]。这个矩阵被分为两部分:一个静态的“键”矩阵，它包含每个概念的固定表示;一个动态的“值”矩阵，它包含每个学习者对每个概念的不断发展的知识水平。DKVMN 还在响应和知识演化模型的外部矩阵上使用单独的“读”和“写”过程;这些过程使它比 DKT 更加灵活。DKT 和 DKVMN 在预测未来学习者表现[9]上拥有最先进的性能，并已成为新的 KT 方法的基准。<br>       自我注意知识追踪(self-attentive knowledge tracing , SAKT)方法[18]是第一个在 KT 上下文（情境）中使用注意机制的方法。注意机制比循环和基于记忆的神经网络更灵活，在自然语言处理任务中表现出更好的性能。SAKT 的基本设置与变压器(Transformer)模型[29]有许多相似之处，后者是许多序列到序列(sequence-to-sequence)预测任务的有效模型。然而，我们观察到，在我们的实验中，SAKT 的性能并不优于 DKT 和 DKVMN；有关详细信息，请参阅第 4 节。这可能的原因包括：i)不像在语言任务中，单词之间强烈的长距离依赖更为普遍，未来学习者表现对过去的依赖可能被限制在更短的窗口内，以及 ii)学习者响应数据集的大小比自然语言数据集低几个数量级，并且不太可能从高度灵活和大规模的注意模型中受益。<br>       (研究问题)更重要的是，没有一种现有的 KT 方法能够真正在未来性能预测和可解释性两方面出类拔萃。早期的 KT 方法表现出很好的可解释性，但对未来学习者的成绩预测没有提供最先进的性能。最近的基于深度学习的知识理论方法在这方面表现出色，但提供的解释力有限。因此，这些 KT 方法并不能完全满足个性化学习的需要，个性化学习不仅需要准确的成绩预测，还需要能够提供自动化的、可解释的反馈和可操作的建议，以帮助学习者获得更好的学习结果。<a href="#_msocom_1">[喻清尘1]</a></p><h2 id="1-1-Contributions-贡献"><a href="#1-1-Contributions-贡献" class="headerlink" title="1.1  Contributions 贡献"></a>1.1  Contributions 贡献</h2><p>对于预测学习者对当前问题的反应的任务，我们提出了注意力知识追踪(AKT)方法，它使用一系列的注意力网络来将这个问题与学习者过去回答的每一个问题联系起来。我们将我们的主要创新总结如下：(本文贡献)<br>(1)与现有的使用原始问题和答案嵌入的注意方法相反，我们将原始嵌入放在上下文中，并通过考虑学习者的整个练习历史来使用针对于过去问题和答案的上下文感知表示。<br>(2)受认知科学关于遗忘机制研究的启发，我们提出了一种新的单调注意机制，该机制使用指数衰减曲线来降低问题在遥远过去的重要性。我们还开发了一种上下文感知措施来表征学习者过去回答过的问题之间的时间距离。<br>(3)利用 Rasch 模型这一简单且可解释的 IRT 模型，在不引入过多模型参数的情况下，使用一系列基于 Rasch 模型的嵌入来捕捉问题之间的个体差异。<br>我们在几个真实世界的基准教育数据集上进行了一系列实验，比较了 AKT 和最新的 KT 方法。我们的结果表明，AKT 在预测未来学习者的表现方面(有时非常显著)优于其他 KT 方法。此外，我们对 AKT 模型的每个关键组件进行了消融研究(ablation studies)，以证明它们的价值。我们还进行了几个案例研究，以表明 AKT 表现出极好的可解释性，并具有自动反馈和练习问题推荐的潜力，这两个都是个性化学习的关键要求。</p><h1 id="2-KNOWLEDGE-TRACING-PROBLEM-SETUP-知识跟踪问题设置"><a href="#2-KNOWLEDGE-TRACING-PROBLEM-SETUP-知识跟踪问题设置" class="headerlink" title="2 KNOWLEDGE TRACING PROBLEM SETUP 知识跟踪问题设置"></a>2 KNOWLEDGE TRACING PROBLEM SETUP 知识跟踪问题设置</h1><p>每个学习者的成绩记录由每个离散时间步的一系列问题和回答组成。对于时间步长 t 的学习者 i，我们将他们回答的问题、这个问题涵盖的概念以及他们的评分答案表示为一个元组<img src="https://cdn.nlark.com/yuque/__latex/8130b895cd0c04ce9451599151d88fe8.svg#card=math&code=%28q_t%5Ei%EF%BC%8Cc_t%5Ei%EF%BC%8Cr_t%5Ei%29&height=24&id=F2A7o">，其中<img src="https://cdn.nlark.com/yuque/__latex/906d682406e8a706acc0a9853ec6f45f.svg#card=math&code=q_t%5Ei%E2%88%88N%5E%2B&height=23&id=tHfBH">是问题索引，<img src="https://cdn.nlark.com/yuque/__latex/dfe46248827c8629b7501e680e083d3a.svg#card=math&code=c_t%5Ei%E2%88%88N%5E%2B&height=23&id=eUIX3">是概念索引，<img src="https://cdn.nlark.com/yuque/__latex/3119467097e5580585b2325ba84e5c6a.svg#card=math&code=r_t%5Ei%E2%88%88%20%5C%7B0%EF%BC%8C1%20%5C%7D&height=24&id=GvyMa">是答案。在这种记号下，<img src="https://cdn.nlark.com/yuque/__latex/067cf5704aa483f7c396ae660d761c98.svg#card=math&code=%28q_t%5Ei%EF%BC%8Cc_t%5Ei%EF%BC%8C1%29&height=24&id=qP9C1">表示学习者 i 在时间 t 对基于概念<img src="https://cdn.nlark.com/yuque/__latex/e1a0940d9146645d7dce33c6ae1d43ba.svg#card=math&code=c_t%5Ei&height=23&id=PL1vI">的问题<img src="https://cdn.nlark.com/yuque/__latex/cc6349dc26819c8790b47b6998ff2611.svg#card=math&code=q_t%5Ei&height=23&id=E9BJ6">做出了正确的回答。我们注意到，这种设置与以往的一些深度知识追踪工作不同，这些工作往往忽略问题索引，将学习者的表现概括为<img src="https://cdn.nlark.com/yuque/__latex/5eae48953a2fcfcc31e937788a45997c.svg#card=math&code=%28c_t%5Ei%EF%BC%8Cr_t%5Ei%29&height=24&id=mS9gA">。此选择是为了避免过度参数化；有关详细分析，请参见第 3.3 节。在下面的讨论中，当我们讨论如何预测单个学习者的未来表现时，我们省略了上标 i。给定他们到时间 t−1 的过去历史为{(q1，c1，r1)，.…，(qt−1，ct−1，rt−1)}，我们的目标是预测他们在当前时间步长 t 对概念 ct 上的问题 qt 的反应 rt。</p><h2 id="2-1-Question-and-Response-Embeddings-问题和回答嵌入"><a href="#2-1-Question-and-Response-Embeddings-问题和回答嵌入" class="headerlink" title="2.1 Question and Response Embeddings 问题和回答嵌入"></a>2.1 Question and Response Embeddings 问题和回答嵌入</h2><p>在前人工作的基础上，我们使用实值嵌入向量 xt∈RD 和 yt∈RD 分别表示每个问题和每个问答对(qt，rt)。xt 表示关于问题的信息，yt 表示学习者通过回答问题获得的知识，正确和错误的回答嵌入为两个单独的嵌入向量。D 表示这些嵌入的维度。因此，用 Q 表示问题的数量，总共有 Q 个问题嵌入向量和 2Q 个问题-回答嵌入向量。在大多数现实世界的教育设置中，题库比概念集大得多，许多问题被分配给很少的学习者。因此，现有的 KT 方法大多使用概念来索引问题，以避免过度参数化；覆盖同一概念的所有问题都被视为单个问题。在这种情况下，qt=ct，Q=C。</p><h1 id="3-THE-AKT-METHOD-AKT-方法"><a href="#3-THE-AKT-METHOD-AKT-方法" class="headerlink" title="3 THE AKT METHOD AKT 方法"></a>3 THE AKT METHOD AKT 方法</h1><p>AKT 方法由四个组件组成：两个自注意编码器，一个用于问题，一个用于知识获取，一个基于注意力的知识检索器，以及一个前馈响应预测模型；图 1 显示了 AKT 方法及其相关组件。<br>我们使用两个自我注意的编码器来学习问题和回答的上下文感知表示。我们将第一个编码器称为问题编码器，它根据学习者之前练习过的问题序列，生成每个问题的修改后的上下文表示形式。类似地，我们将第二个编码器称为知识编码器，它产生学习者在回答过去的问题时所获知识的修改的、上下文的表示。或者，我们可以使用与以前的工作类似的问题和回答的原始嵌入。我们发现，上下文感知表示在大多数数据集中表现得更好。我们将知识演化模型称为知识检索器，它使用注意力机制检索过去获得的与当前问题相关的知识。最后，响应预测模型使用检索到的知识预测学习者对当前问题的响应。AKT 方法是由三种根植于认知科学和心理测量学的直觉驱动的；我们将在下面详细介绍这些直觉。<a href="#_msocom_2">[喻清尘2]</a></p><h2 id="3-1-Context-aware-Representations-and-The-Knowledge-Retriever-上下文感知表示和知识检索器"><a href="#3-1-Context-aware-Representations-and-The-Knowledge-Retriever-上下文感知表示和知识检索器" class="headerlink" title="3.1 Context-aware Representations and The Knowledge Retriever 上下文感知表示和知识检索器"></a>3.1 Context-aware Representations and The Knowledge Retriever 上下文感知表示和知识检索器</h2><p>如上所述，我们在模型中使用了两个编码器。问题编码器采用原始问题嵌入{x1，……，xt}作为输入，并输出使用单调注意机制(在下一小节中详细描述)的上下文感知问题嵌入序列{ˆx1，……ˆxt}。每个问题的上下文感知嵌入既取决于其本身，也取决于过去的问题，即       ˆxt=fenc1(x1，……，xt)。类似地，知识编码器采用原始问题-答案嵌入{y1，……，yt−1}作为输入，并输出使用相同的单调注意机制所获取的实际知识序列{ˆy1，……，ˆyt−1}。所获知识的上下文感知嵌入取决于学习者对当前问题和过去问题的回答，即 ˆyt−1=fenc2(y1，……，yt−1)。<br>（三大直觉）选择使用上下文感知嵌入而不是原始嵌入反映了我们的第一个直觉：学习者在回答问题时理解和学习的方式取决于学习者。这些修改后的表征反映了每个学习者对问题的实际理解和他们实际获得的知识，给出了他们的个人反应历史。这种模式的选择是基于这样一种直觉，即对于两个过去回答顺序不同的学习者来说，他们理解同一问题的方式以及他们从练习中获得的知识可能会有所不同。<br>知识检索器将上下文感知问题和问题-回答对嵌入 ˆx1：t 和 ˆy1：t−1 作为输入，并输出当前问题的检索到的知识状态 ht。我们注意到，在 AKT 中，学习者的当前知识状态也是上下文感知的，因为它取决于他们正在回答的当前问题；这种模型选择与包括 DKT 在内的大多数现有方法不同。我们还注意到，知识检索器只能使用关于过去问题的信息、学习者对这些问题的反应以及当前问题的表示，而不能使用学习者对当前问题的反应，即 ht=fkr(ˆx1，……，ˆxt，      ˆy1，……，ˆyt−1)。响应预测模型使用检索到的知识来预测当前响应。</p><h2 id="3-2-The-Monotonic-Attention-Mechanism-单调注意机制"><a href="#3-2-The-Monotonic-Attention-Mechanism-单调注意机制" class="headerlink" title="3.2 The Monotonic Attention Mechanism 单调注意机制"></a>3.2 The Monotonic Attention Mechanism 单调注意机制</h2><p>对于编码器和知识检索器，我们使用一种改进的、单调版本的缩放点积注意机制(the scaled dot-product attention mechanism)。我们首先简要总结一下最初的缩放点积注意机制。在此框架下，每个编码器和知识检索器都有一个键、查询和值嵌入层，分别将输入映射到维度 Dq=Dk、Dk 和 Dv 的输出查询、键和值。设<img src="https://cdn.nlark.com/yuque/__latex/c319db38d76c50ae32b4076f179fc2d5.svg#card=math&code=qt%E2%88%88R%5E%7BDk%C3%971%7D&height=21&id=fGeEt">表示对应于学习者在时间 t 回答的问题的查询，使用 Softmax 函数[5]计算缩放的点积关注值<img src="https://cdn.nlark.com/yuque/__latex/f2cff72f44e635b4a2768db02c81ad5a.svg#card=math&code=%CE%B1_%7Bt%2C%CF%84%7D&height=16&id=Df2qs">：（一般注意机制相关公式）<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618245979161-f9f5ae77-484a-4aac-b4ed-dd964b986e03.png#height=59&id=qoREG&margin=%5Bobject%20Object%5D&name=image.png&originHeight=59&originWidth=295&originalType=binary%E2%88%B6=1&size=5381&status=done&style=none&width=295" alt="image.png"><br>       然后，由<img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618245986422-16734589-e485-4a71-b31f-20b593e68085.png#height=17&id=kEGor&margin=%5Bobject%20Object%5D&name=image.png&originHeight=17&originWidth=113&originalType=binary%E2%88%B6=1&size=1530&status=done&style=none&width=113" alt="image.png">出缩放的点积注意机制的输出。<img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618245996018-20d5a811-41ce-48b9-a60a-f6e5b24374b2.png#height=20&id=pSedu&margin=%5Bobject%20Object%5D&name=image.png&originHeight=20&originWidth=158&originalType=binary%E2%88%B6=1&size=2058&status=done&style=none&width=158" alt="image.png">分别表示在时间步长 τ 的问题的键和值。根据特定组件的不同，输出要么取决于过去和当前(问题和知识编码器的 τ≤t)，要么仅取决于过去(知识检索器的 τ&lt;t)。<br>       两个编码器都使用自我注意机制，即使用相同的输入计算 qt、kt 和 vt；问题编码器使用{x1，……，xt}，而知识编码器使用{y1，……，yt−1}。另一方面，知识检索器不使用自我注意。如图 1 所示，在时间步，它使用 ˆxt(当前问题的修改嵌入)，{ˆx1，……，ˆxt−1}(过去问题的上下文感知嵌入)和{ˆy1，……，ˆyt−1}(过去问题-回答对的上下文感知嵌入)作为输入，以分别生成查询、键和值。我们注意到，SAKT 使用问题嵌入来映射查询，而响应嵌入来映射键和值。在我们的实验中，我们发现使用问题嵌入来映射查询和键要有效得多。<br>       然而，对于 KT 来说，这种基本的缩放点积注意机制可能是不够的。原因是学习是短暂的，记忆力会衰退[21]；当我们预测学习者对当前问题的反应时，他们在遥远过去的表现并不像最近的表现那样能提供信息。因此，我们开发了一种新的单调注意机制，它反映了我们的第二直觉：当学习者面临一个新的问题时，i)关于无关概念和 ii)来自太久以前的过去经验不太可能是高度相关的。具体地说，我们将乘法指数衰减项添加到注意力得分中，如下所示：<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618246011563-113971e6-5658-47ab-bf32-6c2f0afa3610.png#height=43&id=T9Qk6&margin=%5Bobject%20Object%5D&name=image.png&originHeight=43&originWidth=132&originalType=binary%E2%88%B6=1&size=2371&status=done&style=none&width=132" alt="image.png"><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618246014822-4522e0e1-2100-42a6-b304-adecc0eba57f.png#height=46&id=SF66z&margin=%5Bobject%20Object%5D&name=image.png&originHeight=46&originWidth=182&originalType=binary%E2%88%B6=1&size=2709&status=done&style=none&width=182" alt="image.png"><br>       其中 θ&gt;0 是可学习衰减率参数，d(t，τ)是时间步长 t 和 τ 之间的时间距离度量。换言之，当前问题对过去问题的关注度不仅取决于对应的查询和键之间的相似度，还取决于它们之间的相对时间步数。总而言之，我们的单调注意机制采取的是随着时间推移呈指数衰减曲线的基本形式，当过去的问题与现在的问题高度相似时，可能会在时间步长上出现峰值。我们注意到，我们对注意力权重应用指数衰减，而不是潜在知识，这是现有学习者模型中常用的方法(参见[17，26])。<br>       我们注意到，还有许多其他可能的方法来表征注意力的时间动态。首先，在注意力网络擅长的语言任务中，可以使用加性位置嵌入(additive positional embeddings)或可学习嵌入对时间动态进行建模[29]。其次，在我们的单调注意机制中，我们还可以将指数衰减参数化为<img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618246024882-187961be-821b-427a-bdb7-d304e60aa4ba.png#height=32&id=jARHe&margin=%5Bobject%20Object%5D&name=image.png&originHeight=32&originWidth=134&originalType=binary%E2%88%B6=1&size=2111&status=done&style=none&width=134" alt="image.png">。然而，这两个变化都不会带来与我们选择的模型设置相当的性能；在我们的实验中，我们将使用位置编码(而不是单调注意)来将 AKT 与其变体进行比较。<br>       情境感知的距离测量(A context-aware distance measure)。指数衰减函数决定了随着当前时间指数与之前时间指数之间的距离增加，注意权重衰减的速率。定义两个时间指标之间的距离的一种直接方法是它们的绝对值差，即 d(t,τ)= |t−τ|。然而，这种距离是不受上下文影响的，并且忽略了每个学习者的练习历史。例如，考虑以下两个序列的概念，学习者练习:维恩图(VD)1,VD2，…，VD8，素数(PN)9,PN10 和 PN1,VD2,VD3,… ,VD9,PN10, 其中符号“V D2”表示学习者在时间步长 2 时练习了维恩图的概念。在这个例子中，t = 10（即当前时间索引）时，学习者在这两个序列中都回答了一个质数的问题，在这个例子中，学习者在这两个序列中回答了关于 t=10 的质数(即当前时间索引)的问题，但是最近关于质数的过去练习来自不同的时间索引。由于维恩图和素数的概念关系不大，因此在预测当前习题的答案时，学习者以前关于素数的练习比最近关于维恩图的练习更适合我们。在这种情况下，使用直接的绝对值差异，指数衰减曲线将显著降低分配给 t=1 的素数练习的注意力权重。<br>       因此，对于指数衰减机制(在编码器中)，我们提出以下上下文感知的时间步长 d(t,τ)与 τ≤t 之间的距离度量：<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618246034182-1067fd47-f908-49f3-9df5-542cbdc3367e.png#height=110&id=HiY3y&margin=%5Bobject%20Object%5D&name=image.png&originHeight=110&originWidth=231&originalType=binary%E2%88%B6=1&size=6227&status=done&style=none&width=231" alt="image.png"><br>       对于知识检索器，我们将 τ＇≤t 替换为 τ&lt;t，将 t’≤t 替换为 t＇&lt;t。换言之，该上下文感知距离度量使用另一个 softmax 函数来根据过去练习的概念与当前概念的关系来调整连续时间索引之间的距离。实际上，在模型训练过程中的每一次迭代中，我们都使用当前的 AKT 模型参数来计算并固定修正后的距离度量，而不是通过距离度量传递梯度。<br>       多头注意和子层(Multi-head attention and sub-layers)。我们还结合了多头注意力，多头关注和子层。我们还加入了多头注意，这对在多个时间尺度[29]上注意过去的位置是有效的。因此，我们使用 H 个独立注意头，每个头都有自己的衰减率 θ，将最终输出连接成(Dv·H)×1 向量，并将其传递给下一层。这种模型设计使 AKT 能够在多个时间尺度上总结学习者过去的表现，这与多尺度上下文、DASH 和 DAS3H 模型中的多个时间窗口有一些相似之处[2，15，21]。我们还在每个编码器和知识检索器中使用几个子层，包括一个用于层归一化[14]，一个用于丢弃[27]，一个完全连接的前馈层，以及一个剩余连接层[6]。</p><h2 id="3-3-Response-Prediction-响应预测"><a href="#3-3-Response-Prediction-响应预测" class="headerlink" title="3.3 Response Prediction 响应预测"></a>3.3 Response Prediction 响应预测</h2><p>AKT 方法的最后一个组成部分是预测学习者对当前问题的反应。预测模型的输入是检索到的知识(知识检索器输出 ht)和嵌入当前问题 xt 的连接向量；该输入在最终通过 Sigmoid 函数[5]之前通过另一个完全连接的网络，以生成学习者正确回答当前问题的预测概率 ˆrt∈[0，1]。通过最小化所有学习者响应的二进制交叉熵损失，以端到端方式训练整个 AKT 方法中的所有可学习参数，即<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618246046765-d26fb85a-efaf-4b19-9459-893089747a9d.png#height=37&id=XJ75m&margin=%5Bobject%20Object%5D&name=image.png&originHeight=37&originWidth=260&originalType=binary%E2%88%B6=1&size=2797&status=done&style=none&width=260" alt="image.png"><br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618246052769-4db1bbb4-7c24-4402-a3ec-6f39eb67f28e.png#height=336&id=IfSQg&margin=%5Bobject%20Object%5D&name=image.png&originHeight=336&originWidth=643&originalType=binary%E2%88%B6=1&size=72509&status=done&style=none&width=643" alt="image.png"><br>图 1：AKT 方法概述。我们使用基于 Rasch 模型的嵌入作为问题和回答的原始嵌入。问题和知识编码器计算问题和回答对的上下文感知表示。知识检索器使用这些表示作为输入，并计算学习者的知识状态。为简单起见，我们没有展示编码器中的单调注意机制。我们也不显示子层。</p><h2 id="3-4-Rasch-Model-Based-Embeddings-基于-Rasch-模型的嵌入"><a href="#3-4-Rasch-Model-Based-Embeddings-基于-Rasch-模型的嵌入" class="headerlink" title="3.4 Rasch Model-Based Embeddings 基于 Rasch 模型的嵌入"></a>3.4 Rasch Model-Based Embeddings 基于 Rasch 模型的嵌入</h2><p>如前所述，现有的 KT 方法使用概念来索引问题，即设置 qt= ct。由于数据稀少，这种设置是必要的。设 Q 为问题总数，L 为学习者人数。在大多数真实世界的学习者回答数据集中，学习者回答的数量与 CL 相当，但比 QL 少得多，因为许多问题分配给少数学习者。因此，使用概念对问题进行索引可以有效地避免过度参数化和过拟合。然而，这种基本设置忽略了覆盖同一概念的问题之间的个体差异，从而限制了 KT 方法的灵活性和它们的个性化潜力。<a href="#_msocom_3">[喻清尘3]</a><br>我们使用心理测量学中一个经典而强大的模型，Rasch 模型(也称为 1PL IRT 模型)[16，25]，来构建原始问题和知识嵌入。Rasch 模型使用两个标量来描述学习者正确回答问题的概率：问题的难度和学习者的能力。尽管它很简单，但在正式评估中，当知识是静态的时，它在学习者表现预测上取得了与更复杂的模型相当的性能[12，31]。具体地说，我们将来自概念 ct 的问题 qt 在时间步 t 的嵌入构造为<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618246066019-e3081deb-4daa-4330-a0f2-419e16e0cb68.png#height=24&id=Rbcvv&margin=%5Bobject%20Object%5D&name=image.png&originHeight=24&originWidth=128&originalType=binary%E2%88%B6=1&size=1277&status=done&style=none&width=128" alt="image.png"><br>       其中，<img src="https://cdn.nlark.com/yuque/__latex/d9cd06829ce329a2bbe0bdb52030a7d1.svg#card=math&code=c_%7Bct%7D%E2%88%88R%5ED&height=21&id=ptYSp">是本问题涵盖的概念的嵌入，<img src="https://cdn.nlark.com/yuque/__latex/af3d0f493c9f9f2a12476aadda536c9c.svg#card=math&code=d_%7Bct%7D%E2%88%88R%5ED&height=21&id=X81Sa">是总结涉及此概念的问题的变化的向量，<img src="https://cdn.nlark.com/yuque/__latex/e087ff1ad2a484a1206a1410e8fbe2c8.svg#card=math&code=u_%7Bqt%7D%E2%88%88R&height=20&id=UF0I9">是控制此问题与其涵盖的概念的偏离程度的标量难度参数。使用标量难度参数对每个概念 ct 中的问题-回答对(qt，rt)进行类似地扩展：<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618246148995-cf3ddc11-2e61-4b60-ba0c-95dd6feb3ce1.png#height=29&id=slIob&margin=%5Bobject%20Object%5D&name=image.png&originHeight=29&originWidth=156&originalType=binary%E2%88%B6=1&size=1839&status=done&style=none&width=156" alt="image.png"><br>其中 e(ct，rt)∈RD 和 f(ct，rt)∈RD 是概念反应嵌入和变异向量。这种模式选择反映了我们的第三个直觉：被标记为涵盖相同概念的问题密切相关，但具有重要的个体差异，不应被忽视。这种模型选择在一定程度上受到了融合 KT 和 IRT 模型的另一项工作的启发[8]。<br>这些基于 Rasch 模型的嵌入在对单个问题差异建模和避免过度参数化之间取得了适当的平衡。对于问题嵌入，由于 C≪Q 和 D≫1，该模型的嵌入参数总数为 2CD+Q，略多于使用概念索引问题的模型(CD)，但远低于每个问题单独参数化的模型(QD)。我们进一步定义概念-回答嵌入为<img src="https://cdn.nlark.com/yuque/__latex/59d0080401e0f8364fece06e8e03f6ed.svg#card=math&code=e%28ct%EF%BC%8Crt%29%3Dc_%7Bct%7D%2Bg_%7Brt%7D&height=24&id=bMaUz">，其中 g1 和 g0 分别表示正确答案和错误答案的嵌入(与概念无关)。因此，对于概念-反应嵌入，我们只引入了总共(C+2)D+Q 个新嵌入参数，而没有引入 2CD+Q 个新参数。我们注意到，我们的问题和问题-回答嵌入共享一组参数<img src="https://cdn.nlark.com/yuque/__latex/6f9e619b3a54feb0ac3227fbcf25bcb7.svg#card=math&code=%28c_%7Bct%7D%29&height=20&id=kq0WS">；这种设置不同于现有的基于神经网络的 KT 方法，在现有的 KT 方法中，两者是相互独立的。这些紧凑的嵌入表示法不仅显著减少了 AKT 中的参数数量，而且还显著减少了其他一些 KT 方法中的参数数量，从而提高了对未来学习者成绩预测的性能；有关详细信息，请参见表 5。<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618246265279-5c03ea92-b619-4992-bff8-b2766082ebeb.png#height=100&id=UEVAs&margin=%5Bobject%20Object%5D&name=image.png&originHeight=100&originWidth=638&originalType=binary%E2%88%B6=1&size=19217&status=done&style=none&width=638" alt="image.png"><br>表 5：基于 Rasch 模型的嵌入(有时非常重要)提高了 KT 方法的性能。</p><h1 id="4-EXPERIMENTAL-RESULTS-实验结果"><a href="#4-EXPERIMENTAL-RESULTS-实验结果" class="headerlink" title="4 EXPERIMENTAL RESULTS 实验结果"></a>4 EXPERIMENTAL RESULTS 实验结果</h1><p>在本节中，我们将详细介绍我们在几个真实数据集上进行的一系列实验。我们通过预测学习者未来的反应对 AKT 进行定量评估，并通过一系列可视化和案例研究对 AKT 进行定性评估。</p><h2 id="4-1-Experimental-Setup-实验设置"><a href="#4-1-Experimental-Setup-实验设置" class="headerlink" title="4.1 Experimental Setup 实验设置"></a>4.1 Experimental Setup 实验设置</h2><p><strong>数据集。</strong>我们使用四个基准数据集：ASSISTments2009、ASSISTments2015、ASSISTments2017 和 Statics2011 评估了 AKT 的性能和几个预测未来学习者反应的基线。ASSISTments 数据集是从一个在线教学平台收集的，特别是 ASSISTments2009 数据集在过去十年中一直是 KT 方法的标准基准。Statics2011 数据集是从一门大学级别的静力学工程课程中收集的。在所有这些数据集上，我们遵循文献中的一系列标准预处理步骤。对于 ASSISTments2009 数据集，我们删除与命名概念没有关联的所有交互。对于 ASSISTments2015 数据集，我们删除了“isGent”字段不是 0 或 1 的所有交互。我们在表 1 中列出了学习者、概念、问题和问题回答对的数量。在这些数据集中，只有 ASSISTments2009 和 ASSISTments2017 数据集包含问题 ID；因此，基于 Rasch 模型的嵌入仅适用于这两个数据集。<br>       <strong>基线方法和评估指标。</strong>我们将 AKT 与几种基线 KT 方法进行了比较，包括 BKT+[35]，DKT，DKT+(它是 DKT 的改进版本，具有预测一致性的正则化[34])，DKVMN[36]，以及最近提出的自关注 KT(SAKT)方法[18]，它使用了一种可以被视为 AKT 的特例的注意机制，而没有对问题和回答的上下文感知表示和单调注意机制。我们使用接收器操作特征曲线(AUC)下的面积作为度量来评估所有 KT 方法在预测二值未来学习者对问题的反应方面的性能。<br>       <strong>训练和测试。</strong>为了评估目的，我们对所有模型和所有数据集执行标准的 k 折交叉验证(k = 5)。因此，对于每一个 fold，使用 20%的学习者作为测试集，20%的学习者作为验证集，60%的学习者作为训练集。对于每一个 fold，我们使用验证集对每一个 KT 方法进行早期停止和调整参数。<br>       由于计算效率的原因，我们截断了超过 200 的学习者反应序列[23,36]。如果一个学习者有超过 200 个反应，我们就把他们的整个反应序列分解成多个更短的反应序列。我们使用 Adam 优化器训练所有模型[10]，批量大小为 24 个学习者，以确保我们的机器(配备了一个 NVIDIA Titan X GPU)能够容纳整个批处理。我们在 PyTorch 中实现了 AKT 的所有版本;我们还重新实现了 DKT、DKT+和 SAKT，因为包含问题 id 需要新的数据集分区，并导致新的实验结果。我们对 AKT, DKT, DKT+，和 SAKT 使用 Xavier 参数初始化方法[4];对于 DKVMN，我们遵循他们的工作，使用来自正态分布的样本来初始化参数[36]。我们不重新实现 BKT+;其在各种数据集上的性能均来自于[36]。对于大多数数据集和算法，一个 epoch 需要少于 10 秒。我们将最大纪元数设置为 300。</p><h2 id="4-2-Results-and-Discussion-结果与讨论"><a href="#4-2-Results-and-Discussion-结果与讨论" class="headerlink" title="4.2 Results and Discussion 结果与讨论"></a>4.2 Results and Discussion 结果与讨论</h2><p>表 2 列出了所有 KT 方法在预测未来学习者反应方面在所有数据集中的表现;我们报告了五次测试的平均值和标准偏差。AKT- r 和 AKT- nr 分别表示基于 Rasch 模型的嵌入和不嵌入 AKT 模型的变体。我们看到 AKT(有时显著)在 ASSISTments 数据集上优于其他 KT 方法，而 DKT+在最小的 Statics2011 数据集上略微优于 AKT。总的来说，AKT 在更大的数据集上表现更好;这一结果表明，注意机制比递归神经网络更灵活，因此更有能力捕捉包含在大规模真实世界学习者反应数据集中的丰富信息。在 ASSISTments2015 和 ASSISTments2017 数据集上，AKT-NR 比最近的基线提高了 6%和 1%的 AUC。它在 Statics2011 和 ASSISTments2009 数据集上的性能与最佳基线不相上下。更重要的是，在有问题 id 的 ASSISTments2009 和 2017 数据集上，AKT-R 显著优于其他 KT 方法，分别比最近的基线高出 2%和 6%。我们注意到，在我们的实现中，DKT 的性能优于更高级的 DKVMN 方法。虽然我们能够使用相同的实验设置[36]复制 DKVMN 的性能，但我们发现 DKT 的性能比之前在该研究中报告的要好得多。DKT+与 DKT 的性能相当，对 Statics2011 数据集做了少许改进。我们还观察到，基于 rnn 的模型 DKT 在所有数据集上的性能都优于 SAKT。<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618246277993-134008cb-4407-4632-8c38-1a3c5831438b.png#height=125&id=MfbgU&margin=%5Bobject%20Object%5D&name=image.png&originHeight=125&originWidth=643&originalType=binary%E2%88%B6=1&size=38285&status=done&style=none&width=643" alt="image.png"><br>表 2:所有 KT 方法在所有数据集上预测未来学习者反应的表现。AKT(有时显著)在所有数据集上优于所有基线方法。最好的模特是粗体，第二好的模特是斜体。<br>       <strong>消融研究。</strong>为了证明 AKT 方法中的三个关键创新，即问题和回答的上下文感知表示、单调注意机制和基于 Rasch 模型的嵌入，我们进行了三个额外的消融实验，比较了 AKT 方法的几种变体。第一个实验比较了使用上下文感知问题和响应表示(使用问题和知识编码器)的 AKT-NR 和 AKT-R 与两个变体 AKTraw-NR 和 AKTraw-R；在这些变体中，我们使用原始的问题和响应嵌入作为它们的表示，而不是上下文感知表示(即，不通过编码器传递它们)。第二个实验比较了 AKT-NR 和几个没有单调注意机制的变体。这些变体包括 AKT-NRpos 和 AKT-NRfix，AKT-NRpos 使用(可学习的)位置编码来捕获学习者响应数据中的时间依赖性，AKT-NRfix 使用(固定)位置编码，使用不同频率的正弦和余弦函数[29]。第三个实验将 AKT-R 与 AKT-NR、DKT、DKT-R、DKT+、DKT+-R、DKVMN、DKVMN-R、SAKT 和 SAKT-R 在有问题 ID 的 ASSIST 2009 和 2017 数据集上进行比较；DKT-R、DKT+-R、DKVMN-R 和 SAKT-R 指的是 DKT、DKT+、DKT-R<br>       表 3 显示了上下文感知表示(即问题编码器和知识编码器)的第一次消融实验的结果(由于空间限制，仅测试折叠的平均值，而不是标准偏差)。在所有数据集上，AKT-R 和 AKTNR 的性能都优于 AKTraw-NR 和 AKTraw-R，后者只使用一种指数衰减的自我注意机制(即知识检索器)。这些结果表明，我们对问题和回答的语境感知表征在总结每个学习者的练习历史时是有效的。<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618246285439-c933f944-934a-4b01-89b5-b26b77b8dab4.png#height=128&id=jwbMa&margin=%5Bobject%20Object%5D&name=image.png&originHeight=128&originWidth=365&originalType=binary%E2%88%B6=1&size=17053&status=done&style=none&width=365" alt="image.png"><br>表 3：AKT 的表现优于不使用上下文感知问题和响应表示的变体。<br>       表 4 显示了单调注意机制的第二次消融实验结果。我们发现，AKT-NR 在所有数据集上的表现明显优于其他使用位置嵌入的注意机制，包括 SAKT，大约 1%到 6%。我们假设这一结果的原因是，与语言任务不同的是，在语言任务中，单词之间强烈的远程依赖更常见，未来学习者对过去表现的依赖被限制在更短的时间窗口内。因此，在注意力权重中使用不同指数衰减率的多头注意可以有效地捕捉不同时间尺度上对过去的短期依赖。<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618246290257-7623af58-ba82-46ac-b13d-69429da4c302.png#height=129&id=W6ueT&margin=%5Bobject%20Object%5D&name=image.png&originHeight=129&originWidth=361&originalType=binary%E2%88%B6=1&size=17248&status=done&style=none&width=361" alt="image.png"><br>表 4：AKT 显著优于不使用单调注意的变种。<br>      表 5 显示了基于 Rasch 模型的嵌入在两个 ASSISTments 数据集(其中有问题 ID)上的第三次消融实验结果。所有添加了基于 Rasch 模型嵌入的基线 KT 方法都优于它们的常规版本，特别是在 ASSISTments2017 数据集上。这些结果证实了我们的直觉，即将涵盖同一概念的所有问题视为一个问题是有问题的；只要可以避免过度参数化，这些问题之间的个体差异就不应该被忽视。<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618246297665-146073c5-30dd-4b0b-a998-95ddb8266354.png#height=96&id=e1hjU&margin=%5Bobject%20Object%5D&name=image.png&originHeight=96&originWidth=640&originalType=binary%E2%88%B6=1&size=19135&status=done&style=none&width=640" alt="image.png"><br>表 5：基于 Rasch 模型的嵌入(有时非常重要)提高了 KT 方法的性能。<br><strong>注意。</strong>我们的标准实验设置遵循[23，36]中使用的设置。在此设置中，对于带有多个概念的问题(在 ASSISTments2009 数据集中)，单个学习者的回答重复多次，每个概念一个。其他研究对这些问题使用了不同的实验设置；在[31]中，作者去掉了这些问题，结果，DKT 的成绩降到了 0.71。在[33]中，作者为共现的单个概念的每个组合建立了新的概念，结果，DKT 的性能下降到 0.73。因此，我们还在 ASSISTments2009 数据集上使用了另一种实验设置。对于带有多个概念的问题，我们平均相应的概念嵌入，并将其用作输入嵌入和响应预测。表 6 列出了此设置下 ASSISTments2009 数据集上所有 KT 方法的性能。使用平均嵌入时，DKT 的性能下降到 0.76，比[31，33]下的设置要好。与所有 KT 方法的标准实验设置相比，我们观察到类似的性能下降，而 AKT-R 仍然轻松地超过所有基线。<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618246304142-24afacee-bead-4b55-b9fa-1126a8274144.png#height=100&id=ez00O&margin=%5Bobject%20Object%5D&name=image.png&originHeight=100&originWidth=365&originalType=binary%E2%88%B6=1&size=13531&status=done&style=none&width=365" alt="image.png"><br>表 6：AKT 在 ASSISTments2009 数据集上的性能仍然优于其他 KT 方法，在使用多个概念标记的问题的另一种实验设置下，AKT 仍然优于其他 KT 方法。</p><h2 id="4-3-Visualizing-Learned-AKT-Parameters-可视化学习的-AKT-参数"><a href="#4-3-Visualizing-Learned-AKT-Parameters-可视化学习的-AKT-参数" class="headerlink" title="4.3 Visualizing Learned AKT Parameters 可视化学习的 AKT 参数"></a>4.3 Visualizing Learned AKT Parameters 可视化学习的 AKT 参数</h2><p><strong>单调注意。</strong>图 2 显示了 AKT 使用 ASSISTments2009 数据集的单调注意机制提供的可解释性。图 2(A)以一个学习者为例，可视化了知识检索器中的注意力权重；我们绘制了用于预测他们在三个注意力头部的 20 个连续练习问题上的表现的注意力权重。我们看到，每个注意力头部都有自己的时间尺度：它们都有不同宽度的注意力窗口。例如，第二个头能够关注整个过去，最多 20 个时间点(在本例中)；相反，第三个头只能关注最近的过去，主要关注最后 3-5 个时间点。这一观察表明，过去的一些问题和回答包含了高度预测学习者对当前问题的反应的信息；这些信息可以被具有不同衰减率的多个注意力头脑有效地捕捉到。<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618246311226-32354069-9651-445a-82c2-2fca2446bce5.png#height=365&id=gOWs6&margin=%5Bobject%20Object%5D&name=image.png&originHeight=365&originWidth=370&originalType=binary%E2%88%B6=1&size=38538&status=done&style=none&width=370" alt="image.png"><br>图 2：对学习者来说，(A)AKT 解码器中三个注意力头部的注意力权重和(B)三个连续练习问题的注意力权重的可视化。概念相似度和新近度是控制注意力权重的关键因素。<br>       图 2(B)显示了知识检索器中针对单个学习者连续三个时间步的归一化注意力权重。在第一排，学习者在从 −10 到 T−5 练习了概念 30 之后，正在回答关于概念 30 的问题，然后休息一下练习概念 42，然后回到 TIMET−1 的概念 30。我们看到，AKT 预测他们对当前问题的反应是通过更多地关注以前对这个概念的练习(无论是在最近的过去还是更早的过去)，而不是对同样是在不久的过去的另一个概念的练习。在中间一排，学习者再次切换到练习概念 42。同样，在 T−2 和 T−1 的时候，AKT 学习将注意力集中在同一概念上的过去练习，而不是刚刚过去的不同概念上。在最下面的一排，学习者连续第二次练习概念 42，AKT 显示出与第一行相似的焦点模式，概念 30 和 42 的角色互换了。这些观察表明，AKT 的单调注意机制有可能通过将学习者当前的反应与他们过去的反应联系起来，向教师提供反馈；这些信息可能使教师能够选择他们已经练习过的特定问题，让他们在继续学习之前重新练习并清除误解。我们还注意到，AKT 使用数据驱动的方法，学习这些与现有 KT 方法中手工制作的特征相匹配的注意模式(例如，对该概念的总尝试次数和正确尝试次数)[15，22]。<br>       基于 Rasch 模型的嵌入。图 3 显示了使用 ASSISTments2009 数据集使用 t-SNE[28]对几个概念学习的基于 Rasch 模型的问题嵌入，以及它们对选定问题(学习者的正确答案的一部分)的经验困难。我们还强调了每个概念的最难和最简单的问题，这是基于它们的经验困难。我们看到，同一概念上的问题形成一条曲线，并按其难度级别排序：对于大多数概念，线段一端的问题很容易，而另一端的问题很难。这一结果证实了我们的直觉，即来自同一概念的问题不是完全相同的，而是密切相关的；这种关系可以通过 Rasch 模型的难度参数很好地捕捉到。<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618246318541-1be15a25-8c8c-4b03-886f-33d9aaf267f3.png#height=225&id=a0yW8&margin=%5Bobject%20Object%5D&name=image.png&originHeight=225&originWidth=359&originalType=binary%E2%88%B6=1&size=19853&status=done&style=none&width=359" alt="image.png"><br>图 3：可视化的学习问题嵌入，学习者对所选概念的部分正确回答。<br>       表 7 列出了针对三个不同概念（“正序十进制”，“单个事件的概率”和“分数到百分比的转换”）的样本问题，以及它们的学习难度参数。对于每个概念，我们显示三个问题：一个简单的问题，一个平均的问题和一个困难的问题。以“单个事件的概率”概念为例，学习的难度参数值（µq）对于简单事件为-0.0515，对于平均事件为 0.0088，对于困难事件为 0.0548。这些学习的难度级别与我们对这些问题的难度级别的理解相符。<br>       这些结果表明，AKT 有潜力应用于现实世界的教育环境中。使用估计的难度参数，计算机化学习平台可以 i)根据每个学习者过去的反应自动选择具有适当难度级别的问题，或者 ii)通过向教师提供关于从真实数据中学习的问题难度级别的反馈来支持教师调整课程计划。因此，AKT 不仅提供最先进的预测性能，而且表现出可解释性和个性化学习的潜力，从而改进了现有的 KT 方法。<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618246327348-e4305e88-5cb8-44e5-a10e-1c21c9147f30.png#height=406&id=B8jPB&margin=%5Bobject%20Object%5D&name=image.png&originHeight=406&originWidth=352&originalType=binary%E2%88%B6=1&size=52596&status=done&style=none&width=352" alt="image.png"><br>表 7：关于三个概念的选定问题的问题文本和学习难度参数(µQ)。习得的难度水平符合我们对这些问题难度的直觉。</p><h1 id="5-CONCLUSIONS-AND-FUTURE-WORK-结论和下一步工作"><a href="#5-CONCLUSIONS-AND-FUTURE-WORK-结论和下一步工作" class="headerlink" title="5 CONCLUSIONS AND FUTURE WORK 结论和下一步工作"></a>5 CONCLUSIONS AND FUTURE WORK 结论和下一步工作</h1><p>本文提出了一种新的完全依赖注意力网络的知识追踪方法–注意力知识追踪(Attensitive Knowledge Tracing)。我们的方法改进了现有的知识跟踪方法，通过建立问题和回答的上下文感知表示，使用单调的注意机制来总结过去学习者在正确的时间尺度上的表现，并使用 Rasch 模型来捕捉覆盖相同概念的问题之间的个体差异。在一系列基准真实学习者反应数据集上的实验结果表明，该方法的性能优于最先进的 KT 方法，并表现出良好的可解释性。未来工作的途径包括：i)纳入问题文本，以进一步增强问题和概念嵌入的可解释性；ii)测试我们的方法是否可以提高对发生记忆衰退的语言学习数据集的预测性能[26]。</p><hr><p><a href="#_msoanchor_1">[喻清尘1]</a>研究问题 1</p><p>1、 <a href="#_msoanchor_2">[喻清尘2]</a>选择使用上下文感知嵌入而不是原始嵌入反映了我们的第一个直觉：学习者在回答问题时理解和学习的方式取决于学习者。这些修改后的表征反映了每个学习者对问题的实际理解和他们实际获得的知识，给出了他们的个人反应历史。这种模式的选择是基于这样一种直觉，即对于两个过去回答顺序不同的学习者来说，他们理解同一问题的方式以及他们从练习中获得的知识可能会有所不同。<br>2、第二直觉：当学习者面临一个新的问题时，i)关于无关概念和 ii)来自太久以前的过去经验不太可能是高度相关的。<br>3、第三个直觉：被标记为涵盖相同概念的问题密切相关，但具有重要的个体差异，不应被忽视。</p><p><a href="#_msoanchor_3">[喻清尘3]</a>研究问题 2</p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;上下文感知的注意力集中的知识追踪&quot;&gt;&lt;a href=&quot;#上下文感知的注意力集中的知识追踪&quot; class=&quot;headerlink&quot; title=&quot;上下文感知的注意力集中的知识追踪&quot;&gt;&lt;/a&gt;上下文感知的注意力集中的知识追踪&lt;/h1&gt;&lt;h1 id=&quot;ABSTRACT-摘要&quot;&gt;&lt;a href=&quot;#ABSTRACT-摘要&quot; class=&quot;headerlink&quot; title=&quot;ABSTRACT 摘要&quot;&gt;&lt;/a&gt;ABSTRACT 摘要&lt;/h1&gt;&lt;p&gt;知识追踪(KT)指的是根据学习者过去在教育应用中的表现来预测其未来表现的问题。使用灵活的深度神经网络模型的 KT 的最新发展擅长于这一任务。然而，这些模式的可解释性往往有限，因此不足以满足个性化学习的需要。个性化学习需要使用可解释的反馈和可操作的建议来帮助学习者获得更好的学习结果。在本文中，我们提出了注意力知识追踪(AKT)，它将灵活的基于注意力的神经网络模型与一系列受认知和心理测量模型启发的新颖的、可解释的模型组件相结合。AKT 使用了一种新的单调注意机制，将学习者未来对评估问题的反应与他们过去的反应联系起来；除了问题之间的相似性外，还使用指数衰减和上下文感知的相对距离度量来计算注意力权重。此外，我们使用 Rasch 模型来规则化概念和问题嵌入，这些嵌入能够在不使用过多参数的情况下捕捉同一概念上问题之间的个体差异。我们在几个真实的基准数据集上进行了实验，结果表明，AKT 在预测未来学习者的反应方面优于现有的 KT 方法(在某些情况下 AUC 高达 6%)。我们还进行了几个案例研究，表明 AKT 表现出极好的可解释性，因此在现实世界的教育环境中具有自动反馈和个性化的潜力。&lt;/p&gt;</summary>
    
    
    
    <category term="知识追踪" scheme="https://ytno1.github.io/categories/%E7%9F%A5%E8%AF%86%E8%BF%BD%E8%B8%AA/"/>
    
    <category term="AKT" scheme="https://ytno1.github.io/categories/%E7%9F%A5%E8%AF%86%E8%BF%BD%E8%B8%AA/AKT/"/>
    
    
    <category term="知识追踪" scheme="https://ytno1.github.io/tags/%E7%9F%A5%E8%AF%86%E8%BF%BD%E8%B8%AA/"/>
    
    <category term="AKT" scheme="https://ytno1.github.io/tags/AKT/"/>
    
  </entry>
  
  <entry>
    <title>2019-SAKT-A Self-Attentive model for Knowledge Tracing(Shalini Pandey et al.)</title>
    <link href="https://ytno1.github.io/archives/f5d30e8c.html"/>
    <id>https://ytno1.github.io/archives/f5d30e8c.html</id>
    <published>2021-04-12T14:58:32.000Z</published>
    <updated>2021-08-31T07:14:52.273Z</updated>
    
    <content type="html"><![CDATA[<h1 id="ABSTRACT-摘要"><a href="#ABSTRACT-摘要" class="headerlink" title="ABSTRACT 摘要"></a>ABSTRACT 摘要</h1><p>知识追踪是指当每个学生参与一系列学习活动时，对知识概念(KCs)的掌握情况进行建模的任务。每个学生的知识是通过评估学生在学习活动中的表现来建模的。为学生提供个性化的学习平台是一个重要的研究领域。近年来，基于递归神经网络(RNN)的方法，如深度知识追踪(DKT)和动态键值记忆网络(DKVMN)，由于能够捕获人类学习的复杂表示，因而优于所有传统方法。然而，这些方法在处理稀疏数据时面临的问题是不能很好地泛化【研究问题】，这是真实世界数据的情况下，学生与很少的 KCs 交互。为了解决这个问题，我们开发了一种方法，从学生过去的活动中识别与给定 KC 相关的 KC，并根据选择的相对较少的 KC 预测他/她的掌握情况。由于预测是基于相对较少的过去活动，它处理数据稀疏性问题比基于 RNN 的方法更好。为了识别 KCs 之间的相关性，我们提出了一种基于自我注意的方法——自我注意知识追踪(SAKT)。在各种真实世界数据集上的广泛实验表明，我们的模型在知识追踪方面优于最先进的模型，平均提高了 4.43%的 AUC。</p><p><strong>关键词：</strong>知识追踪，大规模网络开放课程，自我关注，顺序推荐</p><span id="more"></span><h1 id="1-INTRODUCTION-引言"><a href="#1-INTRODUCTION-引言" class="headerlink" title="1. INTRODUCTION 引言"></a>1. INTRODUCTION 引言</h1><p>学生关于他们知识概念(KC)的学习轨迹有海量数据集是可用的，其中 KC 可以是一种练习，一种技能或一个概念，吸引了数据挖掘者开发工具来预测学生的表现并给予适当的反馈[8]。在开发这种个人化学习平台的过程中，知识追踪(knowledge tracing, KT)被认为是一项重要的任务，它被定义为基于学生过去的学习活动对其知识状态进行追踪，这种知识状态代表了学生对 KCs 的掌握程度。KT 任务可以被形式化为一个监督序列学习任务-给定学生过去的练习互动 X = (x1,x2，…，xt)，预测他/她下次互动（回答）xt+1 的某个方面。在问答平台上，交互用 xt= (et, rt)表示，其中 et 是学生在时间戳 t 上尝试的练习，rt 是学生答案的正确性。KT 旨在预测学生是否能够正确回答下一个练习，即预测 p(rt+1= 1|et+1,X)。<br>最近，深度知识追踪(DKT)[6]及其变体[10]等深度学习模型使用递归神经网络(RNN)在一个总结的隐向量中对学生的知识状态进行建模。动态键值记忆网络(DKVMN)[11]为 KT 开发了记忆扩充神经网络[7]。它使用 Key 和 Value 两个矩阵，分别学习练习与潜在的 KC 和学生知识状态之间的关系。DKT 模型面临参数无法解释的问题[4]。DKVMN 比 DKT 更易于解释，因为它显式地维护 KC 表示矩阵(密钥)和知识状态表示矩阵(值)。然而，由于所有这些深度学习模型都是基于 RNN 的，它们在处理稀疏数据时都面临着不能泛化的问题[3]。<br>在本文中，我们建议使用一种纯粹基于注意机制的方法，即转换器（Transformer）[9]。在 KT 任务中，学生在经历一系列学习活动时所获得的技能是相互关联的，在特定练习中的表现取决于他在过去与该练习相关的练习中的表现。例如，在图 1 中，一个学生要解决属于知识概念“方程”的“二次方程”的练习(练习 5)，他需要知道如何找到“平方根”(练习 3)和“线性方程”(练习 4)。本文提出的 SAKT 首先从过去的互动中识别出相关的知识概念，然后根据学生在这些知识概念上的表现来预测学生的表现。为了预测学生在练习中的表现，我们使用练习作为 KC。正如我们稍后所展示的，SAKT 为前面回答的练习分配权重，同时预测学生在特定练习中的表现。在所有数据集上，SAKT 方法的性能明显优于现有的 KT 方法，在 AUC 上的性能平均提高了 4.43%。此外，SAKT 的主要成分(自我注意)适合于并行性，从而使我们的模型比基于 RNN 的模型快了一个数量级【SAKT 优势】。<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618243190838-289e5481-976c-41a8-9ab9-8211ff6f4544.png#align=left&display=inline&height=108&margin=%5Bobject%20Object%5D&name=image.png&originHeight=108&originWidth=380&size=17722&status=done&style=none&width=380" alt="image.png"><br>图 1：左下图显示了学生尝试的练习顺序，右下图显示了每个练习所属的知识概念。</p><h1 id="2-PROPOSED-METHOD-建议的方法"><a href="#2-PROPOSED-METHOD-建议的方法" class="headerlink" title="2. PROPOSED METHOD 建议的方法"></a>2. PROPOSED METHOD 建议的方法</h1><p>我们的模型根据一个学生之前的交互序列 X = x1,x2，…xt，预测他是否能够回答下一个练习 et+1。如图 2 所示，我们可以将问题转换为顺序建模问题。考虑输入为 x1,x2，…xt 且前面一个位置的练习序列 e2, e3，…，et 且输出为对练习响应的正确性 r2, r3，…rt 的模型是方便的。交互元组 xt= (et, rt)在模型中表示为数字 yt= et+ rt× E，其中 E 为练习总数。因此，交互序列中的元素可以接受的总值是 2E，而练习序列中的元素可以接受 E 个可能的值。<br>现在我们来描述我们体系结构的不同层。<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618243262538-16e7fabc-9155-4a0e-9fae-f2544e4bcf00.png#align=left&display=inline&height=285&margin=%5Bobject%20Object%5D&name=image.png&originHeight=285&originWidth=298&size=44411&status=done&style=none&width=298" alt="image.png"><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618243271344-68f30c9e-24c2-4548-b6dc-475efa17ba70.png#align=left&display=inline&height=142&margin=%5Bobject%20Object%5D&name=image.png&originHeight=142&originWidth=306&size=14789&status=done&style=none&width=306" alt="image.png"><br> (a)SAKT 网络。在每个时间戳上，仅为先前元素中的每一个估计关注度权重。从如下所示的嵌入层中提取键、值和查询。当第 j 个元素为查询且第 i 个元素为关键时，关注权重为 ai，j。<br>(b)嵌入层嵌入学生正在尝试的当前练习及其过去的交互。在每个时间戳 t+1 处，使用练习嵌入将当前问题 et+1 嵌入到查询空间中，并且使用交互嵌入将过去交互 xt 的元素嵌入到键和值空间中。<br>图 2：显示 SAKT 体系结构的示意图。<br><strong>嵌入层：</strong>对得到的输入序列 y=(y1，y2，…，yt)转化为 s=(s1，s2，…，sn)，其中 n 是模型可以处理的最大长度。由于该模型可以处理固定长度序列的输入，当序列长度 t 小于 n 时，我们在序列的左边重复添加一个问答对填充。然而，如果 t 大于 n，则我们将序列划分为长度为 n 的子序列。具体地说，当 t 大于 n 时，yt 被划分为 t/n 个子序列，每个子序列的长度为 n。所有这些子序列都用作模型的输入。我们训练一个交互嵌入矩阵 M∈R×d，其中 d 是潜在维数。该矩阵用于获得序列中每个元素 si 的嵌入 Msi。同样，我们训练练习嵌入矩阵 E∈R×d，使得集合 ei 中的每个练习嵌入到第 i 行。<br>位置编码：位置编码是自注意神经网络中用来对位置进行编码的那一层，这样我们就可以像卷积网络和递归神经网络一样对序列的顺序进行编码。这一层在知识追踪问题中尤为重要，因为学生的知识状态会随着时间的推移而逐渐稳定地演变。在某个特定时间实例的知识状态不应显示波状转变[10]。为了整合这一点，我们使用了一个参数，位置嵌入，P∈R×d，它是在训练时学习的。然后将第 i 行位置嵌入矩阵 Pi 加入到交互序列的第 i 个元素的交互嵌入向量。<br>嵌入层的输出是嵌入交互输入矩阵 Mˆ 和嵌入练习矩阵 Eˆ：<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618243357720-1758d9a4-a12e-44da-b991-996cf5c3c650.png#align=left&display=inline&height=92&margin=%5Bobject%20Object%5D&name=image.png&originHeight=92&originWidth=226&size=5029&status=done&style=none&width=226" alt="image.png"><br><strong>自我注意层：</strong>在我们的模型中，我们使用了按比例缩放的点积注意机制[9]。这一层找出与每个先前求解的练习相对应的相对权重，以预测当前练习的正确性。<br>我们使用以下公式获得查询和键-值对：<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618243365540-06d51e38-fc02-4357-87a9-6a0f663c83e0.png#align=left&display=inline&height=31&margin=%5Bobject%20Object%5D&name=image.png&originHeight=31&originWidth=238&size=2736&status=done&style=none&width=238" alt="image.png"><br>其中 W、W、W∈R×d 分别表示查询、关键字和值投影矩阵，它们将各自的向量线性投影到不同的空间[9]。使用注意力权重来确定先前的每个交互与当前练习的相关性。为了计算关注度权重，我们使用缩放的点积[9]，定义如下：<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618243372373-97bc6bac-3414-433b-8b0c-d4d52e83d553.png#align=left&display=inline&height=39&margin=%5Bobject%20Object%5D&name=image.png&originHeight=39&originWidth=255&size=4051&status=done&style=none&width=255" alt="image.png"><br>多头:为了共同处理来自不同代表性子空间的信息，我们使用不同的投影矩阵对查询、键和值进行线性投影 h 次。<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618243383333-45cc8361-eaca-4901-b8cc-723b233a90b9.png#align=left&display=inline&height=22&margin=%5Bobject%20Object%5D&name=image.png&originHeight=22&originWidth=305&size=3432&status=done&style=none&width=305" alt="image.png"><br>其中<img src="https://cdn.nlark.com/yuque/__latex/772b9537c10dd0f2f83ce0ef1fe1f43a.svg#card=math&code=head_i%3D%20Attention%28%5Chat%20E%20W%5EQ_i%EF%BC%8C%5Chat%20MW%5EK_i%EF%BC%8C%5Chat%20MW%5EV_i%29&height=26&width=317">和 W∈R×d。<br>因果关系：在我们的模型中，在预测第(t+1)次练习的结果时，我们应该只考虑前 t 次交互作用。因此，对于查询 Qi，不应考虑使得 j&gt;i 的键 Kj。我们使用因果层来掩蔽从未来交互关键字学习到的权重。<br><strong>前馈层：</strong>上面描述的自我注意层的结果是前面交互的值 Vi 的加权和。然而，从多头层获得的矩阵的行<img src="https://cdn.nlark.com/yuque/__latex/2fdbad74d3b3359634d48a2b1f988430.svg#card=math&code=S%3DMultiHead%28%5Chat%20M%EF%BC%8C%5Chat%20E%29&height=26&width=177">仍然是先前交互的值 Vi 的线性组合。为了在模型中加入非线性，并考虑不同潜在维度之间的相互作用，我们使用了前馈网络。<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618243674509-49bf487d-0415-4d9f-a29d-3f2e27d10c31.png#align=left&display=inline&height=30&margin=%5Bobject%20Object%5D&name=image.png&originHeight=30&originWidth=299&size=3464&status=done&style=none&width=299" alt="image.png"><br>其中 W∈R×d，W∈R×d，b∈R，b∈R 是在训练过程中学习到的参数。<br>残差连接：残差连接[2]用于将较低层的特征传播到较高层。因此，如果低层特征对预测很重要，残差连接将有助于将它们传播到执行预测的最终层。在 KT 的背景下，学生尝试用属于特定概念的练习来强化该概念。因此，残差连接可以帮助将最近求解的练习的嵌入传播到最后一层，使得模型更容易利用低层信息。在自我注意层和前馈层之后都应用了残差连接。<br>层归一化：文献[1]表明，归一化跨特征的输入有助于稳定和加速神经网络。出于同样的目的，我们在我们的体系结构中使用了层归一化。在自我注意层和前馈层也进行了层归一化。<br><strong>预测层：</strong>最后，将得到的矩阵 Fi 值的每一行通过 Sigmoid 激活函数的全连接网络对学生的表现进行预测。<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618243682620-a7630c13-6807-4f1a-9fea-8b4e3fa3591e.png#align=left&display=inline&height=32&margin=%5Bobject%20Object%5D&name=image.png&originHeight=32&originWidth=157&size=2119&status=done&style=none&width=157" alt="image.png"><br>其中 pi 是标量，表示学生对练习 ei 正确回答的概率，Fi 是 F 的第 i 行，Sigmoid(Z)=1/(1+e−z)<br>网络训练：训练的目标是最小化模型下观察到的学生反应序列的负对数似然率。通过最小化 pt 和 rt 之间的交叉熵损失来学习参数。<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618243689528-a0885321-691c-48e9-a2b0-c050a52bbbaa.png#align=left&display=inline&height=34&margin=%5Bobject%20Object%5D&name=image.png&originHeight=34&originWidth=248&size=2747&status=done&style=none&width=248" alt="image.png"></p><h1 id="3-EXPERIMENTAL-SETTINGS-实验环境"><a href="#3-EXPERIMENTAL-SETTINGS-实验环境" class="headerlink" title="3. EXPERIMENTAL SETTINGS 实验环境"></a>3. EXPERIMENTAL SETTINGS 实验环境</h1><h2 id="3-1-Datasets-数据集"><a href="#3-1-Datasets-数据集" class="headerlink" title="3.1 Datasets 数据集"></a>3.1 Datasets 数据集</h2><p>Synthetic：该数据集是通过模拟 4000 名虚拟学生的答题轨迹得到的。每个学生回答相同顺序的 50 个练习，这些练习取自 5 个难度不同的虚拟概念。<br>ASSISTment 2009(ASSIST2009)：此数据集由 ASSISTment 在线辅导平台提供，广泛用于 KT 任务。我们在更新后的“技能构建者”数据集上进行了实验。该数据集是稀疏的，因为该数据集的密度为 0.06，如表 2 所示。<br>ASSISTment 2015(ASSIST2015)：ASSISTment 2015 包含学生对百项技能的回答。共有 19917 名学生和 708631 个互动。虽然此数据集中的记录数量多于 ASSISTment 2009，但每个学生的平均记录数量较少，因为学生数量较多。此数据集是所有可用数据集中最稀疏的，密度为 0.05。<br>ASSISTment Challenges(ASSISTChall)：此数据来自 ASSISTment 2017 大赛。就互动次数而言，它是最丰富的数据集，有 942,816 个互动，686 名学生和 102 个技能。此数据集是所有可用数据集中密度最高的数据集，因为其密度为 0.81。<br>所有数据集的完整统计信息可以在表 2 中找到。<br>STATICS2011(Statics)：此数据集包含工程静力学课程的交互，包含 189,927 个交互、333 名学生和 1223 个技能标签。我们采用了文献[11]中经过处理的数据。它也是一个密度为 0.31 的密集数据集。<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618243696928-9f79ec27-b0cc-49fd-8d32-de2331cb79b8.png#align=left&display=inline&height=137&margin=%5Bobject%20Object%5D&name=image.png&originHeight=137&originWidth=338&size=16095&status=done&style=none&width=338" alt="image.png"><br>与#Users, #Skill tags 和#Interactions 对应的列分别代表学生数量、练习标签总数和记录数量。列密度 Density 表示每个数据集的密度,即， Density = #Unique Interactions/(#Users ×#Skill tags)       密度=#唯一交互/(#用户 ×#技能标签))。</p><h2 id="3-2-Evaluation-Methodology-评价方法"><a href="#3-2-Evaluation-Methodology-评价方法" class="headerlink" title="3.2 Evaluation Methodology 评价方法"></a>3.2 Evaluation Methodology 评价方法</h2><p><strong>Metrics 度量：</strong>预测任务在二进制分类设置中考虑，即正确或不正确地回答练习。因此，我们使用曲线下面积(AUC)度量来比较性能。<br><strong>Approaches 方法：</strong>我们将我们的模型与最先进的 KT 方法 DKT[6]、DKT+[10]和 DKVMN[11]进行比较。这些方法在引言中有介绍。<br><strong>Model Training and parameter selection 模型训练和参数选择：</strong>我们用 80%的数据集对模型进行训练，并在剩余的数据集上进行测试。对于所有的方法，我们尝试了隐藏状态维数 d={50,100,150,200}。对于相互竞争比较的方法，我们使用了与它们各自的论文中报告的相同的超参数。对于权重的初始化和优化，我们使用了与[10]类似的过程。我们使用 TensorFlow 实现了 SAKT，并使用了学习率为 0.001 的 ADAM[5]优化器。我们对 ASSISTChall 数据集使用批处理大小 256，对其他数据集使用批处理大小 128。对于记录数量较多的数据集，例如 ASSISTChall 和 ASSIST2015，我们使用的 dropout rate（丢弃率）为 0.2，而对于其余数据集，我们使用的 dropout rate 为 0.2。【论文中对于不同数据集使用的 dropout rate 描述似乎有错误】我们设置序列的最大长度，n 与每个学生的平均练习标签大致成正比。对于 ASSISTChall 和 Statics 数据集，我们使用 n=500；对于 ASSIST2009，n=100；对于合成数据集和 ASSIST2015 数据集，n 设置为 50。</p><h1 id="4-RESULTS-AND-DISCUSSION-结果与讨论"><a href="#4-RESULTS-AND-DISCUSSION-结果与讨论" class="headerlink" title="4. RESULTS AND DISCUSSION 结果与讨论"></a>4. RESULTS AND DISCUSSION 结果与讨论</h1><p><strong>Student Performance Prediction 学生表现预测：</strong>表 3 显示了 SAKT 与当前最先进方法的性能比较。在合成数据集上，SAKT 的表现优于竞争对手的方法，AUC 值为 0.832，而 DKT+的 AUC 值为 0.824。尽管合成数据集是密度最高的数据集，但 SAKT 的性能优于基于 RNN 的方法，这是因为生成合成数据所使用的方法。对于此数据集，每个单独的练习仅派生自一个概念。使用项目反应理论[8]确定学生正确回答该数据集中习题的概率为：<img src="https://cdn.nlark.com/yuque/__latex/9f4011ace72797ecb6c816d8a86bec54.svg#card=math&code=p%28correct%7C%CE%B1%EF%BC%8C%CE%B2%29%3Dc%20%2B%20%5Cfrac%20%7B1-c%7D%7B1%2Bexp%28%CE%B2-%CE%B1%29%7D&height=43&width=280">，其中 c 表示正确猜测的概率，α 和 β 是随机选择的数字，分别表示概念能力和练习难度。 因此，在此数据集中，属于同一概念的练习具有很强的相关性。与其他基准不同，SAKT 直接尝试识别属于同一概念的练习，因此比其他方法执行得更好。在 ASSIST2009 上，SAKT 的性能好于被用来比较的其他方法，比第二好的方法获得了 3.16%的性能提升。对于 ASSIST2015 数据集，SAKT 显示出 15.87%的令人印象深刻的改进。我们将这一收益归因于这样一个事实，即 SAKT 利用的注意力机制即使在数据集稀疏的情况下也能很好地学习和概括，ASSIST2015 就是这种情况，因为它的密度是其他数据集中最小的。对于 STATICS2011，与 DKT+相比，我们的方法获得了 2.16%的性能提升。对于 ASSISTChall，我们的方法的性能与 DKT 相当。这可以归因于 ASSISTChall 是所有实际数据集中密度最高的数据集。<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618244005161-f0c4fcc4-c77e-4267-9563-d03b3c9d4ce7.png#align=left&display=inline&height=190&margin=%5Bobject%20Object%5D&name=image.png&originHeight=190&originWidth=366&size=20422&status=done&style=none&width=366" alt="image.png"><br>1、粗体数字是最佳性能。2、通过对每个数据集分别进行最佳超参数选择来获得报告结果。<br><strong>Attention weights visualization 注意权重可视化：</strong>可视化过去交互的要素（用作键）和学生接下来要解决的练习（用作查询）之间的注意权重可以帮助理解过去的交互中的哪些练习与查询练习相关。以此动机为基础，我们可以计算所有运动对（e1，e2）在所有序列中的注意权重之和，其中 e1 作为查询，与运动 e2 的交互作为键。然后，我们对注意力权重进行归一化，以使每个查询的权重之和为 1。这将产生一个相关矩阵，其中每个元素（e1，e2）表示 e2 对 e1 的影响。我们对 Synthetic 进行分析，因为此数据集是用已知的隐藏概念生成的，因此关于不同练习的相关性的基本事实对我们来说是已知的。图 3a 显示了与 Synthetic 中练习的相关性矩阵相对应的热图。对于合成，所有序列均由相同的序列（从 1 到 50）中的所有运动标签组成。<br>为了构建练习标签之间的影响图，如图 3b 所示，我们使用相关性矩阵。首先，我们提取出属于每个隐含概念的序列中的第一个练习，并访问关联矩阵的每一行，并将该行对应的练习与按边权重排序的前两个练习相关联，边权重与两个练习之间的注意力权重成正比。我们可以看到，在注意力权重的基础上，我们能够实现基于隐含概念的运动标签的完美聚类。一个有趣的观察是，两个在序列中相隔很远但属于同一个概念的练习可以被 SAKT 识别出来。例如，如图 3b 所示，对练习 22 的查询将大部分权重分配给练习 5 的键，即使它们在序列中出现的距离很远。<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618244015512-705b0f4d-e9d2-46f3-98ae-28ae140d13de.png#align=left&display=inline&height=282&margin=%5Bobject%20Object%5D&name=image.png&originHeight=282&originWidth=304&size=40234&status=done&style=none&width=304" alt="image.png"><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618244021451-2e2b507b-7cd9-4b3f-b20d-25a802d6dfb6.png#align=left&display=inline&height=152&margin=%5Bobject%20Object%5D&name=image.png&originHeight=152&originWidth=305&size=39943&status=done&style=none&width=305" alt="image.png"><br>(a)每组练习之间的注意权重热图。注意，分配给 pair (i, j)的权重，其中 j &gt; i 总是 0，因为所有的序列都由相同的顺序的练习组成<br>(b)描绘练习之间相关性的图表。相关性是由使用 SAKT 在练习之间学习的注意力权重来确定的。我们观察到潜在概念的完美群集。<br>图 3：可视化合成数据集的注意力权重。<br>       两个相互关联的练习往往具有较高的注意力权重，因为其中一个练习的表现会影响另一个练习的表现。此外，在现实世界的场景中，按顺序发生的练习往往属于同一概念。因此，我们预计注意力权重偏向于交互序列中最近发生的练习。为了说明这一点，我们手动分析了 ASSIST2009 数据集，以可视化一些选定样本的注意力权重。表 4 显示了一些练习，以及过去的交互和分配给每个交互的注意力权重。<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618244030196-261ac239-836a-4a7b-8396-008de988a7ef.png#align=left&display=inline&height=137&margin=%5Bobject%20Object%5D&name=image.png&originHeight=137&originWidth=643&size=37821&status=done&style=none&width=643" alt="image.png"><br>1、  与练习标签对应的列是指查询(即，我们必须预测学生的表现的练习)，而过去的交互分别是指为该学生观察到的交互序列。<br>2、  右栏中的红色元素表示过去交互元素中最重要的元素。<br><strong>Ablation Study 消融研究：</strong>表 5 显示了默认 SAKT 架构和所有数据集(d=200)上的所有变体的性能。<br>没有位置编码 No Positional Encoding (PE):在这个默认架构的变体中，我们删除了位置编码。因此，用于预测学生在某一特定练习中的表现的注意权重仅取决于交互嵌入，而不受其在序列中的位置的影响。在 ASSIST2009 和 ASSIST2015 中，数据集是稀疏的，因此与 ASSISTChall 和 STATICS 等密集数据集相比，去除 PE 的影响不太明显。<br>无残差连接 No Residual Connection (RC):RCs 显示了低水平特征的重要性，即在进行预测时的交互嵌入。由于我们的架构不是很深入，RC 对模型的性能贡献不大。事实上，对于 ASSIST2015 数据集，删除残差连接可以提供比默认设置更好的性能。<br>No Dropout 无丢弃层：在神经网络中引入 Dropout，对模型进行正则化处理，使其具有更好的泛化能力。与模型参数数量相比，模型的过拟合问题对于记录数较少的数据集更为有效。因此，对于 ASSIST2009 数据集和 STATICS 数据集，Dropout 的作用更为有效。<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618244080687-4e571912-4f4b-444b-9197-c460dba0af41.png#align=left&display=inline&height=213&margin=%5Bobject%20Object%5D&name=image.png&originHeight=213&originWidth=362&size=20932&status=done&style=none&width=362" alt="image.png"><br>Single head 单头：我们尝试使用仅一个头的变体，而不是像默认体系结构那样使用 5 个头。多头有助于捕获不同子空间中的注意力权重。使用单头始终会降低 SAKT 在所有数据集上的性能。<br>No block 无注意力块：当不使用自我注意块时，对下一练习的预测仅取决于最后一次互动。可以看出，在没有注意阻塞的情况下，系统的性能比默认架构下的性能要差得多。<br>2 Blocks 2 个注意力模块：增加自我关注块的数量会增加模型的参数数量。然而，在我们的例子中，这种参数的增加被证明对改进性能没有用处。这是因为预测学生在习题中的表现的一个重要方面取决于他在过去相关习题中的表现。再增加一块自我关注会使模型变得更加复杂。<br><strong>Training efficiency 训练效率：</strong>图 4 根据训练阶段在 GPU 上的运行时间展示了各种方法的效率。对比计算效率，SAKT 在一个 epoch 中只花费 1.4 秒，比 DKT+(65 秒/epoch)所花费的时间少 46.42 秒，比 DKT(45 秒/epoch)所花费的时间少 32 倍，比 DKVMN(26 秒/epoch)所花费的时间少 17.33 倍。我们在单个 NVIDIA Titan V 型的 GPU 上进行了实验。<br><img src="https://cdn.nlark.com/yuque/0/2021/png/12870328/1618244089705-411525f1-a6a3-4a73-b6f9-47f5a79fb192.png#align=left&display=inline&height=209&margin=%5Bobject%20Object%5D&name=image.png&originHeight=209&originWidth=298&size=25763&status=done&style=none&width=298" alt="image.png"></p><h1 id="5-CONCLUSION-AND-FUTURE-WORK-结论与未来工作"><a href="#5-CONCLUSION-AND-FUTURE-WORK-结论与未来工作" class="headerlink" title="5. CONCLUSION AND FUTURE WORK 结论与未来工作"></a>5. CONCLUSION AND FUTURE WORK 结论与未来工作</h1><p>在本研究中，我们提出了一个基于自我注意的知识追踪模型——SAKT。它模拟学生的交互历史(不使用任何 RNN)，并通过考虑学生过去交互的相关练习来预测他在下一个练习中的表现。在各种真实世界的数据集上进行的大量实验表明，我们的模型可以优于最先进的方法，并且比基于 rnn 的方法快一个数量级。</p><h1 id="6-REFERENCES-参考文献"><a href="#6-REFERENCES-参考文献" class="headerlink" title="6. REFERENCES 参考文献"></a>6. REFERENCES 参考文献</h1><p>……</p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;ABSTRACT-摘要&quot;&gt;&lt;a href=&quot;#ABSTRACT-摘要&quot; class=&quot;headerlink&quot; title=&quot;ABSTRACT 摘要&quot;&gt;&lt;/a&gt;ABSTRACT 摘要&lt;/h1&gt;&lt;p&gt;知识追踪是指当每个学生参与一系列学习活动时，对知识概念(KCs)的掌握情况进行建模的任务。每个学生的知识是通过评估学生在学习活动中的表现来建模的。为学生提供个性化的学习平台是一个重要的研究领域。近年来，基于递归神经网络(RNN)的方法，如深度知识追踪(DKT)和动态键值记忆网络(DKVMN)，由于能够捕获人类学习的复杂表示，因而优于所有传统方法。然而，这些方法在处理稀疏数据时面临的问题是不能很好地泛化【研究问题】，这是真实世界数据的情况下，学生与很少的 KCs 交互。为了解决这个问题，我们开发了一种方法，从学生过去的活动中识别与给定 KC 相关的 KC，并根据选择的相对较少的 KC 预测他/她的掌握情况。由于预测是基于相对较少的过去活动，它处理数据稀疏性问题比基于 RNN 的方法更好。为了识别 KCs 之间的相关性，我们提出了一种基于自我注意的方法——自我注意知识追踪(SAKT)。在各种真实世界数据集上的广泛实验表明，我们的模型在知识追踪方面优于最先进的模型，平均提高了 4.43%的 AUC。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;关键词：&lt;/strong&gt;知识追踪，大规模网络开放课程，自我关注，顺序推荐&lt;/p&gt;</summary>
    
    
    
    <category term="知识追踪" scheme="https://ytno1.github.io/categories/%E7%9F%A5%E8%AF%86%E8%BF%BD%E8%B8%AA/"/>
    
    <category term="SAKT" scheme="https://ytno1.github.io/categories/%E7%9F%A5%E8%AF%86%E8%BF%BD%E8%B8%AA/SAKT/"/>
    
    
    <category term="知识追踪" scheme="https://ytno1.github.io/tags/%E7%9F%A5%E8%AF%86%E8%BF%BD%E8%B8%AA/"/>
    
    <category term="SAKT" scheme="https://ytno1.github.io/tags/SAKT/"/>
    
  </entry>
  
  <entry>
    <title>博客DIY清单</title>
    <link href="https://ytno1.github.io/archives/b46b0450.html"/>
    <id>https://ytno1.github.io/archives/b46b0450.html</id>
    <published>2021-04-10T15:07:41.000Z</published>
    <updated>2021-08-31T07:14:52.160Z</updated>
    
    <content type="html"><![CDATA[<h1 id="开始之前"><a href="#开始之前" class="headerlink" title="开始之前"></a>开始之前</h1><p>生命不息，折腾不止！尝试建一个属于自己个人博客，记录美好生活！</p><span id="more"></span><h1 id="配置项"><a href="#配置项" class="headerlink" title="配置项"></a>配置项</h1><ul><li><p>安装目录</p></li><li><p>宠物 or 看板娘</p></li><li><p>头像 旋转等效果</p></li><li><p>分页效果</p></li><li><p>404 页面 使用模板 or DIY</p></li><li><p>访问、阅读统计 （注：部署后正常显示统计数，仅测试时显示为不蒜子网站的统计数）</p></li><li><p>博客分类</p></li><li><p>评论系统</p></li><li><p>音乐、视频插件</p></li><li><p>sitemap 设置</p></li></ul><p>……</p><h1 id="优质教程"><a href="#优质教程" class="headerlink" title="优质教程"></a>优质教程</h1><ul><li>视频教程 <a href="https://www.bilibili.com/video/BV1Yb411a7ty?from=search&seid=6873047497118275353">https://www.bilibili.com/video/BV1Yb411a7ty?from=search&amp;seid=6873047497118275353</a></li><li>文字教程 <a href="https://segmentfault.com/a/1190000020382983">https://segmentfault.com/a/1190000020382983</a></li><li>模板 <a href="http://litten.me/">http://litten.me/</a></li><li>主题配置优化 <a href="http://dongshuyan.com/2019/05/24/hexo%25E5%258D%259A%25E5%25AE%25A2%25E6%25B3%25A8%25E6%2584%258F%25E4%25BA%258B%25E9%25A1%25B9/">http://dongshuyan.com/2019/05/24/hexo%E5%8D%9A%E5%AE%A2%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9/</a></li><li>语雀文章同步到 heox 博客 <a href="https://luan.ma/post/yuque2blog/">https://luan.ma/post/yuque2blog/</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;开始之前&quot;&gt;&lt;a href=&quot;#开始之前&quot; class=&quot;headerlink&quot; title=&quot;开始之前&quot;&gt;&lt;/a&gt;开始之前&lt;/h1&gt;&lt;p&gt;生命不息，折腾不止！尝试建一个属于自己个人博客，记录美好生活！&lt;/p&gt;</summary>
    
    
    
    <category term="经验帖" scheme="https://ytno1.github.io/categories/%E7%BB%8F%E9%AA%8C%E5%B8%96/"/>
    
    <category term="blog优化" scheme="https://ytno1.github.io/categories/%E7%BB%8F%E9%AA%8C%E5%B8%96/blog%E4%BC%98%E5%8C%96/"/>
    
    
    <category term="经验帖" scheme="https://ytno1.github.io/tags/%E7%BB%8F%E9%AA%8C%E5%B8%96/"/>
    
    <category term="blog优化" scheme="https://ytno1.github.io/tags/blog%E4%BC%98%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>教师资格证考试</title>
    <link href="https://ytno1.github.io/archives/5ddaa770.html"/>
    <id>https://ytno1.github.io/archives/5ddaa770.html</id>
    <published>2021-04-10T14:55:46.000Z</published>
    <updated>2021-08-31T07:14:52.145Z</updated>
    
    <content type="html"><![CDATA[<h1 id="开始之前"><a href="#开始之前" class="headerlink" title="开始之前"></a>开始之前</h1><p>打算从事教师职业的话，还是尽早在在校期间把教师资格证拿到手，要简单很多。</p><h1 id="笔试"><a href="#笔试" class="headerlink" title="笔试"></a>笔试</h1><h2 id="综合素质"><a href="#综合素质" class="headerlink" title="综合素质"></a>综合素质</h2><ul><li>选择题很重要，把题库刷完，⼤多挂在选择题上，可以⽤粉笔⽹等题库（三部分选择题加起来⼤约 2/3 千道选择题）软件（喜欢⽤纸质试卷刷题就⽤试卷），有很多，质量都可以</li><li>中公教育里的试卷或者网上整理的 pdf，应⽤题考的永远是那⼏个题，背下来就好了</li></ul><span id="more"></span><h2 id="教育知识与能力"><a href="#教育知识与能力" class="headerlink" title="教育知识与能力"></a>教育知识与能力</h2><ul><li>同样选择题很重要，将题库刷完</li><li>应⽤题有⼤致范围，可以准备⼀份资料去背，总之多写没错，⾔之有理就有分</li></ul><h2 id="学科知识与教学能⼒"><a href="#学科知识与教学能⼒" class="headerlink" title="学科知识与教学能⼒"></a>学科知识与教学能⼒</h2><ul><li>第三门真的好过，大部分⼈第三门课没复习都过了(都是本专业的，跨科的另讲)，反而前两门公共课没过，因此精力放在前两⻔，多刷选择题，背应⽤题</li></ul><h1 id="⾯试"><a href="#⾯试" class="headerlink" title="⾯试"></a>⾯试</h1><ul><li>学校老师⾯试，看你很亲切，不会卡你，⽐较好过，但也别掉以轻⼼</li><li>推荐粉笔⽹的⼀些⾯试教程，会有收获</li></ul><h1 id="资料分享"><a href="#资料分享" class="headerlink" title="资料分享"></a>资料分享</h1><ul><li>提纲版链接: <a href="https://pan.baidu.com/s/1fFjNq2UPQe9WhoPV0rD7dA">https://pan.baidu.com/s/1fFjNq2UPQe9WhoPV0rD7dA</a> 提取码: clfj</li><li>详细资料链接: <a href="https://pan.baidu.com/s/1kIR3c5j6V-">https://pan.baidu.com/s/1kIR3c5j6V-</a> SsdvhzK9rhkA 提取码: 64wg</li><li>粉笔网课程链接: <a href="https://pan.baidu.com/s/14QcXW99sXpXtHRMk2gULWQ">https://pan.baidu.com/s/14QcXW99sXpXtHRMk2gULWQ</a> 提取码: ddv7</li><li>信息技术教材链接: <a href="https://pan%20.baidu.com/s/">https://pan .baidu.com/s/</a> 1VazhG0F Gnwl8Wr22ydjJg 提取码: pvmz</li></ul>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;开始之前&quot;&gt;&lt;a href=&quot;#开始之前&quot; class=&quot;headerlink&quot; title=&quot;开始之前&quot;&gt;&lt;/a&gt;开始之前&lt;/h1&gt;&lt;p&gt;打算从事教师职业的话，还是尽早在在校期间把教师资格证拿到手，要简单很多。&lt;/p&gt;
&lt;h1 id=&quot;笔试&quot;&gt;&lt;a href=&quot;#笔试&quot; class=&quot;headerlink&quot; title=&quot;笔试&quot;&gt;&lt;/a&gt;笔试&lt;/h1&gt;&lt;h2 id=&quot;综合素质&quot;&gt;&lt;a href=&quot;#综合素质&quot; class=&quot;headerlink&quot; title=&quot;综合素质&quot;&gt;&lt;/a&gt;综合素质&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;选择题很重要，把题库刷完，⼤多挂在选择题上，可以⽤粉笔⽹等题库（三部分选择题加起来⼤约 2/3 千道选择题）软件（喜欢⽤纸质试卷刷题就⽤试卷），有很多，质量都可以&lt;/li&gt;
&lt;li&gt;中公教育里的试卷或者网上整理的 pdf，应⽤题考的永远是那⼏个题，背下来就好了&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    <category term="经验帖" scheme="https://ytno1.github.io/categories/%E7%BB%8F%E9%AA%8C%E5%B8%96/"/>
    
    <category term="教师资格证" scheme="https://ytno1.github.io/categories/%E7%BB%8F%E9%AA%8C%E5%B8%96/%E6%95%99%E5%B8%88%E8%B5%84%E6%A0%BC%E8%AF%81/"/>
    
    
    <category term="经验帖" scheme="https://ytno1.github.io/tags/%E7%BB%8F%E9%AA%8C%E5%B8%96/"/>
    
    <category term="教师资格证" scheme="https://ytno1.github.io/tags/%E6%95%99%E5%B8%88%E8%B5%84%E6%A0%BC%E8%AF%81/"/>
    
  </entry>
  
  <entry>
    <title>Linux常用命令记录</title>
    <link href="https://ytno1.github.io/archives/30a497b6.html"/>
    <id>https://ytno1.github.io/archives/30a497b6.html</id>
    <published>2021-04-10T14:14:34.000Z</published>
    <updated>2021-08-31T07:14:52.109Z</updated>
    
    <content type="html"><![CDATA[<h1 id="开始之前"><a href="#开始之前" class="headerlink" title="开始之前"></a>开始之前</h1><p>在当前的学习生涯中，Linux 服务器主要是用来跑 ML、DL 代码的，所以记录一下自己利用服务器跑实验经常需要用到的命令。</p><span id="more"></span><h1 id="虚拟环境"><a href="#虚拟环境" class="headerlink" title="虚拟环境"></a>虚拟环境</h1><p>好处：1、能够使不同开发环境独立，环境升级不影响其他应用、环境，可以防止系统中出现包管理混乱和版本的冲突。2、深度学习论文的源代码有环境、各种包的版本要求，利用虚拟环境配置一个代码所需的干净环境。</p><h2 id="方式-1-virtualenvwrapper-创建环境"><a href="#方式-1-virtualenvwrapper-创建环境" class="headerlink" title="方式 1 virtualenvwrapper 创建环境"></a>方式 1 virtualenvwrapper 创建环境</h2><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install virtualenvwrapper-win</span><br><span class="line">pip install virtualenvwrapper        <span class="comment"># linux环境</span></span><br></pre></td></tr></table></figure><p>创建虚拟环境 mkvirtualenv test1(虚拟环境名称)</p><h3 id="设置-WORK-HOME-环境变量"><a href="#设置-WORK-HOME-环境变量" class="headerlink" title="设置 WORK_HOME 环境变量"></a>设置 WORK_HOME 环境变量</h3><h3 id="创建、查看、激活等操作"><a href="#创建、查看、激活等操作" class="headerlink" title="创建、查看、激活等操作"></a>创建、查看、激活等操作</h3><ul><li>选择一个 python 解释器来搭建：mkvirtualenv env –python=python2.7</li><li>查看虚拟环境 lsvirtualenv 或者 workon</li><li>进入虚拟环境 workon test1(虚拟环境名称)</li><li>退出虚拟环境 deactivate</li><li>删除虚拟环境 rmvirtualenv test1(虚拟环境名称)</li><li>查看虚拟环境下的安装包 pip list</li></ul><h2 id="方式-2-conda-创建虚拟环境"><a href="#方式-2-conda-创建虚拟环境" class="headerlink" title="方式 2 conda 创建虚拟环境"></a>方式 2 conda 创建虚拟环境</h2><h3 id="安装、配置-Anaconda"><a href="#安装、配置-Anaconda" class="headerlink" title="安装、配置 Anaconda"></a>安装、配置 Anaconda</h3><p>网上教程很多，直接搜即可</p><h3 id="创建、激活虚拟环境等操纵"><a href="#创建、激活虚拟环境等操纵" class="headerlink" title="创建、激活虚拟环境等操纵"></a>创建、激活虚拟环境等操纵</h3><ul><li>查看安装的包 conda list</li><li>查看存在的虚拟环境 conda env list 或 conda info -e</li><li>检查更新 conda conda update conda</li><li>创建虚拟环境  conda create -n your_env_name python=X.X（2.7、3.6 等) 创建 python 版本为 X.X、名字为 your_env_name 的虚拟环境。your_env_name 文件可以在 Anaconda 安装目录 envs 文件下找到。</li><li>激活虚拟环境 conda activate yut</li><li>安装包 conda install [package]</li><li>关闭虚拟环境    Linux: source deactivate   Windows: deactivate</li><li>删除虚拟环境 conda remove -n your_env_name(虚拟环境名称) –all， 即可删除。</li><li>删除环境中的某个包 conda remove –name your_env_name  package_name 。</li></ul><h1 id="服务器中常用命令"><a href="#服务器中常用命令" class="headerlink" title="服务器中常用命令"></a>服务器中常用命令</h1><h2 id="命令行快捷操作"><a href="#命令行快捷操作" class="headerlink" title="命令行快捷操作"></a>命令行快捷操作</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">ctrl + e &#x2F;&#x2F; 光标回到行末</span><br><span class="line">ctrl + k &#x2F;&#x2F; 删除光标处到行尾的字符</span><br><span class="line">ctrl + u &#x2F;&#x2F; 删除命令行的整段命令</span><br><span class="line">ctrl + y &#x2F;&#x2F; 恢复上一次删除内容</span><br><span class="line">nvidia-smi  # 查看GPU使用情况</span><br><span class="line">ps aux  查看进程情况</span><br><span class="line">kill  -9 pid  # 关掉相应进程</span><br><span class="line"># 后台运行</span><br><span class="line">## nohup命令</span><br><span class="line">  nohup python -u RKT.py [Parameters] &gt; RKT.log 2&gt;&amp;1 &amp;  # 挂后台</span><br><span class="line">  tail -f RKT.log  # 查看输出情况</span><br><span class="line">## screen命令 开一个会话的同时时创建多个窗口处理不同的任务</span><br><span class="line">  screen -S test &#x2F;&#x2F;创建一个名为test的新窗口</span><br><span class="line">  ctrl + a + d 断开窗口的连接回到会话界面，注意：只是断开了窗口并未终止任务的运行</span><br><span class="line">  screen -ls &#x2F;&#x2F;显示所有窗口</span><br><span class="line">  screen -r test &#x2F;&#x2F;返回test窗口</span><br><span class="line">  ctrl + d  &#x2F;&#x2F; 断开某个窗口</span><br></pre></td></tr></table></figure><h2 id="vim-快捷操作"><a href="#vim-快捷操作" class="headerlink" title="vim 快捷操作"></a>vim 快捷操作</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">Esc &#x2F;&#x2F; 从当前模式转换到“普通模式”。</span><br><span class="line">i &#x2F;&#x2F; “插入模式”用于插入文字。</span><br><span class="line"></span><br><span class="line">:  &#x2F;&#x2F;“命令行模式” Vim 希望你输入类似于保存该文档命令的地方。例如：</span><br><span class="line">:q&#x2F;&#x2F; 退出 Vim，如果文件已被修改，将退出失败</span><br><span class="line">    :wq &#x2F;&#x2F; 保存文件并退出 Vim</span><br><span class="line"></span><br><span class="line">gg  &#x2F;&#x2F; 将光标移动到文档开头</span><br><span class="line">G  &#x2F;&#x2F; 将光标移动到文档末尾</span><br><span class="line">$  &#x2F;&#x2F; 将光标移动到本行尾</span><br><span class="line">0  &#x2F;&#x2F; 将光标移动到本行行首</span><br><span class="line">ndd  &#x2F;&#x2F; 删除n行(如10+dd)</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;开始之前&quot;&gt;&lt;a href=&quot;#开始之前&quot; class=&quot;headerlink&quot; title=&quot;开始之前&quot;&gt;&lt;/a&gt;开始之前&lt;/h1&gt;&lt;p&gt;在当前的学习生涯中，Linux 服务器主要是用来跑 ML、DL 代码的，所以记录一下自己利用服务器跑实验经常需要用到的命令。&lt;/p&gt;</summary>
    
    
    
    <category term="Linux" scheme="https://ytno1.github.io/categories/Linux/"/>
    
    
    <category term="Linux" scheme="https://ytno1.github.io/tags/Linux/"/>
    
    <category term="常用命令" scheme="https://ytno1.github.io/tags/%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"/>
    
  </entry>
  
</feed>
